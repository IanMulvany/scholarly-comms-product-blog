<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ScholCommsProd</title>
    <link>http://scholarly-comms-product-blog.com/</link>
    <description>Recent content on ScholCommsProd</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Sep 2020 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="http://scholarly-comms-product-blog.com/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Prakash lab - frugal science and the future of research</title>
      <link>http://scholarly-comms-product-blog.com/2020/09/25/prakash_lab_-_frugal_science_and_the_future_of_research_/</link>
      <pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2020/09/25/prakash_lab_-_frugal_science_and_the_future_of_research_/</guid>
      <description>&lt;p&gt;I just wanted to share some work coming out of the Prakash lab in Stanford. Manu Prakash is a young engineer and research scientist who has taken a very different approach to how to think about the tools we use to do science. He has focussed on building very cheap tools that can be used by anyone in the world. One of the first tools that his lab created was the foldscope - a microscope built from paper, and a small piece of plastic - that can be produced very cheaply and given out to people across the world (&lt;a href=&#34;https://indiabioscience.org/videos/foldscope-india)&#34;&gt;https://indiabioscience.org/videos/foldscope-india)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Their lab has also created tools to allow cheap and wide-scale analysis of the microbiome of the ocean.&lt;/p&gt;
&lt;p&gt;They have just launched a massive online course open to anyone in the world on how to use frugal science methods to understand the world around them - &lt;a href=&#34;https://www.frugalscience.org/&#34;&gt;https://www.frugalscience.org/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This is a very very different approach to thinking about science.&lt;/p&gt;
&lt;p&gt;They have also created a hand-crank centrifuge that can be used in the field with no electricity. They have used this as a platform to develop a rapid COVID test that can test for viral RNA at a cost of less than $1 per test and this paper has just come out in medarxiv - &lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2020.06.30.20143255v1&#34;&gt;https://www.medrxiv.org/content/10.1101/2020.06.30.20143255v1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I think Manu is one of the most exciting young researchers working in the world at the moment, and I&amp;rsquo;m delighted to see his work now having an impact on the current COVID crisis.&lt;/p&gt;
&lt;p&gt;It’s great to see how thinking different can yield such impact, and it’s an inspiration to keep trying to think fresh in the way any of us approach innovation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Profit Trees and making business cases</title>
      <link>http://scholarly-comms-product-blog.com/2020/07/12/profit_trees_and_making_business_cases_/</link>
      <pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2020/07/12/profit_trees_and_making_business_cases_/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.invisionapp.com/inside-design/estimate-roi-design-work/&#34;&gt;This&lt;/a&gt; is a very nice overview of how to create a clear business case for design work, but the general approach of walking a tree to find leverage points to build up the argument can be applied any time that you want to make a business case.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CODECHECK - reviewing code in publications</title>
      <link>http://scholarly-comms-product-blog.com/2020/07/03/codecheck_-_reviewing_code_in_publications/</link>
      <pubDate>Fri, 03 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2020/07/03/codecheck_-_reviewing_code_in_publications/</guid>
      <description>&lt;p&gt;CODECHECK is a fascinating service - &lt;a href=&#34;https://codecheck.org.uk/&#34;&gt;https://codecheck.org.uk/&lt;/a&gt; That creates a workflow for academics to provide feedback on research code. The project is being led by Stephen Eglen and and Daniel Nüst.&lt;/p&gt;
&lt;p&gt;They describe what they do succinctly as&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;CODECHECK is a process for independent reproduction of computations and awarding of time-stamped certificates for successful reproductions of scholarly articles.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Checking software in research is a significant challenge, and I really like CODECHECK because it is an initiative that has emerged from researchers themselves.&lt;/p&gt;
&lt;p&gt;I met Stephen at an event earlier this year (when we still did those) and he recently emailed me with an update on the service. Below are some of the recent highlights.&lt;/p&gt;
&lt;p&gt;Of note are their contributions in checking the COVID models.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Our first &amp;ldquo;proper&amp;rdquo; CODECHECK certificate was for a Gigascience paper
that came out in April. Scott Edmunds helped with a blog piece about
it:  &lt;a href=&#34;https://github.com/codecheckers/Piccolo-2020&#34;&gt;https://github.com/codecheckers/Piccolo-2020&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Our biggest impact has been codechecking the (infamous) COVID model
from Neil Ferguson&amp;rsquo;s group at Imperial College; Nature News ran a piece
on this recently:  &lt;a href=&#34;https://www.nature.com/articles/d41586-020-01685-y&#34;&gt;https://www.nature.com/articles/d41586-020-01685-y&lt;/a&gt;
By contrast to all the alarm in the media, I found the results
reproducible.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I&amp;rsquo;m now finishing up a couple of other COVID models from Imperial
(having done some others for LSHTM). For one of the LSHTM papers,
because we worked on a preprint as it (unknown to me) was going through
peer review, the authors acknowledged and cited the certificate (ref 18)
in their Lancet Public Health
paper.  &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S246826672030133X&#34;&gt;https://www.sciencedirect.com/science/article/pii/S246826672030133X&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;All the certificates completed so
far are browsable via:  &lt;a href=&#34;https://zenodo.org/communities/codecheck/&#34;&gt;https://zenodo.org/communities/codecheck/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The certificates are evolving as we learn more about the process, so
I think it will be a while yet before we hit a stable idea of what a
certificate should look like. We&amp;rsquo;ve probably also done a poor job of
collecting the relevant metadata and getting certificates deposited in
correct manner.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Using Datasette to publish data</title>
      <link>http://scholarly-comms-product-blog.com/2020/07/02/using_datasette_to_publish_data/</link>
      <pubDate>Thu, 02 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2020/07/02/using_datasette_to_publish_data/</guid>
      <description>&lt;p&gt;In my department we have started to make a bit of space to allow for self learning to
happen. I took the time to look at &lt;a href=&#34;https://datasette.readthedocs.io/en/stable/&#34;&gt;https://datasette.readthedocs.io/en/stable/&lt;/a&gt;, an ecosystem of tools that support
data publishing. These tools are from Simon Willison, and they are fantastic.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;d been meaning to look at the them for some time now. I used a jupyter notebook
to work my way around getting some data together, and working with the tool.&lt;/p&gt;
&lt;p&gt;In the notebook I:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;get some data about 100 BMJ articles using the crossref API&lt;/li&gt;
&lt;li&gt;take a selection of the metadata from those articles&lt;/li&gt;
&lt;li&gt;install Datasette locally&lt;/li&gt;
&lt;li&gt;use sqlite_utils to create a local sqlite database&lt;/li&gt;
&lt;li&gt;use datasette running locally to explore and interact with this data&lt;/li&gt;
&lt;li&gt;export to CSV from my local instance&lt;/li&gt;
&lt;li&gt;upload this CSV to an online instance of Datasette running on Glitch&lt;/li&gt;
&lt;li&gt;configure the Glitch instance to enable full text search on some of the fields&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The final output can be seen running at  &lt;a href=&#34;https://watery-alder-carpenter.glitch.me/data/article&#34;&gt;https://watery-alder-carpenter.glitch.me/data/article&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Overall it went really well, and I&amp;rsquo;m excited about Datasette.&lt;/p&gt;
&lt;p&gt;You can grab a copy of this notebook from &lt;a href=&#34;https://github.com/IanMulvany/datasette-testing-bmj-articles&#34;&gt;https://github.com/IanMulvany/datasette-testing-bmj-articles&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Goodbye SAGE, hello BMJ!</title>
      <link>http://scholarly-comms-product-blog.com/2020/02/13/goodbye_sage_hello_bmj_/</link>
      <pubDate>Thu, 13 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2020/02/13/goodbye_sage_hello_bmj_/</guid>
      <description>&lt;p&gt;I’m excited to share the news that from mid-April I will be joining the BMJ as their new CTO. I’ll be leaving SAGE on great terms with the teams here, and for sure there will be a little sadness and I’ll definitely miss my colleagues, but equally I’m super excited about the new challenges ahead.&lt;/p&gt;
&lt;p&gt;Crucially, at SAGE we are hiring for my replacement, so if you are interested in applying for one of the most fun roles in the Scholarly Publishing industry at the moment then reach out, I’m happy to chat about what we have been up to!&lt;/p&gt;
&lt;p&gt;I think BMJ is going to be a great fit, we have a shared set of values, they have always been a leader in supporting open science, and there is no more interesting time to be in a tech role in the publishing industry, with the changes being driven by Plan S, the advent of easy to use machine learning technologies, and a growing understanding of the value and importance of truth and unbiased evidence. (Also, as it happens, my wife publishes with BMJ, so I’m going to have direct exposure to a very vocal user).&lt;/p&gt;
&lt;p&gt;My whole career has been driven by wanting to use technology to help improve the way research is done and communicated, and I am thrilled to be able to continue with that goal at the BMJ. Seriously, I feel tremendously lucky to have these opportunities, and to have been able to work with amazing colleagues to date, on some amazing projects.&lt;/p&gt;
&lt;p&gt;I’ll spend the rest of this post looking back a bit, as my chapter at SAGE is coming to a close. I joined 2016, and over the past three years I’ve been able to learn a ton about lean product development, rapid prototyping, machine learning, neural networks, working within a slightly larger organisation, the absolute value of creating great ground rules for teams, the intricacies of publishing in the humanities and social sciences, how to think about product names with an American accent, the joys and horror of MS Teams, what Lean means to an organisation and the difference between top down and bottom up scaling of process, how not to pick good hotels, learning by doing, cascading goals down into bets and then picking across those bets given the best information available at the time. Maybe the most valuable skill I have learnt is how to tear off a post-it note the right way (thanks &lt;a href=&#34;https://twitter.com/Cennydd&#34;&gt;@Cennydd&lt;/a&gt;)!.&lt;/p&gt;
&lt;p&gt;Looking back over the last three years I want to call out two specific pieces of work that I am very proud to have been involved with. The first is all of the work that led to the launch of SAGE Ocean and it’s associated offerings. SAGE Ocean is about supporting social science researchers who want to work with data at scale. This emergent approach to social science is often called computational social science. It involves many areas of expertise, from data collection, network analysis, algorithm development, right through to understanding the sources of power in our digital systems and the ethics and implications that machine systems have on society. From a business, product and branding perspective it was also a chance to create something from scratch, and to delight in seeing the positive response that we received from the communities we wanted to serve through creating this initiative. We brought to bear a rapid approach to product development inside of a fairly large organisation and we also partnered with some amazing organisations along the way. It was a joy to be involved in.&lt;/p&gt;
&lt;p&gt;The second area of work that I want to call out is around internal innovation within SAGE. The title that I am moving on from is “Head of transformation”. I’ve heard it said that no one in an organisation should have either “transformation” or “Innovation” titles, as everybody should have the autonomy to be transformative and innovative. Well, maybe, but rather fixating on titles it’s better to look at results, and I’m really proud that the team I worked with introduced new technologies, data sources and ways of working that got adopted across the company. We definitely started off in more of an “Innovation Theatre” mode, but combining our technical abilities with a sober assessment of where we could add value to process or systems led to some great results.&lt;/p&gt;
&lt;p&gt;Of course what makes an organisation is the people, and I have had just the most wonderful colleagues at SAGE. The stories I could tell - of fun, intensity, kindness, a willingness to experiment and learn as we went along. It’s been great. I can’t mention everyone, just too many, but I have to give a thank you to &lt;a href=&#34;https://twitter.com/coffeepot&#34;&gt;Martha&lt;/a&gt;&lt;a href=&#34;https://twitter.com/loucoady&#34;&gt;Lou&lt;/a&gt; &lt;a href=&#34;https://twitter.com/KMetzlerSAGE&#34;&gt;Katie&lt;/a&gt;&lt;a href=&#34;https://twitter.com/sheen&#34;&gt;Sheen&lt;/a&gt;&lt;a href=&#34;https://www.thoughtworks.com/profiles/kylie-castellaw&#34;&gt;Kylie&lt;/a&gt;&lt;a href=&#34;https://twitter.com/danielagduca&#34;&gt;Daniela&lt;/a&gt;&lt;a href=&#34;https://twitter.com/AdamSci12&#34;&gt;Adam&lt;/a&gt; &lt;a href=&#34;https://www.linkedin.com/in/razvantelitoiu/&#34;&gt;Raz&lt;/a&gt; &lt;a href=&#34;https://www.linkedin.com/in/ziyad-marar-531146165/&#34;&gt;Ziyad&lt;/a&gt;  &lt;a href=&#34;https://github.com/andyhails&#34;&gt;Andy&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/Crayola_26&#34;&gt;Shiran&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you think that any of this sounds interesting or enticing, do please reach out, as we are looking to fill my position!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>thoughts on GetFTR</title>
      <link>http://scholarly-comms-product-blog.com/2019/12/11/thoughts_on_getftr_/</link>
      <pubDate>Wed, 11 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2019/12/11/thoughts_on_getftr_/</guid>
      <description>&lt;h3 id=&#34;what-does-it-do&#34;&gt;what does it do?&lt;/h3&gt;
&lt;p&gt;By using a single sign-on system (like SAML, or OAuth) a researcher can have their browser remember who they are. Today most access to subscription content is done via IP authentication. A university pays a publisher to access the content that the publisher hosts, and the university sends over a list of the IP ranges that cover the university buildings. Any researcher on campus just gets access. The big problem is that when you are off campus that doesn’t work any more.&lt;/p&gt;
&lt;p&gt;If researchers log in to the publisher site via a university authenticated single sign on system, then the browser can remember that the researcher has access to the content, on their behalf.  This allows the researcher to get access to the publisher site, even when they are off campus. (It also allows the publisher to have a much better understanding of who is accessing their content).&lt;/p&gt;
&lt;p&gt;That’s the key idea behind ”seamlessaccess.org”, but there could be concerns about the publisher now knowing who the researcher is.&lt;/p&gt;
&lt;p&gt;So far, so good, but what does GetFTR do that’s different. As I understand it, from reading what’s on the GetFTR site, GetFTR has an API and when any website that shows a DOI on it wants to, that website can send a request to this API with the DOI, and if the website has it, it can also send information about the institution from where the request is coming. Now rather than the identification of the university being any old identifying string, it needs to be one backed by a single sign on system, like SAML. The API will then send  some information back that can allow the website to create a ”WAYFless URL”. That lest the person clicking the link get directly to the content without having to go through the payday. Now GetFTR have said that they don’t need or receive any user specific information, they just need the institutional authentication.&lt;/p&gt;
&lt;p&gt;OK, so what is the difference, for the user, between seamlessaccess and GetFTR? I &lt;em&gt;think&lt;/em&gt; that the difference is the following - with seamless access you the user have to log in to the publisher site. With GetFTR if you are providing pages that contain DOIs (like on a discovery service) to your researchers, you can give them links they can click on that have been setup to get those users direct access to the content. That means as a researcher, so long as the discovery service has you as an authenticated user, you don’t need to even think about logins, or publisher access credentials.&lt;/p&gt;
&lt;p&gt;On the other hand, the links that you are being presented are now determined by the publisher.&lt;/p&gt;
&lt;p&gt;If you are on campus, and your institution has access to the publisher site, then the end result should be indistinguishable  from just clicking on the DOI and getting a redirect to the publisher site (unless the publisher decides to stop supporting IP authentication, and maybe that is a long term goal of this initiative, but who knows ¯_(ツ)_/¯).&lt;/p&gt;
&lt;p&gt;If you are off campus the experience could be different for paywalled journals, in that if you are logged in to your discovery service you should now get links that will work for you without you needing to log in to specific publisher sites, whereas just clicking on the DOI won’t necesscarily get you that access.&lt;/p&gt;
&lt;p&gt;How about using unpaywall, the open access button, or some other browser extension for finding alternative links for that article that you need (like links to repository versions, for example?). Well the good folks behind GetFTR &lt;em&gt;could&lt;/em&gt; also provide those links if they were to wrap around the unpaywall API, for example, but what will happen is that some publishers will have the idea of wanting to provide researchers with some form of bronze version of the article, perhaps through platforms that restrict the ability of the researcher to copy and paste form the text, or to share onwardly, all presumable with the aims of being able to “count” usage while at the same time reducing the surface area from which content can “leak” from the publisher site. In a world in which contracts with university libraries is focussed on data like COUNTER usage, then this is a perfectly reasonable concern from publishers, however the choices that publisher make about how to present “alternative” versions to reachers is going to vary.&lt;/p&gt;
&lt;p&gt;One example of how this could suck, lets imagine that there is a very usable green OA version of an article, but the publisher wants to push me to using some “e-reader limited functionality version” that requires an account registration, or god forbid a browser exertion, or desktop app. If the publisher shows only this limited utility version, and not the green version, well that sucks.&lt;/p&gt;
&lt;p&gt;OK, so that’s my understanding of what GetFTR does (if I’m wrong, then I hope I’ll get corrected quickly.&lt;/p&gt;
&lt;h3 id=&#34;why-is-it-doing-that&#34;&gt;why is it doing that?&lt;/h3&gt;
&lt;p&gt;OK, so, I’ve covered what it does, but why are the big five doing this? Weeeeeellllll. It’s not totally clear, but the elephant in the room is sci-hub - the website where everyone in the world can go and get an illegal copy of any research article. That combined with the economic system of value exchange in scholarly publishing, probably gives sufficient motivation for publishers to create this service.&lt;/p&gt;
&lt;p&gt;I think there are two things that make it possible for sci-hub to be possible.&lt;/p&gt;
&lt;p&gt;The first is there is a huge fragmentation of publishers in the world, and the user experience of navigating all of these sites, and systems is a pain. There is no one clear unified experience for interacting with the scholarly research, and in the googlified world we live in, that just does not gel with the kind of experience that people want, so to start with sci-hub just provides a better user experience (I’m told, I’ve actually only used sci-hub once, and I found it a pain to use back then).&lt;/p&gt;
&lt;p&gt;The second thing that makes sci-hub possible is a combination of IP access (no-authentication access) along with weak on-campus security for researcher logins. That allows sci-hub to attack researcher accounts to enable them to scrape all of the content that they put on their servers.&lt;/p&gt;
&lt;p&gt;A service like GetFTR kind of targets both of the above.&lt;/p&gt;
&lt;p&gt;The motivation is probably mostly to try to get the industry on board to try to get beyond IP authentication, and to allow publishers to “count” usage so that the “usage” numbers they present to libraries look higher and allow them to justify either price rises or indeed, just simple renewals of contracts.&lt;/p&gt;
&lt;p&gt;I have to say, the moving beyond IP authentication is a really good idea for a whole host of security reasons, and it will indeed require a big shift across the whole industry, so if this initiative could be run in a way the builds trust amongst all parties, then that would be no small thing, and should be applauded.&lt;/p&gt;
&lt;h3 id=&#34;what-would-make-this-succeed&#34;&gt;what would make this succeed?&lt;/h3&gt;
&lt;p&gt;So, in order for this to work for a cohort of researchers, every place that they look for research will need to be to implement calls to this API, and will need to have information about that researchers authenticated access to their universities.&lt;/p&gt;
&lt;p&gt;That sounds like a tall order, but there are some services, and pieces of infrastructure, that have wide deployment footprints in the world, so if you can convince them to implement this, then maybe you get to a critical mass.&lt;/p&gt;
&lt;p&gt;Oh, you also need all the publishers to implement this too, and for publishers to do this, then they are going to need to be provided with a very low cost, turn-key solution to be able to implement it.&lt;/p&gt;
&lt;p&gt;To keep a service like this running, I would expect that you would want to have a small dedicated team (maybe 5 — 10 people), and support a reasonable volume of API queries, with a good response rate. I’d benchmark the cost of running this service to be on the order of a few hundred thousand dollars. With maybe 14k publishers in the world, then the cost of joining this service should be on the order of a few thousand dollars per year, not withstanding the cost of implementation and integration. That seems like a reasonable ball park.&lt;/p&gt;
&lt;h3 id=&#34;what-would-make-this-fail&#34;&gt;what would make this fail?&lt;/h3&gt;
&lt;p&gt;I’ve not done anything like a full analysis on this, in terms of looking at a lean canvas and trying to tease out the riskiest assumptions around the sustainability of this service, but the one thing that sticks out that could make this fail is lack of researcher adoption. If researchers use different discovery systems, and they don’t see these links everywhere, then they may just fall back to using a behavioural route that works everywhere. (That’s a totally untested hypothesis on my part, and indeed, the very reasonableness of it means that it should be tested).&lt;/p&gt;
&lt;p&gt;Contributing to lack of adoption could be if GetFTR is complex or costly for institutions and publishers to implement.&lt;/p&gt;
&lt;h3 id=&#34;what-would-make-this-irrelevant&#34;&gt;what would make this irrelevant?&lt;/h3&gt;
&lt;p&gt;If publishing moves to being fully OA then this service is irrelevant.&lt;/p&gt;
&lt;h3 id=&#34;what-are-the-concerns-that-we-might-think-about-with-this-and-do-they-hold-water&#34;&gt;what are the concerns that we might think about with this, and do they hold water?&lt;/h3&gt;
&lt;p&gt;OK, hold on to your hats, this is where we get into tin-foil hat territory.&lt;/p&gt;
&lt;h3 id=&#34;will-google-scholar-implement-this-will-other-discovery-services-do-so&#34;&gt;Will google scholar implement this, will other discovery services do so?&lt;/h3&gt;
&lt;p&gt;Well, that’s a question. I suppose that publishers could coerce GS to implement this by threatening to rescind access to their full text, but would they really want to take the hit to their traffic? I don’t see that as being worth the risk. GS is a small team, if all publishers were implementing this, they might join in, they might not, they have their own effort to try to solve this for users.&lt;/p&gt;
&lt;p&gt;Someone on twitter way saying the the big five could coerce discovery services to implement this through contractual enforcement. If the system is easy enough to implement and truly adds value, then you should not need to enforce implementation, and if it is not/does not, then enforced implementation is not where your key problems are with this service.&lt;/p&gt;
&lt;h4 id=&#34;concern---this-could-replace-crossref&#34;&gt;concern - this could replace crossref&lt;/h4&gt;
&lt;p&gt;This was my first thought when I head about it, and joined up this with some of the comments that were reported from Crossref Live. The thinking is that is this a way for publishers to provide a routing system independent of the handle system that corssref redirects are based off of? Well, yes it does do that, however the entire service depends on DOIs, so while it could replace some of the existing infrastructure, it as the same time remains dependant on the existing infrastructure. I can appreciate that to move fast sometimes its good to just get on and do things, but I would advocate that if this service were to become a true replacement for IP authentication then it should hand governance over to someone like NISO or some other independent body.&lt;/p&gt;
&lt;h4 id=&#34;concern---this-could-kill-green-oa&#34;&gt;concern - this could kill green OA&lt;/h4&gt;
&lt;p&gt;As I mentioned earlier, this could enable publishers to decide to provide links to restricted access, but free to read, versions of articles, in place of GreenOA. I don’t think this is going to be a big concern for two reasons. The first is that not all publishers will be able to do this, and secondly the motivation to deposit into institutional repositories is not currently driven by usage data from those repositories, but rather from mandates from funders, who are unlikely to be influenced by this service in how they decide to modify their mandates in the future.&lt;/p&gt;
&lt;h4 id=&#34;concern---this-could-leak-proprietary-publisher-information&#34;&gt;concern - this could leak proprietary publisher information&lt;/h4&gt;
&lt;p&gt;OK, so I’m not sure how the API is provisioned, but I am assuming that it is provisioned by a single end point. Who owns that endpoint and the logged data that this endpoint collects? That endpoint will have information about what resources different publishers have provided to institutions, combined with usage data from those institutions. If I am publisher A and have a journal close to that from publisher B, but that is “lower tier”, then if I could access usage data from publisher B I could modify my sales price. How am I going to be sure that my own usage data does not leak to other publishers signed up to this service?&lt;/p&gt;
&lt;h4 id=&#34;concern---this-does-no-look-like-the-kind-of-initiative-that-a-publisher-who-is-on-a-transformative-journey-to-open-access-would-invest-in&#34;&gt;concern - this does no look like the kind of initiative that a publisher who is on a transformative journey to Open Access would invest in.&lt;/h4&gt;
&lt;p&gt;If I am sitting in the European Commission or in JISC I am probably not going to look at this initiative as something that proves publishers are on a journey to becoming open access. I think GetFTR do need to make clear what the overall benefits to the scholarly ecosystem are with this initiative, because at the moment the information on their FAQ seems to mostly support the creation of publisher value.&lt;/p&gt;
&lt;h4 id=&#34;concern---this-will-allow-publishers-to-track-users-and-behaviourally-target-them&#34;&gt;concern - this will allow publishers to track users, and behaviourally target them.&lt;/h4&gt;
&lt;p&gt;If the users are now logging in to some service directly, won’t this mean that the publisher can now track that person directly? Actually GetFTR explicitly states that they don’t get any information about the user, and given that this needs to be implemented server side, then there are actually fewer potential security concerns compared to using a browser plugin based solution.&lt;/p&gt;
&lt;h3 id=&#34;what-do-i-think-about-it-all-what-are-my-recommendations&#34;&gt;What do I think about it all? What are my recommendations?&lt;/h3&gt;
&lt;p&gt;I don’t really know yet. Doing anything is hard, doing so in coordination amongst large competitors is even more so, so the team behind this should be recognised for having put tougher a well executed initial proposal for how to move on access management.&lt;/p&gt;
&lt;p&gt;I think if you are a researcher I would recommend continuing to use a bag of tools to get access to your research. For the interim this will only be a partial solution.&lt;/p&gt;
&lt;p&gt;If you are a publisher I would recommend calculating cost of implementation, time to implement against expectations around how much additional usage it is going to give you, and how much revenue that is going to translate to. Basically I would advocate using a cost of deals analysis on whether to implement this or now.&lt;/p&gt;
&lt;p&gt;I have two other recommendations for the GetFTR team. Both relate to building trust. First up, don’t list orgs as being on an advisory board, when they are not. Secondly it would be great to learn about the team behind the creation of the Service. At the moment its all very anonymous.&lt;/p&gt;
&lt;h3 id=&#34;links-that-i-referred-to-when-putting-this-together&#34;&gt;Links that I referred to when putting this together.&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://wiki.geant.org/display/gn43wp5/Seamless+Access+Description&#34;&gt;Seamless Access Description - GN4-3 Work Package 5 - GÉANT federated confluence&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/TheIdentitySelector/thiss-js&#34;&gt;GitHub - TheIdentitySelector/thiss-js: The identity selector software source&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://thiss-js.readthedocs.io/en/latest/&#34;&gt;Welcome to The Identity Selector Service’s documentation! — The Identity Selector Service 1.0.0 documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://dltj.org/article/publishers-alone-with-getftr/&#34;&gt;Publishers going-it-alone (for now?) with GetFTR | Disruptive Library Technology Jester&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://seamlessaccess.org&#34;&gt;Welcome to SeamlessAccess.org | SA Site&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.getfulltextresearch.com/faqs/&#34;&gt;https://www.getfulltextresearch.com/faqs/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://scholarlykitchen.sspnet.org/2019/12/10/why-are-librarians-concerned-about-getftr/&#34;&gt;https://scholarlykitchen.sspnet.org/2019/12/10/why-are-librarians-concerned-about-getftr/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.openaccessbutton.org/get-to-fulltext-ourselves-not-getftr-e952e798564b&#34;&gt;Get To Fulltext Ourselves, Not GetFTR. - openaccessbutton&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Some brief thoughts on scholarly publishing and climate change.</title>
      <link>http://scholarly-comms-product-blog.com/2019/12/06/some_brief_thoughts_on_scholarly_publishing_and_climate_change._/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2019/12/06/some_brief_thoughts_on_scholarly_publishing_and_climate_change._/</guid>
      <description>&lt;p&gt;Who’s job is it to address the crisis of climate change? This question came up at an STM Tech Leaders workshop that I attended earlier this week? The answer is that we all are, and in that context we had a very interesting conversation on the topic.&lt;/p&gt;
&lt;p&gt;This is the first time that I’ve seen this topic come up in a forum like this, so a bit thank you to Dave Smith from the IET for pushing the topic forward.&lt;/p&gt;
&lt;p&gt;Oh man, this is such a big topic, but we have to start somewhere, right?&lt;/p&gt;
&lt;p&gt;We had a short, and very useful conversation. I hope there is more space made for this in the future. There has to be. At the recent SpontOn London looking at publishing and the UN sustainable development goals one of the speakers outlined how as a species we have destroyed every habitat that we have colonised, and that the most appropriate description of a species that does that is a parasite. She also pointed out that all of our science and technology has been a force multiplier for that parasitism. (She made a call for us to focus on embedding sustainable values into our education, at that very moment where we equip ourselves with all of this knowledge we have generated). Well I digress, but it’s clear that this work we are engaged in - the knowledge creation work - has a pivotal role to play to address climate change. Furthermore we are working in companies of not insignificant scale and so we have an opportunity to embody best practice in a meaningful way to help make a difference.&lt;/p&gt;
&lt;p&gt;SpringerNature told us about how they had made the biggest  impact on their carbon footprint last year by re-configuring their Indian offices to be more energy efficient.&lt;/p&gt;
&lt;p&gt;Ours is an industry where flying happens a lot. How might we reduce this need? How can we not only cut down on travel for internal staff, but also help the editorial boards we work with reduce the need to fly? The clear answer here is just to fly less. I’m not sure that AR/VR are going to be a huge disruptor, but they could be, they could be. It’s not a crazy thing for us to keep experimenting with these things.&lt;/p&gt;
&lt;p&gt;Paper remains a bit factor in our footprint. Let’s go online only already. (I just don’t understand the print side of our industry, and I keep forgetting about it, but there are a lot of atoms and not just bits involved here. Perhaps we could start to tell stakeholders about the carbon price of their print editions, perhaps thin information would be another nudge to get them to go e-only?&lt;/p&gt;
&lt;p&gt;We do a lot of compute. We are all excited about AI. AI is compute intensive. It’s probably not a significant % of the kind of compute we do as an industry, which is probably dominated by maintaining databases, and by production workflows, but AI is compute intensive. Are there ways to move to shared models that could be built that cover the broad use-cases that are of interest to us, and that could make a big dent in the cost of building the same, or similar, models over and over again?&lt;/p&gt;
&lt;p&gt;Of course, we help to publish the damn research. What could we do to help make this key work more sticky for consumption by the general public? We had a great chat about whether AI could help with summarisation in a way that easy to digest versions of articles could be created in a cost effective way? Say for example by training a language model on simplified-english wikipedia, and passing abstracts or full texts through this model? That’s one approach to the idea, with a bit of thought I am certain we could come up with others.&lt;/p&gt;
&lt;p&gt;At least for now though, I think we should all take this away to our own organisations. Is this being debated internally? Are there action groups? Is it making its way into strategic planning decisions? How can we make that happen? Who should we talk to in our organisations? I’m going to start by forwarding this article to some of the planning groups I’m involved in within SAGE. We know and accept that climate change is a social justice issue, and we know that lowering our carbon footprint has a benefit to our bottom line, we just, I think, need to keep pushing on this to allow ourselves to get continuously better at addressing this issue.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>STM Research data workshop.</title>
      <link>http://scholarly-comms-product-blog.com/2019/12/06/stm_research_data_workshop._/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2019/12/06/stm_research_data_workshop._/</guid>
      <description>&lt;p&gt;The start of December is always a busy time for news in the STM / Product space. There is the annual STM meeting in London, and AWS re-invent also kicks off this week. As a result, within just a few days, I find that I have more things to write about than I can ever possibly have time to get through before the end of the year, we must plough on, and plough on we will.&lt;/p&gt;
&lt;p&gt;This post is about a half day workshop that I attended on Monday on the topic of research data sharing, and the role that publishers can play in this. The STM society is going to have a focus on research data in 2020. They say:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As part of STM’s mission to support Open Science and Research Reproducibility, STM will commence an action-driven initiative over the next year to promote the transparency, availability, linking, and proper citation of research data in scholarly communication.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is being led by Joris van Rossum.&lt;/p&gt;
&lt;p&gt;The workshop mainly involved updates on what has, or hasn&amp;rsquo;t worked for different stakeholders around promoting research data sharing.&lt;/p&gt;
&lt;p&gt;If I were to summarise very briefly, then here are my main takeaways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implement a data policy for your journals.&lt;/li&gt;
&lt;li&gt;Try to mandate data availability statements.&lt;/li&gt;
&lt;li&gt;Further to that, try to mandate data availability statements that require links, where the data is available, rather than statements about “data being available upon request”, which usually means that the data is not available upon request.&lt;/li&gt;
&lt;li&gt;There remains skepticism in many fields about data sharing (engineering and chemistry), but hey, better get in now, before your funders demand this, and you have to scrabble.&lt;/li&gt;
&lt;li&gt;That said, growth in publications with links to data is strong (21% year on year growth to a current annual rate of about 60K publications annually).&lt;/li&gt;
&lt;li&gt;Growth in papers with a data availability statement is positively exploding.&lt;/li&gt;
&lt;li&gt;If you deposit good metadata about links to data repositories, CrossRef will auto-filter these for you, to make it easy to see these connections, using the event data infrastructure (good to see that having a real use-case!).&lt;/li&gt;
&lt;li&gt;If you were so inclined, and wanted to create your own data linking hub, then use the scholix standard.&lt;/li&gt;
&lt;li&gt;Most publishers consider themselves to be at the start of this journey (that means we should have no embarrassment about just getting started, as everyone can only look better from here on out!).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It was clear that there are lots of great pieces of current infrastructure that support data to object linking. It’s almost as if having shared infrastructure and standards that can be flexible enough to support emerging scholarly norms, and the needs of researchers, funders and publishers is a good thing, and that it might be a royal pain in the ass to re-write core pieces of this infrastructure to support edge cases, or a small number of stakeholder (just a thought).
It was also clear from the engagement in the room that many people are taking this seriously. This is great to see, after nearly a decade of observing conversations about this kind of thing, this kind of thing is now becoming a real thing.&lt;/p&gt;
&lt;p&gt;A personal highlight for me was seeing the utility of the JATS extensions to marking up citations as data citations. I participated in the group that led to that change. We imagined back then that publishers would flock to this standard. OK, so that didn’t happen then, but c’mon, it’s starting to happen now!&lt;/p&gt;
&lt;p&gt;Big shout-outs go to Taylor &amp;amp; Francis, Elsevier, Wiley and SpingerNature who are all doing great work to support this kind of linking.&lt;/p&gt;
&lt;p&gt;I was most impressed by the work that Taylor &amp;amp; Francis have done to support the adoption of data citation and data linking. They have done a lot of work to make this easier for all stakeholders, and what they have done really represents a great shopping list for anyone thinking about what it might take to implement really well. They:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;have a 5-tier data policy&lt;/li&gt;
&lt;li&gt;run workshops for authors (SpringerNature do this too, as I am sure do others).&lt;/li&gt;
&lt;li&gt;check papers that have data to ensure they cite the data&lt;/li&gt;
&lt;li&gt;give guidance on what repository authors should use&lt;/li&gt;
&lt;li&gt;provide guiding information on how to create data availability statements&lt;/li&gt;
&lt;li&gt;a catch-all email inbox to handle queries about data&lt;/li&gt;
&lt;li&gt;updates to their publishing platform and submissions platforms&lt;/li&gt;
&lt;li&gt;typesetters with specific instructions on how to handle data&lt;/li&gt;
&lt;li&gt;implementing open science badges&lt;/li&gt;
&lt;li&gt;creating author guides in different languages, especially in Chinese.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The two big issues in my mind are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;there are lots of patterns for data policies. Do you follow TOP guidelines, do you have two, three, four, five, six or more levels? We have all been finding our way. If you are getting started, just follow the &lt;a href=&#34;https://cos.io/top/&#34;&gt;TOP Guidelines&lt;/a&gt;!&lt;/li&gt;
&lt;li&gt;metadata infrastructure is still creaky (I mean, it&amp;rsquo;s always going to be, but this means that there is a real cost to implementing this stuff). If we could get vendors to agree and share on good practice about how to identify links to data sets, and how to tag them appropriately, then this could become a turn-key ask by publishers, and that would accelerate adoption.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Overall I think things are really moving in the right direction. It was a great workshop, and I’d like to thank Joris and Eefke Smit for running it!&lt;/p&gt;
&lt;h3 id=&#34;some-related-and-interesting-links&#34;&gt;Some related and interesting links.&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://fairsharing.org&#34;&gt;FAIRsharing&lt;/a&gt; were at the meeting. They curate and host a registry of data repositories and data policies, and make it possible for you to determine which repositories match which policies. This is a fantastic resource.&lt;/p&gt;
&lt;p&gt;Chris Graf from Wiley mentioned this preprint &lt;a href=&#34;https://www.authorea.com/users/260319/articles/399874-the-open-data-challenge-an-analysis-of-124-000-data-availability-statements-and-an-ironic-lesson-about-data-management-plans?commit=ad98ddb9a1e072174f34e0c5ae88de32c8431409&#34;&gt;The open data challenge: An analysis of 124,000 data availability statements, and an ironic lesson about data management plans - Authorea&lt;/a&gt;. They saw a huge uptick in papers with a DAS in 2019, after requiring it in more of their journals. Now about 120K papers have such a statement, but with large growth.&lt;/p&gt;
&lt;p&gt;Many researchers are now writing data management plans, and there are tools out there to support this (e.g. &lt;a href=&#34;https://dmponline.dcc.ac.uk&#34;&gt;DMPonline&lt;/a&gt; and &lt;a href=&#34;https://www.google.com/search?client=safari&amp;amp;rls=en&amp;amp;q=DMP+tool+CDL&amp;amp;ie=UTF-8&amp;amp;oe=UTF-8&#34;&gt;DMP tool CDL - Google Search&lt;/a&gt;). It would be great if these tools could also provide template DAS’s for authors!&lt;/p&gt;
&lt;p&gt;If you are interested in machine learning models that can connect data to papers, the rich context project from the Coleridge initiative is running a competition to build such models. We didn’t discuss this approach in the workshop, but it might be worth looking at in a followup workshop. You can read about the last iteration of the comp here: &lt;a href=&#34;https://coleridgeinitiative.org/richcontextcompetition&#34;&gt;The Coleridge Initiative&lt;/a&gt; and the next iteration will be announced soon.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>STM Research data workshop.</title>
      <link>http://scholarly-comms-product-blog.com/2019/12/05/stm_research_data_workshop._/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2019/12/05/stm_research_data_workshop._/</guid>
      <description>&lt;p&gt;The start of December is always a busy time for news in the STM / Product space. There is the annual STM meeting in London, and AWS re-invent also kicks off this week. As a result, within just a few days, I find that I have more things to write about than I can ever possibly have time to get through before the end of the year, we must plough on, and plough on we will.&lt;/p&gt;
&lt;p&gt;This post is about a half day workshop that I attended on Monday on the topic of research data sharing, and the role that publishers can play in this. The STM society is going to have a focus on research data in 2020. They say:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As part of STM’s mission to support Open Science and Research Reproducibility, STM will commence an action-driven initiative over the next year to promote the transparency, availability, linking, and proper citation of research data in scholarly communication.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is being led by Joris van Rossum.&lt;/p&gt;
&lt;p&gt;The workshop mainly involved updates on what has, or hasent worked for different stakeholders around promoting research data sharing.&lt;/p&gt;
&lt;p&gt;If I were to summarise very briefly, then here are my main takeaways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implement a data policy for your journals.&lt;/li&gt;
&lt;li&gt;Try to mandate data availability statements.&lt;/li&gt;
&lt;li&gt;Further to that, try to mandate data availability statements that require links, where the data is availalbe, rather than statements about “data being available upon request”, which usually means that the data is not available upon request.&lt;/li&gt;
&lt;li&gt;There remains skepticism in many fields about data sharing (engineering and chemistry), but hey, better get in now, before your funders demand this, and you have to scrabble.&lt;/li&gt;
&lt;li&gt;That said, growth in publications with links to data is strong (21% year on year growth to a current annual rate of about 60K publications annually).&lt;/li&gt;
&lt;li&gt;Growth in papers with a data avilability statment is positivly exploding.&lt;/li&gt;
&lt;li&gt;If you deposit good metadata about links to data repositoryes, CrossRef will auto-filter these for you, to make it easy to see these connections, using the event data infrastructure (good to see that having a real use-case!).&lt;/li&gt;
&lt;li&gt;If you were so inclined, and wanted to create your own data linking hub, then use the scholix standard.&lt;/li&gt;
&lt;li&gt;Most publishers consider themselves to be at the start of this journey (that means we should have no embarrassment about just getting started, as everyone can only look better from here on out!).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It was clear that there are lots of great pieces of current infrastructure that support data to object linking. It’s almost as if having shared infrastructure and standards that can be flexible enough to support emerging scholarly norms, and the needs of researchers, funders and publishers is a good thing, and that it might be a royal pain in the ass to re-write core pieces of this infrastructure to support edge cases, or a small number of stakeholder (just a thought).
It was also clear from the engagement in the room that many people are taking this seriously. This is great to see, after nearly a decade of observing conversations about this kind of thing, this kind of thing is now becoming a real thing.&lt;/p&gt;
&lt;p&gt;A personal highlight for me was seeing the utiltiy of the JATS extensions to marking up citations as data citations. I participated in the group that led to that change. We imagined back then that publishers would flock to this standard. OK, so that didn’t happen then, but c’mon, it’s starting to happen now!&lt;/p&gt;
&lt;p&gt;Big shoutouts go to Taylor &amp;amp; Francis, Elsevier, Wiley and SpingerNature who are all doing great work to support this kind of linking.&lt;/p&gt;
&lt;p&gt;I was most impressed by the work that Taylor &amp;amp; Francis have done to support the adoption of data ciation and data linking. They have done a lot of work to make this easier for all stakeholders, and what they have done really represents a great shopping list for anyone thinking about what it might take to implement really well. They:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;have a 5-tier data policy&lt;/li&gt;
&lt;li&gt;run workshops for authors (SpringerNature do this too, as I am sure do others).&lt;/li&gt;
&lt;li&gt;check papers that have data to ensure they cite the data&lt;/li&gt;
&lt;li&gt;give guidance on what repositories authors shold use&lt;/li&gt;
&lt;li&gt;provide guiding information on how to create data availability statements&lt;/li&gt;
&lt;li&gt;a catch-all email inbox to handle queries about data&lt;/li&gt;
&lt;li&gt;updates to their publishing platform and submissions platforms&lt;/li&gt;
&lt;li&gt;typesetters with specific instructions on how to handle data&lt;/li&gt;
&lt;li&gt;implementing open science badges&lt;/li&gt;
&lt;li&gt;creating author guides in different languages, especially in Chinese.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The two big issues in my mind are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;there are lots of patterns for data policies. Do you follow TOP guidelines, do you have two, three, four, five, six or more levels? We have all been finding our way. If you are getting started, just follow the &lt;a href=&#34;https://cos.io/top/&#34;&gt;TOP Guidelines&lt;/a&gt;!&lt;/li&gt;
&lt;li&gt;metadata infrastructure is still creaky (I mean, it&amp;rsquo;s always goig to be, but this means that there is a real cost to implementing this stuff). If we could get vendors to agree and share on good practice about how to identify links to data sets, and how to tag them appropriately, then this could become a turn-key ask by publishers, and that would accelerate adoption.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Overall I think things are really moving in the right direction. It was aa great workshop, and I&amp;rsquo;d like to thank Joris and Eefke Smit for running it!&lt;/p&gt;
&lt;h3 id=&#34;some-related-and-interesting-links&#34;&gt;Some related and interesting links.&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://fairsharing.org&#34;&gt;FAIRsharing&lt;/a&gt; were at the meeting. They curate and host a registry of data repositories and data policies, and make it possible for you to determine which repositories match which policies. This is a fantastic resource.&lt;/p&gt;
&lt;p&gt;Chris Graf from Wiley mentioned this preprint &lt;a href=&#34;https://www.authorea.com/users/260319/articles/399874-the-open-data-challenge-an-analysis-of-124-000-data-availability-statements-and-an-ironic-lesson-about-data-management-plans?commit=ad98ddb9a1e072174f34e0c5ae88de32c8431409&#34;&gt;The open data challenge: An analysis of 124,000 data availability statements, and an ironic lesson about data management plans - Authorea&lt;/a&gt;. They saw a huge uptick in papers with a DAS in 2019, after requiring it in more of their journals. Now about 120K papers have such a statement, but with large growth.&lt;/p&gt;
&lt;p&gt;Many researchers are now writing data managment plans, and there are tools out there to suppor this (e.g. &lt;a href=&#34;https://dmponline.dcc.ac.uk&#34;&gt;DMPonline&lt;/a&gt; and &lt;a href=&#34;https://www.google.com/search?client=safari&amp;amp;rls=en&amp;amp;q=DMP+tool+CDL&amp;amp;ie=UTF-8&amp;amp;oe=UTF-8&#34;&gt;DMP tool CDL - Google Search&lt;/a&gt;). It would be great if these tools could also provide template DAS’s for authors!&lt;/p&gt;
&lt;p&gt;If you are interested in machine learning models that can connect data to papers, the rich context project from the Coleridge initiative is running a competition to build such models. We didn’t discuss this approach in the workshop, but it might be worth looking at in a followup workshop. You can read about the last iteration of the comp here: &lt;a href=&#34;https://coleridgeinitiative.org/richcontextcompetition&#34;&gt;The Coleridge Initiative&lt;/a&gt; and the next iteration will be announced soon.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Google data set search.</title>
      <link>http://scholarly-comms-product-blog.com/2019/11/19/google_data_set_search._/</link>
      <pubDate>Tue, 19 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2019/11/19/google_data_set_search._/</guid>
      <description>&lt;p&gt;I’ve just got back from a fantastic workshop looking at infrastructure for research data discovery. I’ll blog about the workshop in due course, but I was asked to comment about Google Dataset Search - &lt;a href=&#34;https://toolbox.google.com/datasetsearch&#34;&gt;Dataset Search&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I had the change to meet with Natasha Noi from Google who is behind the service. &lt;a href=&#34;https://ai.google/research/people/NatalyaNoy/&#34;&gt;Natasha Noy – Google AI&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As with many google services, it has been created by a small team, but with the underlying web scale infrastructure of Google to build on top of. They look for data sets on the web that have been identified using &lt;a href=&#34;https://schema.org&#34;&gt;Home - schema.org&lt;/a&gt; tags. Data repositories that expose these tags will get indexed by google dataset search (this includes both Figshare and DataDryad).&lt;/p&gt;
&lt;p&gt;Natasha said the following about the service:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There are thousands of data repositories on the Web, providing access to millions of datasets. Google’s  &lt;a href=&#34;https://urldefense.proofpoint.com/v2/url?u=http-3A__g.co_datasetsearch&amp;amp;d=DwMFaQ&amp;amp;c=slrrB7dE8n7gBJbeO0g-IQ&amp;amp;r=omwcNBUqPba9pikmkXZXk2bFQ7zxZPhI5OH9dd8lFDA&amp;amp;m=AFklQ01ByxTiruuR_qSMkwBS-yjywt47u9stg6e4iCQ&amp;amp;s=Fk_6LE_tXUImuSF9yYHnJvRPH8uRSa4rlwJrInQHXm8&amp;amp;e=&#34;&gt;Dataset Search&lt;/a&gt;  provides search capabilities over dataset descriptions (metadata) in all of these repositories, enabling the users to find datasets, wherever they are. You can think of Dataset Search as “Google Scholar for datasets”. Dataset Search includes open-government datasets from many local and federal governments across the globe, a large number of repositories for scientific data, economics data, data for machine learning, and so on. It includes both commercial and noncommercial providers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While any other group &lt;em&gt;could&lt;/em&gt; in principle replicate this by extracting all schema related pages from something like common crawl, in reality there are probably only a handful of other organisations that might feel like they have a mandate or the resources to look at doing something like this. I could imagine Archive.org, Microsoft or Allen.ai having a crack at something like this. In the meantime, google dataset search is there, and it’s great that someone is doing this.&lt;/p&gt;
&lt;p&gt;Peter Kraker has been vocal in his concerns about this project (&lt;a href=&#34;https://science20.wordpress.com/2019/03/28/update-on-the-dontleaveittogoogle-campaign/&#34;&gt;Update on the #DontLeaveItToGoogle campaign | Science and the Web&lt;/a&gt;). While I agree with Peter that there should be better support for the research and discovery infrastructures, I’m not opposed to the existence of google data set search. I had a chance to talk to Peter briefly about his views on this at the Force2019 conference, and my takeaway from that discussion is that he was less concerned about the potential for Google to capture researcher attention (I mean surfacing these data sets is broadly a good thing) but was a more concerned that the existence of something like google dataset search potentially acts as a suppressor of research and innovation support. The argument could go — well GDS exists, so why should we fund anything like it? That is a fair point, but the overall landscape of how we allocate funding decisions is a complex one. As much as we might with things to be different, we can’t wish away the existence of web scale players.&lt;/p&gt;
&lt;p&gt;What I like about what Natasha is doing is that she is asking for repositories to use schema.org, which is an open standard, and which, as I mentioned above, opens the door for others to replicate what they have done. A project like Hyphe - &lt;a href=&#34;https://hyphe.medialab.sciences-po.fr&#34;&gt;Hyphe&lt;/a&gt; - shows that there are interesting open tools out there to address crawling, but in the end we are left with having to compete with researcher attention, so I’m not totally convinced by the argument that were GDS not to exist we would be in a place to create an alternative that would gain both funding and researcher attention. I say that due to the very large amounts of scar tissue that I carry around from many years of trying to get academics to draw their attention to the tools that I have had the opportunity to work on.&lt;/p&gt;
&lt;p&gt;One note of caution that I would strike is that how we chose what represnets a trust metric, and how we report back how data is being used are both increasingly critical aspects of the impact discussion, and are beginning to be built into how researchers are being assessed. I don’t know anything about how dataset creators get any indication of whether their datasets are being used from GDS, but as the community starts to create standards and norms around this I think its important to get all players to adopt those standards and norms.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>rescognito - a route towards a weighted scholarly graph.</title>
      <link>http://scholarly-comms-product-blog.com/2019/11/13/rescognito_-_a_route_towards_a_weighted_scholarly_graph./</link>
      <pubDate>Wed, 13 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2019/11/13/rescognito_-_a_route_towards_a_weighted_scholarly_graph./</guid>
      <description>&lt;p&gt;I sat down with Richard Wynne. Richard has a long career in the publishing space, having been one of the senior team at Aries for a long time.&lt;/p&gt;
&lt;p&gt;Over the last year Richard has founded &lt;a href=&#34;https://rescognito.com&#34;&gt;Rescognito&lt;/a&gt;. Rescognito is a service to allow institutions or individuals to award credits (or &lt;code&gt;rescogs&lt;/code&gt; ) to anyone for anything within the scholarly ecosystem. You can reward article contributions, reviews, presentations, knowledge, providing open data. The schema is extendible, so if you have a particular use-case in mind it could be accommodated.  The key thing is that all of the awarding is pinned on top of the persistent identifier graphs. Everyone involved has to have an ORCID. You can see my own page here: &lt;a href=&#34;https://rescognito.com/0000-0002-6754-1421#&#34;&gt;Rescognito - Ian Mulvany&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Initially the underlying model is that the service will sell a bag of credits to institutions, and when an institution awards someone for something their credits will have a different value to those awarded by individuals. This provides a way to see “trusted” claims along with “normal” claims, while at the same time providing an instant monetisation framework for the service.&lt;/p&gt;
&lt;p&gt;I was initially skeptical because what is going on here is that for this service to work everyone has to invest in beliveing that these tokens have real value, and until everyone does, they don’t.&lt;/p&gt;
&lt;p&gt;However, there are a number of other ways to look at what Richard is building here, and it&amp;rsquo;s been interesting to stay in touch with him over the last few months and observe the ongoing evolution of the service.&lt;/p&gt;
&lt;p&gt;For example one way to look at this is that by combining different kinds of recognition, and being able to weight contributions, one can start to build out a directed graph of recognition and contribution. Instead of having to get over the cold start one could take a network of interactions that was of interest to a stakeholder (grants awarded, or data deposited) and using the weighted graph one could start to use something like the Page rank algorithm to find key loci in the network. For a paper or individual you could also start to provide insight into weighted contributions by contribution type. If we truly wanted to support a diverse research ecosystem something like this could be a valuable tool for funders to determine whether their values are being realised in practice. You can get a sense of this from the following view: &lt;a href=&#34;https://rescognito.com/doiVisualization.php?doi=10.12688/f1000research.19585.1&#34;&gt;Rescognito&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Another thing that is interesting here is that everything is built on top of the PID graph. This means that there is an opportunity for rescognito to offer the infrastructure to run services that are wired together with persistent identifiers, where it might be challenging to create that infrastructure on a per service level in a distributed way. The key thing is that if the claims that you want to provide services for can be described as relationships between persistent identifiers, then rescognito could make those claims operable through a PID-driven API without a huge amount of extra work on top of what has already been built. For example let’s imagine that open review was a demand of certain funders, and different funders had different requirements for what they mean by open review, then if rescognito could name these differences, then it could offer these things as services to publishers, building once, and meaning that each publisher would not have to build out each different use case themselves.&lt;/p&gt;
&lt;p&gt;Richard himself says:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Rescognito is a great way for journal publishers to rapidly and efficiently collect data throughout the research/publication lifecycle &lt;strong&gt;without&lt;/strong&gt; modifying workflow or adding to back-office administration.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You can learn more from this webinar: &lt;a href=&#34;https://www.youtube.com/watch?v=6RGf2g8DmDY&#34;&gt;Structured Recognition via Rescognito &amp;amp; ORCID - YouTube&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Overall I’ve been impressed by what Richard has succeeded in building out in a short time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Where is research going - a Kudos report</title>
      <link>http://scholarly-comms-product-blog.com/2019/11/12/where_is_research_going_-_a_kudos_report/</link>
      <pubDate>Tue, 12 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2019/11/12/where_is_research_going_-_a_kudos_report/</guid>
      <description>&lt;p&gt;I’ve finally gotten around to looking at the report that Kudos developed earlier this year looking into where research outputs go to, and where they get consumed, after they have been published.&lt;/p&gt;
&lt;p&gt;You get grab a copy of the report here &lt;a href=&#34;https://info.growkudos.com/upstream-dl&#34;&gt;How to build a global, engaged audience for your research.&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The report is based on a survey of 10k researchers, supported with interviews and desk research.&lt;/p&gt;
&lt;p&gt;It’s a short read (19 pages), so go ahead and grab the report and have a look.&lt;/p&gt;
&lt;p&gt;Some key take-aways for me are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;only 5% of academics thought that they didn’t need to demonstrate the broader impact of their work. OK, that’s not surprising, but I would love to find out who those people are. More surprising is the claim that HSS researchers feel that this is less important to them than STM researchers. They claim significantly so. That’s just puzzling to me, and I could make some guesses as to why, but I’m not going to venture a guess now.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Institutions are struggling to keep up with the growing level of support that researchers require around communications, engagement and impact, and there is a big gap in the market here.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;About 40% of funders require an impact plan. Just under 30% require data to be open.&lt;/li&gt;
&lt;li&gt;Slide 15 is the kicker. Researchers need lots of support in doing things like developing websites to communicate their work, and yet feel that they don’t get this support. There is money from funders to support some of this work.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The key implication from this report is that there might be a market opportunity in helping researchers create other kinds of published online outputs for their work, that are not just research articles. That’s a huge field of product opportunity to be playing in, and how one decides to try top into it is very open ended.&lt;/p&gt;
&lt;p&gt;Perhaps APC’s could cross fund a micro-website for each research publication, that used summarisation technology to create a skeleton outline for how to describe the broader impact that the work has?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Rave tech conference 2019</title>
      <link>http://scholarly-comms-product-blog.com/2019/11/11/rave_tech_conference_2019_/</link>
      <pubDate>Mon, 11 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2019/11/11/rave_tech_conference_2019_/</guid>
      <description>&lt;p&gt;A few weeks ago I was at the aannual Rave publishing technology conference. It&amp;rsquo;s always an interesting event to attend.&lt;/p&gt;
&lt;p&gt;My main recollection from last year was the interest in blockchain.&lt;/p&gt;
&lt;p&gt;This year, as I reflect on how I feel about the meeting, I think I have two main things that have stayed with me.&lt;/p&gt;
&lt;p&gt;The first was the appeal from Tasha to ask us as a community and an industry to do more to think about removing barriers for early career researchers. This touches on a long held challenge that I think we have as an industry which is that our systems are creaky and a bit resistant to change.&lt;/p&gt;
&lt;p&gt;That led to the other impression that I had from the meeting. There was a call to embrace polyglot programming, but pushback from people in the audience where there seemed to be a weary note of some slight resignation that technological transformation can be challenging to achieve.&lt;/p&gt;
&lt;p&gt;Here are some notes on some of the presentations from the day:&lt;/p&gt;
&lt;h3 id=&#34;david-smith---the-information-and-the-network&#34;&gt;David Smith - the information and the network.&lt;/h3&gt;
&lt;p&gt;Interestingly a lot of this discussion touches on topics of computational social science.&lt;/p&gt;
&lt;p&gt;I’ll try to distill the main points here.&lt;/p&gt;
&lt;p&gt;The gist of the talk is that the way information flows in existing networks can be highly biased. This has been demonstrated by a number of recent academic theoretical studies, as well as specific examples in the news recently.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Boris Bus stories&lt;/li&gt;
&lt;li&gt;Boris model stories&lt;/li&gt;
&lt;li&gt;Facebook allowing any content into political ads&lt;/li&gt;
&lt;li&gt;The measles mumps and rubella scare, and how global dissemination of this scare has been amplified due to you tube.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;David suggests that we need a vaccine for this kind of misinformation propagation. We need this to be tied to regulation. We also need to understand the world as it is, rather than hoping that things be different.&lt;/p&gt;
&lt;p&gt;I’ll just point to SAGE Ocean here as an example of what one publisher is doing about this. (&lt;a href=&#34;https://www.google.com/search?client=safari&amp;amp;rls=en&amp;amp;q=sage+ocean&amp;amp;ie=UTF-8&amp;amp;oe=UTF-8&#34;&gt;sage ocean - Google Search&lt;/a&gt;), and in particular our amazing talk series &lt;a href=&#34;https://ocean.sagepub.com/events&#34;&gt;https://ocean.sagepub.com/events&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are things that we could be doing better in our industry for finding fake research articles by sharing data that we have about our submissions, but that starts to cross the line into editorial decisions by fiat, and not only does that need a solid ethical background, it’s also hard and expensive.&lt;/p&gt;
&lt;h3 id=&#34;how-do-publishers-and-technology-providers-need-to-change-to-better-serve-early-career-researchers-by-tasha-mellins-cohen-director-of-publishing-at-microbiology-society&#34;&gt;How do Publishers and Technology Providers Need to Change to Better Serve Early Career Researchers?&amp;rsquo; by Tasha Mellins-Cohen, Director of Publishing at Microbiology Society&lt;/h3&gt;
&lt;p&gt;Tasha points out that post-doc contracts usually last only 18 months. This leads to crazy-levels of job instability. (This is a very big problem. It can lead to significant mental health issues in the sciences.).&lt;/p&gt;
&lt;p&gt;There is an identified lack of mentoring (reminds me of SAGE Advice, a product we tried to get off the ground that matched academics with people who could advise them on specific questions. We felt there was a market for this kind of service precisely because mentorship can be so shoddy in academia).&lt;/p&gt;
&lt;p&gt;As publishers are we really helping early career researchers to get things done right, or we just adding to their burdens?&lt;/p&gt;
&lt;p&gt;Tasha asks us to have an open discussion in the room about the topic of what do we do that causes a burden for ERC’s.&lt;/p&gt;
&lt;p&gt;There is a lot of discussion around format free submissions, but Tasha puts the gauntlet down and asks if there are ways that would allow them to go from research artefact direct to publication output. She also asks us to talk to them and commit to fixing some of the burdens that they are faced with every day.&lt;/p&gt;
&lt;h3 id=&#34;phil-jones--paul-mollahan---shared-code-and-shared-standards---how-openess-in-technology-enables-openess-in-scholarship&#34;&gt;Phil Jones &amp;amp; Paul Mollahan - shared code and shared standards - how openess in technology enables openess in scholarship.&lt;/h3&gt;
&lt;p&gt;Digerati have been involved in some open source projects in our space.&lt;/p&gt;
&lt;p&gt;They have worked on an open source image viewer built on top of the IIIF standard.&lt;/p&gt;
&lt;p&gt;They have also worked with eLife on some of their platform build.&lt;/p&gt;
&lt;p&gt;There is a discussion around where the value lies in terms of working with open source. Do you gain by giving your code away?&lt;/p&gt;
&lt;p&gt;In terms of development practice by working in an open way with open review practices you get more opportunities to get feedback from your peers.&lt;/p&gt;
&lt;p&gt;(My own view on this is that whether one is working open or closed source, the drive towards creating high quality software is primarily a cultural and tooling issue, but underpinning both of these there needs to be an understanding of the flows of value that is being created by that given software project. That value is what gives you the breathing space to be able to think about “building it right”. This comes down to sustainability.).&lt;/p&gt;
&lt;p&gt;I like the comment from the floor that points out that there needs to be an intersection between technology, platform and strategy. This alignment is often missing in any discussion around these topics.&lt;/p&gt;
&lt;h3 id=&#34;starting-the-transition-from-words-to-data---publishing-to-services---sharon-cooper---cdo-the-economist-intelligence-unit&#34;&gt;Starting the transition from words to data - publishing to services - Sharon Cooper - CDO the economist intelligence unit.&lt;/h3&gt;
&lt;p&gt;The EIU (Economist Intelligence Unit) is not a publisher, it is a forecasting unit, however they deliver this information out as reports, so they have sometimes felt like a publisher.&lt;/p&gt;
&lt;p&gt;They are now looking at a wide transformation of their business, but that requires engaging stakeholders across the business. How do you have conversations about data with people? If you can’t talk about data lakes because of organisational immaturity you can start small and think about “data puddles”. Asking questions like the following can be good starting points:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do you have data?&lt;/li&gt;
&lt;li&gt;Do you have the rights to use it?&lt;/li&gt;
&lt;li&gt;Is there any personal information in it?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They are very interested in geospatial data. By taking the shadow of an oil tanker from satellite data you can figure out how much oil is in a tanker. Suddenly you can predict where every drop of oil is in the world at any one time. (This is a stunning example of an accidental data source).&lt;/p&gt;
&lt;p&gt;There is a big shift going on in EIU to go from a reporting process that is one size fits all to reports that are highly customised for the business that are interested in it. One of the interesting challenges in this space here is that they are dealing with a very large diversity of sources of information.&lt;/p&gt;
&lt;p&gt;Instead of disseminating their analysis statically monthly they want to move towards doing nowcasting — being able to provide insight on the back of changing indicators, in close to real time.&lt;/p&gt;
&lt;p&gt;The example of what they are doing with location based data off of mobile phones in China Is impressive. This is as much about the quality of the data as it is about how to make the data useful through UX and UI.&lt;/p&gt;
&lt;p&gt;Nice point about things like data lakes being now available as a service.&lt;/p&gt;
&lt;p&gt;Flexible platforms are key, clients will want data delivered, and will not want to go to your site to get that data. APIs and micro services are key here.&lt;/p&gt;
&lt;p&gt;Data science is going to be a big part of what they do. They want to move towards augmented intelligence.&lt;/p&gt;
&lt;p&gt;The want to create an EIU knowledge Graph based on the AI analysis of the content and data that they are ingesting. They think their customers are not buying the data they can deliver, but are buying the interpretation that they can provide.&lt;/p&gt;
&lt;p&gt;I LOVE the idea of the minimal sellable product.&lt;/p&gt;
&lt;h3 id=&#34;sarah-boyd---senior-product-manager---emerald-publishing---update-of-rolling-out-the-scaled-agile-framework&#34;&gt;Sarah Boyd - senior product manager - Emerald publishing - update of rolling out the scaled agile framework.&lt;/h3&gt;
&lt;p&gt;The story so far is that moving towards delivery involved putting in place a scaled agile framework, but after a number of reorganisations they had still not delivered a single sprint. They tried to copy the Spotify squad model (reminds me of &lt;a href=&#34;https://vimeo.com/240125835&#34;&gt;Joakim Sundén: You can do better than the Spotify Model #lascot on Vimeo&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;They felt that they had lost sight of the purpose of what they were doing. The purpose was not to create an agile methodology.&lt;/p&gt;
&lt;p&gt;They went back to what looks like smaller teams with planning, development and delivery more tightly aligned.&lt;/p&gt;
&lt;p&gt;Without seeing in detail how the first structure failed to deliver its hard to infer what lessons to take from this discussion.&lt;/p&gt;
&lt;h3 id=&#34;heather-st-pierre-product-director---chelsey-horstmann-dennis-product-analyst---prioduct-management-tf&#34;&gt;Heather St. Pierre Product Director - Chelsey Horstmann Dennis Product Analyst. - Prioduct Management T&amp;amp;F.&lt;/h3&gt;
&lt;p&gt;T&amp;amp;F have scaled up the PM function over the last 18 months.&lt;/p&gt;
&lt;p&gt;Things are changing in the industry. OA has been a big change.&lt;/p&gt;
&lt;p&gt;Most of the talk is about the problems of payment and submissions.
T&amp;amp;F have created a submissions portal.&lt;/p&gt;
&lt;p&gt;The submissions portal connects to information about their “transformative deals”.&lt;/p&gt;
&lt;p&gt;They are offering format-free submissions on 340 titles, and this has allowed them to create an article transfer system.&lt;/p&gt;
&lt;p&gt;They can give the author clarity around the location of their submission within the system.&lt;/p&gt;
&lt;p&gt;They don’t have data yet on whether the system has led to more transfers. It was built with them and RAVE.&lt;/p&gt;
&lt;h3 id=&#34;astrid-engelen-visser---the-strategy-of-transforming-a-small-business&#34;&gt;Astrid Engelen-Visser - the strategy of transforming a small business&lt;/h3&gt;
&lt;p&gt;This is a nice broad overview of how a small publisher has addressed the challenges of technical and product development.&lt;/p&gt;
&lt;p&gt;Of interest to me is that they have created their own submission system for some specific journals.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unpaywall Journals - possibly the most interesting thing to happen this year in library subscription land.</title>
      <link>http://scholarly-comms-product-blog.com/2019/11/06/unpaywall_journals_-_possibly_the_most_interesting_thing_to_happen_this_year_in_library_subscription_land._/</link>
      <pubDate>Wed, 06 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2019/11/06/unpaywall_journals_-_possibly_the_most_interesting_thing_to_happen_this_year_in_library_subscription_land._/</guid>
      <description>&lt;p&gt;Heather and Jason from &lt;a href=&#34;https://ourresearch.org/&#34;&gt;https://ourresearch.org/&lt;/a&gt; have just released a preview of their new tool - &lt;code&gt;unpaywall journals&lt;/code&gt;. You can have a look at the preview of this tool now - &lt;a href=&#34;https://journals.unpaywall.org/&#34;&gt;Unpaywall Journals&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;They previewed this two weeks ago at &lt;a href=&#34;http://force2019.sched.com&#34;&gt;FORCE2019&lt;/a&gt; and have clearly gone through a ton of work to get the tool the state it is in todday, so big congraatulations to them on the product release.&lt;/p&gt;
&lt;p&gt;For those of you not in the know they have a long track record of building useful open infrastructure in the scholarly communications spaace. The product that they hvae built to date that has probably had the biggest impact is &lt;a href=&#34;https://unpaywall.org&#34;&gt;Unpaywall&lt;/a&gt; which maps over 24M scholarly articles to open access versions of those articles. By using data from this tool they have recently been able to make what they believe are high resolution projections of the growth of different forms of open access for most journals in the world - paper here - &lt;a href=&#34;https://www.google.co.uk/search?q=bioarxive+piwower&amp;amp;ie=UTF-8&amp;amp;oe=UTF-8&amp;amp;hl=en-gb&amp;amp;client=safari&#34;&gt;bioarxive piwower - Google Search&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you were a librarian, and you knew how much of the a journal you are currently subscribing too were to become open access over the next five years, and furthermore you could match that data with the specific usage of that journal by your patrons, along with estimating how much it would cost you to get some of that content via inter library loans, then that might make it easy for you to decide whether to keep subscribing to that journal in the future. Now imagine you could do that in one moment for your entire collection, and have a tool that automatically modelled different scenarios for you. That’s what unpaywall journals is aiming to be.&lt;/p&gt;
&lt;p&gt;The tool released for preview this week has A LOT of functionality in it, but I’ll just show two screen shots that I grabbed that nicely summarise the gist of what is going on here.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s3.amazonaws.com/partially-attended-media-bucket/unpaywall-journals-1.png&#34; alt=&#34;Unpaywall journals - scenario cost summary&#34;&gt;&lt;/p&gt;
&lt;p&gt;This screenshot shows the view that gives you a rolled up summary of projected cost savings, based on canceling the journals (indicated by the grey dots on the right). The tool allows a ton of modelling to be rolled in to the projection, like % of expected inter library loan, expected cost growth of the current package deal.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s3.amazonaws.com/partially-attended-media-bucket/unpawall-journals-2.png&#34; alt=&#34;Unpaywall Journals single journal usage projection&#34;&gt;&lt;/p&gt;
&lt;p&gt;This screenshot shows how they project future expected usage for a single journal.&lt;/p&gt;
&lt;p&gt;There are many other aspect of the product, including future citation prediction, based on authors at the given institution.&lt;/p&gt;
&lt;p&gt;The bottom line here is that if librarians believe the modelling, then they will have powerful arguments to drive down the cost of their subscriptions.&lt;/p&gt;
&lt;p&gt;I think the tool looks powerful, and is the kind of thing that has been within reach of many people to be able to deliver on for some time now, but whether this tool will find a sustainable market I think will depend on two things. 1) if the main use case is for the tool to be used in licence negotiations by libraries then those who make the deals will have to both have confidence in the tool, the data and the modelling, as well as be convinced to change their behaviour based on this information. It will take some early adopters to be willing to talk about their experiences of using this in their negotiations for this tool to be on that becomes a must-have for libraries, and given the length and intricacy involved in content negotiations that lead time might be quite long. 2) if they can’t reach that market quickly enough then reconfiguring the cost and value proposition to sell into an adjacent use case will be critical. There is so much useful information in the tool that a more like that should be possible, but with such aa small team behind the tool even an adjacent shift might be difficult.&lt;/p&gt;
&lt;p&gt;I’m bullish on the chances of success of this, and am going to be keeping a close eye on how it progresses.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Responsible metrics - the state of the art - Elizabeth Gadd at Force2019</title>
      <link>http://scholarly-comms-product-blog.com/2019/11/04/responsible_metrics_-_the_state_of_the_art_-_elizabeth_gadd_at_force2019_/</link>
      <pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2019/11/04/responsible_metrics_-_the_state_of_the_art_-_elizabeth_gadd_at_force2019_/</guid>
      <description>&lt;p&gt;At Force2019 the other day the one session that I really wanted to see, but missed, was the one by Dr. Elizabeth Gadd on responsible metrics.&lt;/p&gt;
&lt;p&gt;She has posted her slides here &lt;a href=&#34;https://repository.lboro.ac.uk/articles/Responsible_metrics_what_s_the_state_of_the_art_/10003274/1&#34;&gt;Responsible metrics: what’s the state of the art?&lt;/a&gt;. This is a great deck, and I highly encourage reading through it.&lt;/p&gt;
&lt;p&gt;My takeaways from reading through them are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;misapplication of metrics is dangerous, leads to stress, has led to some tragic incidents.&lt;/li&gt;
&lt;li&gt;Misapplication of metrics just leads to bad decisions.&lt;/li&gt;
&lt;li&gt;Consider the “advise-police-judge” spectrum.&lt;/li&gt;
&lt;li&gt;Get senior leadership to own this.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;She introduces the inorms scope model &lt;a href=&#34;https://inorms.net/activities/research-evaluation-working-group/&#34;&gt;https://inorms.net/activities/research-evaluation-working-group/&lt;/a&gt;. This starts from your values and moves from there towards a model of evaluation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Beware using quantitative indicators as a proxy for qualitative things.&lt;/li&gt;
&lt;li&gt;Citations do not equal quality&lt;/li&gt;
&lt;li&gt;Ranking position does not equal excellence&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is advice to probe the potential negative effects of measurement, ahead of implementing a measurement practice.&lt;/p&gt;
&lt;p&gt;I love the image of the research evaluation food chain:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s3.amazonaws.com/partially-attended-media-bucket/assessment_foodchain.jpg&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt;
&lt;p&gt;I’ve been recently reading about outcomes, and using outcomes in preference to outputs. In this webinar on outcomes over outputs Josh Seiden discusses the topic in an approachable level of detail.
(&lt;a href=&#34;https://www.youtube.com/watch?feature=youtu.be&amp;amp;v=o5z4Dj1EhbQ&amp;amp;app=desktop&#34;&gt;Webinar: Outcomes over output with Josh Seiden - YouTube&lt;/a&gt;.). Looking at the state we are in, in research assessment, it looks like what is often being measured on are outputs over outcomes. Furthermore it also looks like the kinds of outputs that people are looking at are very one sided.  Josh talks about using two-sided outcomes to ensure that you don’t create incentives for building things that ultimatly have negative impacts on the business. An example he gives in the Q&amp;amp;A is about driving visits to a page. You might be able to spend a lot on driving traffic, but if you don’t check that the people landing on the page are having good interactions there, then you have created an outcome, but one that is ultimately futile.&lt;/p&gt;
&lt;p&gt;In research we often have a system that rewards citation, publication, awarding of grants, but rarely is self aware enough to be able to setup the rewards in a way that look at two-sided outcomes. I don’t think this is a way of looking at the world for which a specific product can be created. It takes nuance, and buy-in from the communities involved. There are going to be a lot of different people who have views on what the ultimate outcomes are that they are shooting for. I guess a good place to start is just by raising awareness, trying to make spaces where we can discuss these issues more.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Responsible metrics, the state of the art, Elizabeth Gadd at Force2019</title>
      <link>http://scholarly-comms-product-blog.com/2019/11/04/responsible_metrics_the_state_of_the_art_elizabeth_gadd_at_force2019_/</link>
      <pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2019/11/04/responsible_metrics_the_state_of_the_art_elizabeth_gadd_at_force2019_/</guid>
      <description>&lt;p&gt;At Force2019 the other day the one session that I really wanted to see, but missed, was the one by Dr. Elizabeth Gadd on responsible metrics.&lt;/p&gt;
&lt;p&gt;She has posted her slides here &lt;a href=&#34;https://repository.lboro.ac.uk/articles/Responsible_metrics_what_s_the_state_of_the_art_/10003274/1&#34;&gt;Responsible metrics: what’s the state of the art?&lt;/a&gt;. This is a great deck, and I highly encourage reading through it.&lt;/p&gt;
&lt;p&gt;My takeaways from reading through them are the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;misapplication of metrics is dangerous, leads to stress, has led to some tragic incidents.&lt;/li&gt;
&lt;li&gt;Misapplication of metrics just leads to bad decisions.&lt;/li&gt;
&lt;li&gt;Consider the “advise-police-judge” spectrum.&lt;/li&gt;
&lt;li&gt;Get senior leadership to own this.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;She introduces the inorms scope model &lt;a href=&#34;https://inorms.net/activities/research-evaluation-working-group/&#34;&gt;https://inorms.net/activities/research-evaluation-working-group/&lt;/a&gt;. This starts from your values and moves from there towards a model of evaluation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Beware using quantitative indicators as a proxy for qualitative things.&lt;/li&gt;
&lt;li&gt;Citations do not equal quality&lt;/li&gt;
&lt;li&gt;Ranking position does not equal excellence&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is advice to probe the potential negative effects of measurement, ahead of implementing a measurement practice.&lt;/p&gt;
&lt;p&gt;I love the image of the research evaluation food chain:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://s3.amazonaws.com/partially-attended-media-bucket/assessment_foodchain.jpg&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt;
&lt;p&gt;I’ve been recently reading about outcomes, and using outcomes in preference to outputs. In this webinar on outcomes over outputs Josh Seiden discusses the topic in an approachable level of detail.
(&lt;a href=&#34;https://www.youtube.com/watch?feature=youtu.be&amp;amp;v=o5z4Dj1EhbQ&amp;amp;app=desktop&#34;&gt;Webinar: Outcomes over output with Josh Seiden - YouTube&lt;/a&gt;.). Looking at the state we are in, in research assessment, it looks like what is often being measured on are outputs over outcomes. Furthermore it also looks like the kinds of outputs that people are looking at are very one sided.  Josh talks about using two-sided outcomes to ensure that you don’t create incentives for building things that ultimately have negative impacts on the business. An example he gives in the Q&amp;amp;A is about driving visits to a page. You might be able to spend a lot on driving traffic, but if you don’t check that the people landing on the page are having good interactions there, then you have created an outcome, but one that is ultimately futile.&lt;/p&gt;
&lt;p&gt;In research we often have a system that rewards citation, publication, awarding of grants, but rarely is self aware enough to be able to setup the rewards in a way that look at two-sided outcomes. I don’t think this is a way of looking at the world for which a specific product can be created. It takes nuance, and buy-in from the communities involved. There are going to be a lot of different people who have views on what the ultimate outcomes are that they are shooting for. I guess a good place to start is just by raising awareness, trying to make spaces where we can discuss these issues more.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Things holding back your analytics.</title>
      <link>http://scholarly-comms-product-blog.com/2019/11/04/things_holding_back_your_analytics._/</link>
      <pubDate>Mon, 04 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2019/11/04/things_holding_back_your_analytics._/</guid>
      <description>&lt;p&gt;One of my colleagues sent me over this short report “”&lt;a href=&#34;https://hbr.org/2017/06/3-things-are-holding-back-your-analytics-and-technology-isnt-one-of-them&#34;&gt;3 Things Are Holding Back Your Analytics, and Technology Isn’t One of Them&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;They think the following elements should be considered:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;How the analysis teams are structured - they need to report in a way that is understandable (i.e. not be too separate from the business), but at the same time be independent enough to provide unbiassed views.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You need to get the culture right, but to be honest the example given is almost information free.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Applicability of the models - the analytics team needs to create artefacts that can be used by the business.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They go on to recommend building an “”analysis nerve centre” with representation at C-Suite level - I guess where data can speak truth to power, or something. I do like their idea of creating “MVP” insight products, but this is advice is not much different from how one might approach other kinds of business improvements.&lt;/p&gt;
&lt;p&gt;I think their advice on how to structure teams is something you can take or leave. I’m reminded of a talk I saw last year at an O’Reilly conference on AI about how to build data science teams. The gist from that was that you can go for a centralised or cross-functional approach.&lt;/p&gt;
&lt;p&gt;The part though about jumping from a model to a useful model is critical, and that totally depends on the current capabilities of your organisation, the state of your data, and the lead time to be able to implement a decision.&lt;/p&gt;
&lt;p&gt;On reflection my advise would be to think about how quickly you can take any data insight and validate whether it has helped the business. Anything at all, at even a small scale, will help you understand the context that you are working in.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Are scholarly publishers technology companies?</title>
      <link>http://scholarly-comms-product-blog.com/2019/09/05/are_scholarly_publishers_technology_companies_/</link>
      <pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2019/09/05/are_scholarly_publishers_technology_companies_/</guid>
      <description>&lt;p&gt;There is tension in this question that gets to the heart of where a publisher should be putting its resources, and perhaps more importantly what the reasons are behind those investment decisions.&lt;/p&gt;
&lt;p&gt;The side of the argument that says they are not technology companies might say that at the heart of what publishers do is content, and so they are content and service companies. Invest then in acquisition, in reach, in marketing, in branding, in distribution and in making the sales process as cost effective asa possible.&lt;/p&gt;
&lt;p&gt;On the other side one can make the argument that the materiality of content is digital - online platforms, PDFs, COUNTER stats, appropriate indexing and discovery, publishing systems that can support fan-out model where we take many formats as input and distribute them back out in enhanced formats.&lt;/p&gt;
&lt;p&gt;I’ve just read the following blog post &lt;a href=&#34;https://www.ben-evans.com/benedictevans/2019/7/31/Netflix&#34;&gt;Netflix is not a tech company — Benedict Evans&lt;/a&gt;. I think it’s critical and it resonated strongly with me. Here are some key quotes&lt;/p&gt;
&lt;p&gt;Netflix is a TV company, not a technology company:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;… Netflix has used technology as a crowbar to build a new TV business. Everything about how it executed that technology has to be good. The apps are good, the streaming and compression are good, the UI is good, the recommendation engine is good, and the customer service and experience are good. Unlike American cable subscribers, Netflix subscribers are generally pretty happy with the tech. The tech has to be good - but, it’s still all about the TV.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You have to be good at technology, but it’s not enough:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It’s challenging to call things like user experience or indeed software commodities, especially when talking to people who work in software. These are certainly not &lt;em&gt;easy&lt;/em&gt;things to do, and when incumbents from other industries try to build them (‘we can just hire some techies!’) they often mess them up. But that doesn’t mean they‘re defensible, and it doesn’t mean they’re what determines success.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You have to understand what questions matter to your area of business&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Hulu is smaller than Netflix because of TV questions, not tech questions  …  this framing is important - ‘what &lt;em&gt;kind&lt;/em&gt; of questions matter for this business? … The more that we see new companies using software to create new businesses in industries outside of technology, the more generally this applies.  … executing this properly is not the same as defensibility. Selling online &lt;em&gt;per se&lt;/em&gt; - even selling online really well - is fundamentally a commodity. Hence, one asks whether there is something unique and defensible about this company’s online channel&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So what conclusions should be drawn for the scholarly publishing industry?&lt;/p&gt;
&lt;p&gt;We are indeed not a technology industry, but nonetheless we need to excel at a technology. &lt;a href=&#34;https://scholarlykitchen.sspnet.org/2019/08/20/guest-post-a-case-for-universal-and-simplified-journal-systems/?utm_source=feedburner&amp;amp;utm_medium=email&amp;amp;utm_campaign=Feed%3A+ScholarlyKitchen+%28The+Scholarly+Kitchen%29&#34;&gt;There are areas of the experience where we have fallen down.&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are some questions that I think will always be pertinent to our audience, and in each of these areas technology done well can help. I think those questions are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How quickly can a research article be published?&lt;/li&gt;
&lt;li&gt;Can reviews for the article be fair and responsive?&lt;/li&gt;
&lt;li&gt;Can the article reach the audience that needs it?&lt;/li&gt;
&lt;li&gt;Can the authors get credit for this item (and by this we usually mean citations) quickly?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think these are the ways to best serve the ecosystem, but from the perspective of the business there are other questions that support these. Whereas the Netflix model is to spend $15BN on buying new content to attract customers, in scholarly publishing we don’t think about “acquiring content” through purchase, but we do think about developing relationships with communities, in the expectation that those communities will chose to publish their work in the journals that we publish. From a defensibility perspective although the general market of journals is an open one, within a specific research discipline there are usually only a small number of journals that matter to that community.&lt;/p&gt;
&lt;p&gt;The operational aspects of running a journal are the entry ticket to having a successful business, things like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How to cap the cost of producing an article, while increasing the ability of that process to support the questions I laid out above.&lt;/li&gt;
&lt;li&gt;How to scale the costs of publishing infrastructure at a rate lower than the growth of content on the infrastructure.&lt;/li&gt;
&lt;li&gt;How to ensure contracts for publishing have renewals built in to support the customers need to forecast their budgets, while supporting growth of the business.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All of these kinds of questions though, are operational, and not related to defensibility.&lt;/p&gt;
&lt;p&gt;What then are the questions that should drive strategy, especially in an environment where national pay to publish deals look like the smart money for the future of economic flows in our industry?&lt;/p&gt;
&lt;p&gt;Answers on a postcard please!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Strategies to reduce cognitive load, and make systems more robust whilst doing so, and why that’s important for product development.</title>
      <link>http://scholarly-comms-product-blog.com/2019/09/05/strategies_to_reduce_cognitive_load_and_make_systems_more_robust_whilst_doing_so_and_why_thats_important_for_product_development._/</link>
      <pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2019/09/05/strategies_to_reduce_cognitive_load_and_make_systems_more_robust_whilst_doing_so_and_why_thats_important_for_product_development._/</guid>
      <description>&lt;p&gt;At my current company we are looking at strategies for improving the resilience of our core systems, and looking at the issue of disaster recovery from a broad perspective. This comes under the heading &lt;code&gt;business continuity management&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;From a product development perspective these considerations are also important.&lt;/p&gt;
&lt;p&gt;For successful products / product organisations consider these two perspectives:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Most new business value that we create from innovation projects  comes from improvements or iterations to existing products over the creation of totally new products (from an evolutionary perspective this makes sense, products or services that are already making revenue have proved they they have an environmental fitness function that works, whereas new products are like genetic modifications, the vast majority of which lead towards extinct endpoints. When you find an entirely new nice to exploit, or when the environment changes rapidly enough hat existing products are no longer sufficient, then radially new ideas can be hugely profitable, but for the most part, most of what we do is incremental and iterative, and this is mostly fine.).&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;We often underestimate the effort involved in maintenance and upkeep of new services. There can be an excitement about building something new, but budgeting for the maintenance and upkeep of the service or product is something that can get overlooked.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;OK, but what does business continuity management have to do with either of these perspectives? Well there is one aspect that I want to touch on today that can effect team continuity and project velocity and that is the issue of cognitive load. In both of the above scenarios what can help is finding ways to enable the team to move quickly, or at least to reduce the risk of making moves.&lt;/p&gt;
&lt;p&gt;When making iterations to existing products, being able to do so without breaking things is helpful.&lt;/p&gt;
&lt;p&gt;When looking to reduce the burden of ongoing maintenance for new services, finding ways to make those services robust from the get go can be helpful.&lt;/p&gt;
&lt;p&gt;From a &lt;code&gt;BCM&lt;/code&gt; perspective one of the biggest risks for software systems that we have built and maintain ourselves is the risk of losing implicit knowledge about the project when staff leave or when the project is handed over to other parts of the organisation, or where management for the service is outsources.&lt;/p&gt;
&lt;p&gt;Recently I’ve read some excellent posts about reducing the cognitive load of developing systems.&lt;/p&gt;
&lt;p&gt;In &lt;a href=&#34;https://techbeacon.com/app-dev-testing/forget-monoliths-vs-microservices-cognitive-load-what-matters&#34;&gt;https://techbeacon.com/app-dev-testing/forget-monoliths-vs-microservices-cognitive-load-what-matters&lt;/a&gt; Matthew Skelton and Manuel Pais explain why cognitive load is the right framework for thinking about the complexity of our systems, and that this thinning transcends the debate over monoliths vs micro services. They recommend working towards reducing the &lt;strong&gt;intrinsic&lt;/strong&gt; cognitive load of systems, e.g. through good choice of systems. You should eliminate &lt;strong&gt;extrinsic&lt;/strong&gt; cognitive load e.g. by automatic the boring tasks. This then leaves space for what they describe as &lt;strong&gt;germane&lt;/strong&gt; cognitive load - the space to think about how to solve the business problems that your software should be solving. They also have a very nice description of how to configure teams to improve the interaction patterns between teams.&lt;/p&gt;
&lt;p&gt;In &lt;a href=&#34;https://12devsofxmas.co.uk/2015/12/day-3-managing-cognitive-load-for-team-learning/&#34;&gt;Day 3: Managing Cognitive Load for Team Learning – 12 Devs of Xmas&lt;/a&gt; Jo Pearce describes the value of comments, applied appropriately, as well as the role that code review can play.&lt;/p&gt;
&lt;p&gt;In terms of reducing extrinsic cognitive load a few other blog posts have come up recently that have some very nice approaches for this. Aspects of extrinsic cognitive load include having to remember how to access the servers that systems are running on, managing permissions, and situations where people are blocked getting access to systems when they should have access, configuring logging, configuring backups, and being able to quickly understand how to access those backups.&lt;/p&gt;
&lt;p&gt;In &lt;a href=&#34;https://medium.com/ft-product-technology/documentation-day-how-the-ft-com-team-improved-our-documentation-to-95-usefulness-in-7-hours-b73d1a7e6f30&#34;&gt;https://medium.com/ft-product-technology/documentation-day-how-the-ft-com-team-improved-our-documentation-to-95-usefulness-in-7-hours-b73d1a7e6f30&lt;/a&gt; Jennifer Jonnson described how a team at the Financial Times used a hack day to improve the documentation of their projects. They built up a system where key documentation lived in the project software repository, and got pulled from those repositories into a central documentation store automatically. In this way the documentation lives near to the software, and at the same time a cereal location is created where people can see documentation across all the projects, but done so in an automated way! This documentation includes the run books and the ops information, critical pieces of information for systems maintenance.&lt;/p&gt;
&lt;p&gt;In &lt;a href=&#34;https://tech.findmypast.com/repository-driven-development/&#34;&gt;Repository Driven Development | Findmypast Tech&lt;/a&gt; Neil Crawford describes how they use templates within their repositories that are used to wire services together. This requires building up some infrastructure for projects, but once that infrastructure is in place developers don’t have to think about things like logging or metrics, they just get them as part of their project configuration, and as there is consistency across all projects.&lt;/p&gt;
&lt;p&gt;One of my favourite tips that I’ve seen is the &lt;code&gt;do nothing scripting&lt;/code&gt; approach described here: &lt;a href=&#34;https://blog.danslimmon.com/2019/07/15/do-nothing-scripting-the-key-to-gradual-automation/&#34;&gt;Do-nothing scripting: the key to gradual automation – Dan Slimmon&lt;/a&gt;. The idea is that for boring repetitive tasks, instead of scripting them, start by writing proto-scripts that remind you of the steps you have to take. This helps to reduce error, and is the kind of thing that can be improved gradually over time. It’s a bit like taking a checklist approach to operations, a technique that is critical for airline safety.&lt;/p&gt;
&lt;p&gt;I found many of these links through following &lt;a href=&#34;https://twitter.com/simonw&#34;&gt;Simon Willison (@simonw) on Twitter&lt;/a&gt;, and if you are interested in these kinds of topics I highly recommend following him too!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>key questions about AI in the publishing knowledge industry</title>
      <link>http://scholarly-comms-product-blog.com/2019/08/01/key_questions_about_ai_in_the_publishing_knowledge_industry_/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2019/08/01/key_questions_about_ai_in_the_publishing_knowledge_industry_/</guid>
      <description>&lt;p&gt;At the moment one thing that is front and centre in my thinking about AI and machine learning in publishing and the scholarly ecosystem is how to make the case for ROI for investment in the technology, and more specifically investing in making data actionable.&lt;/p&gt;
&lt;p&gt;Overall I think there is great promise for challenges like knowledge discovery and machine generated hypotheses, but there is massive potential for these technologies to also just make the quality of our work better, and to increase the value of our work by reducing and removing toil in the workplace.&lt;/p&gt;
&lt;p&gt;The broader more aspirational kinds of goals of AI will probably by necessity be only initially available to a small number of people to work on, and that work will need to find a path to be translated to many other people through intermediaries, however it is in the smaller ambition of making our workplaces better that perhaps more people can be directly impacted in the short to medium term.&lt;/p&gt;
&lt;p&gt;To get there though, there are a number of blockers that have to be overcome.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Finding capacity to find problems worth working on.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This applied not only to AI, but to any business improvement. Work tends to expand to fill the time available, and so designing time to look for efficiency gains can feel risky, especially when there is no immediately obvious solution. Our work can create data as a by-product, and our systems and ways of working can be amenable to improvement, if we take the time to look, but taking that time means that we have to go slower in some other areas for some period of time.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;the expertise to understand how to work with data is expensive&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Even after we find problems to work on, finding people with data science skills is hard. I think there are some areas of hope here. As data skills are becoming more available as core prats of university curricula, people entering the workforce are more capable. How do we create environments within our organisations that encourage these people to apply their skills to the work that they are doing? How do we leverage the skills that we have elsewhere and share them out within our organisations?&lt;/p&gt;
&lt;p&gt;Helen King from BMJ pointed me towards &lt;a href=&#34;https://qz.ai&#34;&gt;Quartz AI Studio - Helping journalists use machine learning&lt;/a&gt; - an imitative to spur training around these technologies for journalism, something similar in scholarly publishing would not go amiss.&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;the cost of change of our systems can be expensive&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Even after we have identified a problem, applied the skills to that problem and created a prototype solution, institutional inertia can make it hard to roll out a change. We need to encourage a change in how our entire organisation thinks about change to support this kind of internal innovation. One perspective on this is &lt;code&gt;rightsifting&lt;/code&gt; &lt;a href=&#34;https://flowchainsensei.wordpress.com/rightshifting/&#34;&gt;Rightshifting | Think Different&lt;/a&gt;.&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;we may have to change how we think about our data.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Data in our companies is processed under a single use case framework. Need to do X, make the data look like Y. To support machine learning we often need the data to be put into a more general form, we often need to think about capturing data with a view to unspecified uses later. When raising requests like this, there can often be pushback about taking on work now for an unspecified future benefit. Overall I’m a fan of &lt;a href=&#34;https://www.martinfowler.com/bliki/Yagni.html&#34;&gt;YAGNI&lt;/a&gt; - but in this case there is some other weighting of effort that might be useful.&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;data is infantile, algorithms are just immature.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Working with data is working with a moving target. A good friend of mine describes it like this. Looking after a classical algorithm that you have created is like looking after a pre-teen. It needs some care attention, but give it an iPad and some crisps and sit it in a corner and it will probably be OK as long as you check in every now and again.&lt;/p&gt;
&lt;p&gt;Working with a machine learning system where you have to take care about the data is like looking after a toddler. You know the toddler is going to shit it’s nappy, you just don’t know when. The best you can hope for is that the nappy has no shit in it right now, but you know you are going to have to deal with a shitty nappy before too long.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>test blog post</title>
      <link>http://scholarly-comms-product-blog.com/2019/08/01/test_blog_post/</link>
      <pubDate>Thu, 01 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2019/08/01/test_blog_post/</guid>
      <description>&lt;p&gt;This is a test post while trying to fix some domain issues with the blog.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AGILE or agile?</title>
      <link>http://scholarly-comms-product-blog.com/2018/11/29/agile_or_agile_/</link>
      <pubDate>Thu, 29 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2018/11/29/agile_or_agile_/</guid>
      <description>&lt;p&gt;Three links today looking at the state of agile as a software development practice.&lt;/p&gt;
&lt;h4 id=&#34;flavours-of-agile&#34;&gt;Flavours of Agile&lt;/h4&gt;
&lt;p&gt;In &lt;a href=&#34;https://www.thekua.com/atwork/2018/07/flavours-of-agile/&#34;&gt;Flavours of Agile&lt;/a&gt; Pat Kua briefly describes and rates a number of agile processes. There are a ton here, and loads that I’d not heard of. One of the key messages that I get from reading this is that “AGILE” as a fixed practice has been growing, especially within in enterprise, and perhaps not to the benefit of actually delivering or simplifying the delivery of complex processes.&lt;/p&gt;
&lt;p&gt;Pat highly rates Extreme Programming for the technical underpinnings that it brings to the delivery of software, and I am 100% behind him on this.&lt;/p&gt;
&lt;p&gt;I’m interested in learning more about &lt;a href=&#34;http://modernagile.org&#34;&gt;Modern Agile&lt;/a&gt; and about &lt;a href=&#34;https://heartofagile.com&#34;&gt;https://heartofagile.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Feature leads also sounds really interesting - &lt;a href=&#34;https://www.thekua.com/atwork/2012/07/talking-feature-leads/&#34;&gt;Talking Feature Leads - patkua@work&lt;/a&gt;. Feature leads  &lt;em&gt;sound&lt;/em&gt; like a hybrid between a product manager or product owner an architect and a developer. From my understanding of the content that Pat tends to work in these were probably developers who were tasked to think more broadly about the broader context of specific features in the wider application. This sounds like a great way to organise things.&lt;/p&gt;
&lt;p&gt;I was interested to see that he seems to pretty much disparage the scaled agile framework (SAFe).&lt;/p&gt;
&lt;p&gt;There are two practices that Pat didn’t discuss in this overview — lean value tree (&lt;a href=&#34;https://www.slideshare.net/steve236/lean-value-tree-overview-82783795&#34;&gt;Lean Value Tree Overview&lt;/a&gt;) and the pragmatic marketing framework (&lt;a href=&#34;https://www.pragmaticmarketing.com/framework&#34;&gt;The Pragmatic Marketing Framework&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;I’ve also picked up a copy of &lt;a href=&#34;https://www.amazon.co.uk/gp/product/B00DY3KQSQ?ref=dbs_p2d_P_R_popup_yes_alc_T2&#34;&gt;Adaptive Software Development: A Collaborative Approach to Managing Complex Systems (Dorset House eBooks) eBook: James Highsmith III: Amazon.co.uk: Kindle Store&lt;/a&gt; after reading this post, some good xmas reading in the bag!!&lt;/p&gt;
&lt;h4 id=&#34;the-state-of-agile-software-in-2018&#34;&gt;The state of Agile software in 2018&lt;/h4&gt;
&lt;p&gt;Martin Fowler has posted the transcript of a talk that he gave about the state of Agile Software Development. Well worth a read.&lt;/p&gt;
&lt;p&gt;Some key quotes from that talk:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Our challenge at the moment isn’t making agile a thing that people want to do, it’s dealing with what I call faux-agile: agile that’s just the name, but none of the practices and values in place.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;You need to find good people that work together at a human level, so they can collaborate effectively. The choice of what tools they use or what process they should follow is second order.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“if you’re doing Extreme Programming the same way as you were doing it a year ago, you’re no longer doing Extreme Programming”. Because if you don’t take charge and you don’t alter things to fit your circumstance, then you are missing the key part of it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The agile movement was part of trying to push that, to try to say, “The teams involved in doing the work should decide how it gets done,&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The lowliest, juniorest programmer tapping away on JavaScript should be connected to people who are out there thinking about the business issues and business strategies of the group that they’re working with.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;When I summarize agile to people, I usually say there’s two main pieces to it. One, I’ve already talked about, the primacy of the team, and the team’s choices of how they do things, but the other is our ability to change rapidly, to be able to deal with change easily.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Refactoring is small changes, each of which keeps everything working. It doesn’t change the observable behavior of the software, that’s its definition. And I should know because I was the one who got to define it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;if you want to make changes, you want to add things quickly, you’ve got to quickly understand which parts of the program matter, what do they do, and how do I work so that I can quickly make that change. This also burrows up into modularity. If I’ve got a well-modularized piece of software, instead of having to understand the whole thing, I can just understand part of it. Technical excellence is about being able to build that kind of adaptive software&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;refactoring relies on testing, and refactoring also relies on continuous integration, and together with continuous integration, you have the practice of continuous delivery and the notion that we’re gonna be able to release the software very, very frequently.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The third thing that I want to stress is the importance of getting rid of software projects as a notion. Instead we want to switch to a  &lt;a href=&#34;https://martinfowler.com/articles/products-over-projects.html&#34;&gt;product-oriented view&lt;/a&gt;  of the world where instead of projects that you spin up, run for a while and then stop; you instead say, “Let’s focus on things that are much more long-lasting and organize a product team around that.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;If I was on that team as a developer, I’d want to be on first-name terms with all of the users. I would want to be talking. I would want to watch what they do, I’d want to understand how they think about making their customers happier and see that whole flow.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Another book recommendation that I picked up from reading this blog post was: &lt;a href=&#34;https://www.amazon.com/gp/product/B07B9F83WM?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B07B9F83WM&#34;&gt;Accelerate: The Science of Lean Software and DevOps: Building and Scaling High Performing Technology Organizations 1, Nicole Forsgren PhD, Jez Humble, Gene Kim, eBook - Amazon.com&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;agile-is-dear---dave-thomas&#34;&gt;Agile is dear - Dave Thomas&lt;/h3&gt;
&lt;p&gt;The last link on this topic is to a talk by Dave Thomas titles “Agile is dead” from back in 2015. — &lt;a href=&#34;https://www.youtube.com/watch?v=a-BOSpxYJ9M&amp;amp;feature=youtu.be&#34;&gt;GOTO 2015 • Agile is Dead • Pragmatic Dave Thomas - YouTube&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;OK, I admit, I’ve not had time to look at this yet, but I’m betting that it’s going to be interesting to listen to.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Some ruminations on software architecture and diagramming</title>
      <link>http://scholarly-comms-product-blog.com/2018/11/22/some_ruminations_on_software_architecture_and_diagramming/</link>
      <pubDate>Thu, 22 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2018/11/22/some_ruminations_on_software_architecture_and_diagramming/</guid>
      <description>&lt;p&gt;So Pat Kua recently tweeted:&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;and this got me thinking about the worries I&amp;rsquo;ve had about not doing &amp;ldquo;diagramming&amp;rdquo; right, but the above tweet led me to read a ton of really interesting posts on software architecting.&lt;/p&gt;
&lt;p&gt;Bottom line is, as with so many things, pick the artefact that fits the purpose and the audience.&lt;/p&gt;
&lt;p&gt;UML - the unified modelling language (&lt;a href=&#34;https://en.wikipedia.org/wiki/Unified_Modeling_Language&#34;&gt;https://en.wikipedia.org/wiki/Unified_Modeling_Language&lt;/a&gt;) is the de-facto standard for creating entity-relationship diagrams. The following thread talks about why it looks like this approach has &amp;ldquo;failed&amp;quot;https://dzone.com/articles/uml-failed-so-here-we-have-aml. The question under consideration is whether UML has met the needs of creating great software.&lt;/p&gt;
&lt;p&gt;This post for me nails it: &lt;a href=&#34;http://memeagora.blogspot.com/2008/12/irrational-artifact-attachment.html&#34;&gt;http://memeagora.blogspot.com/2008/12/irrational-artifact-attachment.html&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you create a beautiful UML diagram using some tool like Visio that takes 2 hours, you have an irrational attachment to that artifact that&amp;rsquo;s roughly proportional to the amount of time invested. That means that you&amp;rsquo;ll be more attached to a 4 hour diagram than a 2 hour one. By &amp;ldquo;irrational attachment&amp;rdquo;, I mean that it&amp;rsquo;s harder to listen to reason as to why it&amp;rsquo;s wrong because you know how much time it took to create it (and therefore the required effort to update it).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What this is saying is that you need to not get too tied to an artefact when what you are doing is changing underneath you, as it will prevent you from adapting you thinking towards the best solution.&lt;/p&gt;
&lt;p&gt;One of my good friends who is an architect for transport for London had a great take on this. For communicating plans, and working through the design phase keeping diagrams really low fidelity is important, and this is the level of abstraction that architects should probably live in, but a problem arises when developers think that the architects should be doing detailed diagramming, and vice versa. The  &lt;a href=&#34;https://c4model.com&#34;&gt;C4 model&lt;/a&gt; sounds like a great way to navigate these kinds of discussions, at what level of abstraction are we dealing with? Who are the stakeholders that are involved? What is being communicated? How is this artefact going to reduce uncertainty and increase the likelihood for success?&lt;/p&gt;
&lt;p&gt;If you do want to create diagrams though then do try to make your diagrams easy to read: &lt;a href=&#34;https://blog.zone24x7.com/communicate-with-more-appealing-diagrams/&#34;&gt;Communicate with more Appealing Diagrams | Sparks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Tools that I have found to be useful for creating “diagrams” of the high level design of system that I try to show to people are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Keynote&lt;/li&gt;
&lt;li&gt;Paper by 53 on iOS&lt;/li&gt;
&lt;li&gt;Omnigraffle with AWS service stencils&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Recently (within the last week), I’ve found a nice tool for crating network relationships — &lt;a href=&#34;http://www.apcjones.com/arrows/#&#34;&gt;Arrow Tool&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Finally, (this episode)[&lt;a href=&#34;http://www.se-radio.net/2015/06/episode-228-software-architecture-sketches-with-simon-brown/&#34;&gt;Episode 228: Software Architecture Sketches with Simon Brown  : Software Engineering Radio&lt;/a&gt;] of software engineering radio sounds like it has a good take on this topic (but I’ve not had the time to fully listen to it yet). It talks about the c4 model for describing software architecture &lt;a href=&#34;https://c4model.com&#34;&gt;The C4 model for software architecture&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The state of retractions in the research literature.</title>
      <link>http://scholarly-comms-product-blog.com/2018/10/26/the_state_of_retractions_in_the_research_literature._/</link>
      <pubDate>Fri, 26 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2018/10/26/the_state_of_retractions_in_the_research_literature._/</guid>
      <description>&lt;p&gt;The results below are oldish, but interesting around the rate of retractions in the scholarly literature, and there is currently a bit of a debate going on around retractions (e.g. &lt;a href=&#34;https://www.sciencemag.org/news/2018/10/what-massive-database-retracted-papers-reveals-about-science-publishing-s-death-penalty&#34;&gt;What a massive database of retracted papers reveals about science publishing’s ‘death penalty’ | Science | AAAS&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Steen RG, Casadevall A, Fang FC (2013) Why Has the Number of Scientific Retractions Increased? PLoS ONE 8(7): e68397. doi:10.1371/journal.pone.0068397
&lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0068397&#34;&gt;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0068397&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://journals.plos.org/plosone/article/figure?id=10.1371/journal.pone.0068397.g001&#34;&gt;https://journals.plos.org/plosone/article/figure?id=10.1371/journal.pone.0068397.g001&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The increase in retracted articles appears to reflect changes in the behaviour of both authors and institutions. Lower barriers to publication of flawed articles are seen in the increase in number and proportion of retractions by authors with a single retraction. Lower barriers to retraction are apparent in an increase in retraction for “new” offenses such as plagiarism and a decrease in the time-to-retraction of flawed work.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In “Misconduct accounts for the majority of retracted scientific publications”&lt;/p&gt;
&lt;p&gt;Proc Natl Acad Sci U S A. 2012 Oct 16; 109(42): 17028–17033.
&lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3479492/&#34;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3479492/&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Journal-impact factor showed a highly significant correlation with retractions because of fraud or error but not with those because of plagiarism or duplicate publication ( &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3479492/figure/fig03/?report=objectonly&#34;&gt;Fig. 3 A–C&lt;/a&gt; ). Moreover, the mean impact factors of journals retracting articles because of fraud or error differed significantly from that of journals retracting articles because of plagiarism or duplicate publication. Accordingly, retractions for fraud or error and retractions for plagiarism or duplicate publication were encountered in distinct subsets of journals, with differences in impact factor ( &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3479492/figure/fig03/?report=objectonly&#34;&gt;Fig. 3D&lt;/a&gt; ) and limited overlap ( &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3479492/table/t01/?report=objectonly&#34;&gt;Table 1&lt;/a&gt; ).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;Reading these it kind of seems that increase in rates of retractions may be related to both changes in behaviour and changes in norms, which makes a straight up companion of retractions “then” and “now” a slightly subtle exercise.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>RAVE publishing technology conference 2018</title>
      <link>http://scholarly-comms-product-blog.com/2018/10/24/rave_publishing_technology_conference_2018_/</link>
      <pubDate>Wed, 24 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2018/10/24/rave_publishing_technology_conference_2018_/</guid>
      <description>&lt;p&gt;I took some notes on some of the sessions at the conference yesterday.&lt;/p&gt;
&lt;h3 id=&#34;dave-smith---object-oriented-publishing&#34;&gt;Dave Smith - object oriented publishing.&lt;/h3&gt;
&lt;p&gt;Dave makes a good case here about how we should think about the future of scholarly publishing — in terms of objects that can be reformed based on the needs and competencies of the readers. He points out that while we do have in place good ontologies and domain models, our entire view of publishing and our publishing infrastructures, remain laggy and are a barrier to moving in this direction.&lt;/p&gt;
&lt;p&gt;He mentions the usual examples of initiatives that point towards the future, protocols.io, project Jupyter, code ocean, Figshare, overleaf, stencilla, the CoKo foundation model and I think some others.&lt;/p&gt;
&lt;p&gt;For me one of the core issues with this vision is that these interesting initiatives are essentially island efforts with few routes of connection between them.&lt;/p&gt;
&lt;p&gt;Finally David mentions the very critical point that we might be moving into an environment where the core elements of a scholarly publishing infrastructure may be owned by a few key players. Would this be good or bad overall for the entire ecosystem? The answer to this question is sort of left hanging, however he also makes the great point that funders seem not to be looking at this aspect of the ecosystem and that there is a need for financing the experimentation needed to work towards this more object oriented future.&lt;/p&gt;
&lt;h3 id=&#34;louise-russell---blockchain---distraction-or-opportunity&#34;&gt;Louise Russell - blockchain - distraction or opportunity?&lt;/h3&gt;
&lt;p&gt;The benefits described in this presentation are general, and not specifically related to scholarly publishing.&lt;/p&gt;
&lt;p&gt;MyCelia (&lt;a href=&#34;http://myceliaformusic.org&#34;&gt;http://myceliaformusic.org&lt;/a&gt;) looks interesting, a platform for managing music for artists. It’s in alpha test, so it’s very early to tell whether it will work. It provides data around ownership contracts and payments for music artists. It looks a lot like the Eureka initiative.&lt;/p&gt;
&lt;p&gt;Louise mentions the following scholarly related initiatives (most of which I had head of before, so that’s cool).:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.scienceroot.com&#34;&gt;https://www.scienceroot.com&lt;/a&gt; - they have launched an ICO that is now completed&lt;/li&gt;
&lt;li&gt;Pluto - &lt;a href=&#34;https://scinapse.io&#34;&gt;Sci-napse | Academic search engine for paper&lt;/a&gt; search engine (based in Korea, a not for profit)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scientificcoin.com&#34;&gt;ScientificCoin&lt;/a&gt; - a blockchain based funding platform (I quite like this idea, kind of).&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://orvium.io&#34;&gt;Orvium - Open and Transparent Science Powered By Blockchain&lt;/a&gt; - authors gain instantaneous “proof of existence” by publishing on this platform.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.katalysis.io&#34;&gt;Katalysis&lt;/a&gt; I have heard of before, and is about sharing review information amongst publishers.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;alice-fleet---from-sofas-to-safe-agility-empowerment-and-the-heat-treat-oven---head-of-technology-emerald-publishing&#34;&gt;Alice Fleet - from Sofas to SAFe; agility empowerment and the heat treat oven - head of technology Emerald Publishing&lt;/h3&gt;
&lt;p&gt;This is a great talk looking at how to improve the process of a development team within an organisation, following trying to implement process improvement based on the theory of constraints (with an actual picture of a heat-treat oven, referencing “The Goal”).&lt;/p&gt;
&lt;p&gt;They re-organised their development teams, while at the same time allowing for space to look at consistency across teams for things like testing and security (this is called the squad model). (&lt;a href=&#34;https://www.productplan.com/product-squads/&#34;&gt;Can Product Squads Improve Your Agile Development Process?&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;They put in place a freeze on work to allow them to find the bottlenecks, and looked at how to reduce the work in progress in the teams.&lt;/p&gt;
&lt;p&gt;They introduced the three amigos conversation when it needed to be rather than at a specific point in the process.&lt;/p&gt;
&lt;p&gt;They worked on making work visible, and worked on bringing in smaller units of work into the pipeline.&lt;/p&gt;
&lt;p&gt;They are only about two months into this process, but they are already seeing work completion rates going up, even though they are taking less work into the system.&lt;/p&gt;
&lt;p&gt;They want to make this system of work scale, so they are looking at introducing the SAFe framework - Scaled Agile.&lt;/p&gt;
&lt;p&gt;This is a great talk.&lt;/p&gt;
&lt;h3 id=&#34;blockchain-panel---some-reflections&#34;&gt;Blockchain panel - some reflections&lt;/h3&gt;
&lt;p&gt;Lynsy Haire and Joris van Rossim who are involved in the Katalysis pilot project presented the problems they are trying to address in relation to peer review are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Call for transparency.&lt;/li&gt;
&lt;li&gt;Aid to peer review.&lt;/li&gt;
&lt;li&gt;Better Infrastructure.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The panel discussion was good natured, and I offered some objections_questions_challenges to the use of blockchain. Joris was articulate on why he believes that blockchain, and blockchain-related  technologies are the right kind of technologies to address these problems, and was also clear that they are investigating these in the pilot, bur are not wedded to them if they find better alternatives. I do think that by getting researchers publishers and funders involved they will learn a lot, but I still think that blockchain is possibly not the right technology for this. My points were the following (but as ever I could be wrong).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The technology is opaque, the coding and algorithms are non trivial and have proven to be frequency insecure.&lt;/li&gt;
&lt;li&gt;Other forms of distributed databases or distributed append only logs can provide the same benefits for less complexity - DAT / bit torrent. Append only databases - Kafka has a much better developed infrastructure for  event subscriptions, and architectures like fan outs.&lt;/li&gt;
&lt;li&gt;The claim of independence, in my mind does, not hold water. I think that would lead to vendor lock in as I don’t think that publishers will implement this tech on their own, without some standardisation we are going to end up depending on a vendor. That’s not decentralised in practice.&lt;/li&gt;
&lt;li&gt;There is a tech utopian issue here : tear down the publisher vs fix the rights and permissions issue. Both points of view are converging on blockchain, both cannot be right.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;HOWEVER&lt;/p&gt;
&lt;p&gt;There is a need for creating new sharing, anonymization and metadata standards and  if blockchain acts as a catalyst for people to do that, then this is grand, but just please don’t actually use blockchain core Technologies. I guess I’m calling for - BINO - Blockchain In Name Only.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Belmont Forum Round Table - data accessibility statements</title>
      <link>http://scholarly-comms-product-blog.com/2018/10/19/belmont_forum_round_table_-_data_accessibility_statements_/</link>
      <pubDate>Fri, 19 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2018/10/19/belmont_forum_round_table_-_data_accessibility_statements_/</guid>
      <description>&lt;p&gt;Yesterday I attended a round table discussion hosted by the &lt;a href=&#34;http://www.belmontforum.org&#34;&gt;Belmont Forum&lt;/a&gt; about the release of their position on data accessibility statements and digital objects management plans. (It’s a bit of a mouthful, but the reason is that they are aiming to be clear and comprehensive around what they are asking to make it easier for researchers, publishers and other stakeholders to get to compliance around this policy.)&lt;/p&gt;
&lt;p&gt;You can read their position paper — &lt;a href=&#34;https://docs.google.com/document/d/1Aw1y6MysyVxPwhBjECTT_0w1EKCLcyT-rGyHAX9mrhM/edit&#34;&gt;Draft DAS Statement and Policy for October 2018 Plenary - Google Docs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This work built on a number of previous workshops, including  &lt;a href=&#34;http://www.bfe-inf.org/sites/default/files/doc-repository/R2R_Workshop-summary.pdf&#34;&gt;this one&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There were people representing the following orgs at the meeting yesterday: Belmont, Hindawi, PLOS, Taylor and Francis, the IOP, Digital Science, Figshare, Springer Nature and myself from SAGE.&lt;/p&gt;
&lt;p&gt;The Belmont forum is an umbrella group of funders representing 26 funders. NERC form the UK is a member as is the NSF.&lt;/p&gt;
&lt;p&gt;The guidelines they have posted are sensible, and well thought out. For me at the heart if their recommendations is the requirement to have a data accessibility statement available outside of the paywall for all articles that they fund.&lt;/p&gt;
&lt;p&gt;Specifically:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The DP3 is concentrating chiefly on delivering a set of template Data Accessibility Statements (DASs) for guiding Belmont Forum grantees when publishing their research results. The DAS is included as part of a journal article and articulates which data underlie a paper, where the data are available and under what conditions they can be accessed.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There was a great discussion at the round table yesterday. Points that came out that I found useful were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is easy to require a DAS (data availability/accessibility statement), but harder to dictate what goes into it. E.g. in life science journals if you need to check for gene deposition it can be harder to implement.&lt;/li&gt;
&lt;li&gt;Though this is hard, there was broad agreement that even requiring a DAS is a step in the right direction&lt;/li&gt;
&lt;li&gt;There was general agreement that doing this increases that amount of data that is being made available.&lt;/li&gt;
&lt;li&gt;The question around where responsibility lies for checking these has no simple answer.&lt;/li&gt;
&lt;li&gt;PLOS have found it useful to have a structured question in the review form. This can help when the manuscript is being assessed.&lt;/li&gt;
&lt;li&gt;At Springer Nature they have about 400 journals that have some kind of requirement for a statement.&lt;/li&gt;
&lt;li&gt;Rebecca Grant and Iain Hrynaszkiewicz wrote a paper showing &lt;a href=&#34;https://www.biorxiv.org/content/early/2018/02/13/264929&#34;&gt;The impact on authors and editors of introducing Data Availability Statements at Nature journals&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;We talked a bit about the impact on the social sciences and the additional complexity of this kind of data.&lt;/li&gt;
&lt;li&gt;Unsurprisingly institutions were mentioned as a key stakeholder.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>One Two Four All - a technique for getting insights from groups</title>
      <link>http://scholarly-comms-product-blog.com/2018/10/18/one_two_four_all_-_a_technique_for_getting_insights_from_groups/</link>
      <pubDate>Thu, 18 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2018/10/18/one_two_four_all_-_a_technique_for_getting_insights_from_groups/</guid>
      <description>&lt;p&gt;I’ve started working my way through &lt;a href=&#34;https://www.amazon.co.uk/Surprising-Power-Liberating-Structures-Innovation-ebook/dp/B00JET2S76/ref=sr_1_1?ie=UTF8&amp;amp;qid=1539642357&amp;amp;sr=8-1&amp;amp;keywords=the+surprising+power+of+liberating+structures&#34;&gt;The Surprising Power of Liberating Structures&lt;/a&gt; - a hand book of techniques for collaborative work.&lt;/p&gt;
&lt;p&gt;So far I’ve tried one technique from the book - &lt;code&gt;one two four all&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The idea is super simple and is an alternative to open brainstorming or post-it note sessions. Before describing the technique with a few comments, I’ll just point out one of the weaknesses of a group work activity like a retrospective.&lt;/p&gt;
&lt;p&gt;We have been doing a lot of retrospectives over the last year, and we usually get the team to all write up thoughts against a set of questions Then we do a cluster analysis on the combined post-it notes. Overall this is a great technique, but it has a couple of potential down sides. It doesn’t scale  well once you get past about ten people. This technique also gives more weight to the ideas of people who write fast, and can churn out a lot of post-it notes.&lt;/p&gt;
&lt;p&gt;“One two four all” is an elegant mechanism that avoids both of those problems.&lt;/p&gt;
&lt;p&gt;The way it works is that you pose one or two questions to the group. The group is given one minute where everyone thinks on their own about their answer. Then they pair up in twos and are given two minutes to discuss. Then groups of four are created and are given four minutes to discuss. Finally each group of four reports back on the best idea that they feel they have discussed.&lt;/p&gt;
&lt;p&gt;This allows ideas to filter up from people who might otherwise remain quiet, it scales horizontally, and is really easy to implement.&lt;/p&gt;
&lt;p&gt;When we ran it a few weeks ago the first minute of silence seems awkward to me. I was doing the exercise with quite a high level stakeholders in the organisation. After that though there was great and engaged conversation and we really came up with a nice diverse set of thoughts on the questions posed. I feel given the room we had and the number of people involved (about 25), that we got a lot more out of this process than if we had tried to run a retrospective.&lt;/p&gt;
&lt;p&gt;As I was setting out the process I got a few questions from the group; Should we do it once for one of the questions and again for the second question? Do they need to split directly into fours at the end, some of the tables had five people and it looked like that could work. We decided to have the groups work on both questions at the same time, rather than serially, and I think that worked well. Given this is a technique to enable high quality conversations I didn’t see the need to hold fast to being totally rigorous with the guidelines, so I was OK with some groups of five, and there didn’t seem to be any problem with that.&lt;/p&gt;
&lt;p&gt;I’m looking forward to running this exercise again in the future.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>blockchain in STEM - part 3</title>
      <link>http://scholarly-comms-product-blog.com/2018/07/04/blockchain_in_stem_-_part_3__/</link>
      <pubDate>Wed, 04 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2018/07/04/blockchain_in_stem_-_part_3__/</guid>
      <description>&lt;p&gt;Over the last few weeks I’ve been writing up some thoughts on the uses of blockchain in STEM.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;http://scholarly-comms-product-blog.com/2018/06/19/blockchain_in_stem_-_part_1/&#34;&gt;first post&lt;/a&gt; I gave a general overview of my understand of blockchain.&lt;/p&gt;
&lt;p&gt;In the &lt;a href=&#34;http://scholarly-comms-product-blog.com/2018/06/26/blockchain_in_stem_-_part_2/&#34;&gt;second post&lt;/a&gt; I looked at potential use cases of blockchain in STEM, and came up somewhat short.&lt;/p&gt;
&lt;p&gt;That said, a lot, really, a lot of very smart people are talking about this, and doing things in this space, so in this post I wanted to look at a few of those efforts and see how their thinking lines up or diverges from mine.&lt;/p&gt;
&lt;h3 id=&#34;what-do-others-say&#34;&gt;What do others say?&lt;/h3&gt;
&lt;h6 id=&#34;artifactsai&#34;&gt;Artifacts.ai&lt;/h6&gt;
&lt;p&gt;&lt;a href=&#34;http://artifacts.ai/&#34;&gt;http://artifacts.ai/&lt;/a&gt; proves a solution to allow researchers to publish linkages of artefacts, but they don’t explain on their homepage who else is going to participate in the verification network. Indeed they refer to a patent pending solution, which seems to me to defeat the purpose of distributed trust. If they don’t need distributed trust they can use any append only log file format.&lt;/p&gt;
&lt;h6 id=&#34;are-blockchain-and-stma-marriage-made-inheaven&#34;&gt;Are &lt;strong&gt;Blockchain and STM — a marriage made in heaven?&lt;/strong&gt;&lt;/h6&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/pagemajik/blockchain-and-stm-a-marriage-made-in-heaven-fecf077ea522&#34;&gt;https://medium.com/pagemajik/blockchain-and-stm-a-marriage-made-in-heaven-fecf077ea522&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This article pulls out the following problems&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The industry is plagued with disputes around ownership, provenance, authenticity and credibility, and battles are regularly fought around the plagiarism and misappropriation of scientific endeavours.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;…&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;scholarly communication could undoubtedly benefit from unequivocal, time-stamped records for every submission, citation, edit or transaction taking place along the chain. If any industry could do with a “Network of trust”, which is what the STM Association is billing blockchain, it’s STM.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I agree with this sentiment, but I’m still finding it hard to see how to do this with blockchain, over say improving reliability of metadata deposit into CrossRef.&lt;/p&gt;
&lt;p&gt;The article also says that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Scholarly publishers are also discovering that blockchain can offer plenty of benefits in terms of helping them fine-tune and automate day-to-day processes. In a business like STM journal publishing, where a publisher is likely to have a range of journals to manage, with multiple articles and papers on the go, and teams of staff working across editorial and production, blockchain can offer a lifeline when it comes to version control, providing clarity on ownership and navigating digital rights management.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I’m very interested in speaking to these publishers to find out how blockchain is actively helping today with those problems. If they are just solving internal problems, then of course I hope they have considered using, you know, a database.&lt;/p&gt;
&lt;h6 id=&#34;looking-at-digital-educational-certificates-as-a-model&#34;&gt;Looking at digital educational certificates as a model&lt;/h6&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/lambo&#34;&gt;Lambert Heller&lt;/a&gt; as a &lt;a href=&#34;https://www.repo.uni-hannover.de/handle/123456789/3285&#34;&gt;presentation about educational certificates as a model for a P2P commons of scholarly metadata interaction&lt;/a&gt;. The presentation available on that page makes the great point that ORCID is mostly a place where other parties push info about authors into it, and authors themselves are largely not doing this. I’d not thought about that aspect of ORCID before. It’s primarily seen as a useful aggregator.  The key of the presentation (I think, and I have huge respect for Lambert), is that ownership of data and claims needs to be returned to academics, and a system of participation with them at the centre needs to be created. A lot of the components of that vision &lt;em&gt;could&lt;/em&gt; be supported by a blockchain technology, but a lot of the human problems in getting there will remain to be solved, and for the blockchain specific solution we need to see how participation incentives can be created.&lt;/p&gt;
&lt;h6 id=&#34;irisai-are-creating-a-token-and-platform&#34;&gt;Iris.ai are creating a token and “platform”&lt;/h6&gt;
&lt;p&gt;&lt;a href=&#34;http://blocktribune.com/blockchain-ai-project-aiur-hopes-to-boost-science-research-breakthroughs&#34;&gt;http://blocktribune.com/blockchain-ai-project-aiur-hopes-to-boost-science-research-breakthroughs&lt;/a&gt; is a bit more interesting, they are creating their own blockchain and token and connecting their “platform” to a DB of 120M articles. It sounds as if they want to create an open market of challenges for people to build AI driven reporting tools on top of the literature, so basically like a Kaggle, but for research questions. You get tokens for participating and then you can do, um, I’m not sure what, with those tokens.&lt;/p&gt;
&lt;p&gt;This sounds interesting, but I’m not seeing the motivation to create a network of nodes on this network, over say, just running a Kaggle-Like competition. Except of course that by raising an ICO they can fund some development to deliver on what they are talking about, but once delivered, I’m not sure how you are going to build a robust set of participants to make your blockchain trustable.&lt;/p&gt;
&lt;h6 id=&#34;katalysis-are-building-smart-contracts-and-a-way-to-identify-authorship&#34;&gt;Katalysis are building smart contracts and a way to identify authorship&lt;/h6&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/katalysis-io/why-blockchain-because-duh-ce0a18747ede&#34;&gt;https://medium.com/katalysis-io/why-blockchain-because-duh-ce0a18747ede&lt;/a&gt; have created smart contracts to indicate who the owner of an article is. It’s not clear which blockchain these contracts operate over, but if you wanted to use this to pay contributors in countries where regular payment mechanisms were blocked, then maybe OK.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.digital-science.com/press-releases/digital-science-and-katalysis-lead-initiative-to-explore-blockchain-technologies-for-peer-review&#34;&gt;https://www.digital-science.com/press-releases/digital-science-and-katalysis-lead-initiative-to-explore-blockchain-technologies-for-peer-review&lt;/a&gt; goes into a bit more detail. They say&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In its initial phase, this initiative aims to look at practical solutions that leverage the distributed registry and smart contract elements of blockchain technologies.  Later phases aim to establish a consortium of organisations committed to working together to solve scholarly communications challenges that centre around peer review.&lt;br&gt;
Digital Science will manage the project and looks forward to coordinating with further partners who wish to become involved; Katalysis will use its market-leading expertise in blockchain technologies to implement the test platform; Springer Nature will participate with a selection of its journals and give key input around publisher and peer review workflows; ORCID will provide insights and knowhow around personal identifiers and authentication.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I’m hopeful that they can answer some of my questions from above. They will face a risk of ending up solving for one publisher, they should try to get a submissions system vendor onboard. In the end though, this sounds like the kind of technology that will be run by a network of publishers.&lt;/p&gt;
&lt;h6 id=&#34;poet-are-also-building-a-way-to-identify-authorship-will-it-interoperate-with-the-other-efforts-to-do-the-same&#34;&gt;Po.et are also building a way to identify authorship (will it interoperate with the other efforts to do the same?)&lt;/h6&gt;
&lt;p&gt;Speaking of platforms for publishing and giving credit to authors Po.et are doing this too. They have a blog post about this (&lt;a href=&#34;https://blog.po.et/welcome-to-the-gutenberg-era-d38137ae9632),&#34;&gt;https://blog.po.et/welcome-to-the-gutenberg-era-d38137ae9632),&lt;/a&gt; which also seems to pass over the question of who is going to be running the nods on the network?&lt;/p&gt;
&lt;p&gt;I think they need to be careful that they don’t hire any more talented people though. They say in the blog post that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Filling these key roles with such strong leaders is a significant achievement that multiplies our ability to deliver exponentially&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If they multiply any more exponentials they might end up with an out of bounds error.&lt;/p&gt;
&lt;p&gt;They also say that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;It’s clear that creators need a turnkey solution to help them protect and manage their assets in the digital age. Po.et aims to become the ubiquitous media protocol for Web 3.0.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now that &lt;em&gt;sounds&lt;/em&gt; like a centralised system rather than a decentralised system, which sounds like something that is not very “blockchain” like. I wonder if they have been following what the &lt;a href=&#34;https://ghost.org&#34;&gt;Ghost&lt;/a&gt; publishing platform has been up to.?&lt;/p&gt;
&lt;h6 id=&#34;blockchain-for-science&#34;&gt;Blockchain for science&lt;/h6&gt;
&lt;p&gt;There is key document in this space, the &lt;a href=&#34;https://www.blockchainforscience.com&#34;&gt;open document on blockchain for science&lt;/a&gt;. I’ve just skimmed it, and its filled with far more use cases than the ones I discuss over this series of posts.&lt;/p&gt;
&lt;p&gt;I think that at heart of this document is the claim that you don’t need permission-less blockchains for science because there are already trusted players in the science space.&lt;/p&gt;
&lt;p&gt;I think that once you accept that you don’t need to do proof of work, and you just want to have a shared cryptographic ledger, then you open the door to using a lot of off the shelf technology (IPFS, Kafka), but on the other hand you have to be very clear about what problems you are solving and what problems you are not solving, and you have to be very clear about what is and what is not a problem in search of a disruption.&lt;/p&gt;
&lt;h3 id=&#34;some-almost-final-thoughts&#34;&gt;Some almost final thoughts&lt;/h3&gt;
&lt;p&gt;I think there are two key, and different trends, in the desire to look at block chain in science.&lt;/p&gt;
&lt;p&gt;The first comes from commerical interests where management of rights, IP and ownership is complex, hard to do, and has led to unusable systems that are driving researchers to sites like SciHub, scaring the bejesus out of publishers in the process.&lt;/p&gt;
&lt;p&gt;The other trend is for a desire to move to a decentralised web and a decentralised system of validation and reward, in a way trying to move even further away from the control of publishers.&lt;/p&gt;
&lt;p&gt;It is absolutely fascinating to me that two diametrically opposite philosophical sides are converging on the same technology as the answer to their problems. Could this technology perhaps be just holding up an unproven and untrustworthy mirror to our desires, rather than providing any real viable solutions?&lt;/p&gt;
&lt;p&gt;For the first class of problems I have some hard news for you. One of the reasons that rights management and access control is hard is that clean metadata management is hard, and building delightful usable systems is also hard. Those two hard problems don’t go away with blockchain, if anything they become harder.&lt;/p&gt;
&lt;p&gt;For the second class of problems I have some hard news for you. Decentralised systems require decentralised responsibility, and in a world where people are busy finding ways to financially support decentralised systems is hard (for example see all of open source software, all of it), and incentivising people to keep decentralised systems up to date and to participate in them is also hard (see all Twitter and facebook killers, ever).&lt;/p&gt;
&lt;p&gt;For the former class of problems we just have to keep rolling our sleeves up, getting together as a community and putting in the hard house around initiative like &lt;a href=&#34;http://www.metadata2020.org&#34;&gt;metadata 2020.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For the second class of problems now is a time of ripe invention for the decentralised web. I think initiatives like &lt;a href=&#34;https://ipfs.io&#34;&gt;IPFS&lt;/a&gt; and &lt;a href=&#34;https://datproject.org&#34;&gt;Dat&lt;/a&gt; are truly exciting.&lt;/p&gt;
&lt;p&gt;Both are worthy challenges, and perhaps blockchain, whatever it is, is providing a useful jargon that allows people to come tougher to tackle such problems, even when not fully aligned.&lt;/p&gt;
&lt;h3 id=&#34;gniht-erom-eno--one-more-thing&#34;&gt;gniht erom eno | one more thing&lt;/h3&gt;
&lt;p&gt;A final aside, one thing I didn’t really touch on in any depth in my above analysis about how cryptography plays a key role in blockchain technologies. If we did have a situation emerge where academics were making claims of priority on discoveries by posting encrypted messages into the public domain, waiting for later verification, then it would just almost be bringing us full circle back to how &lt;a href=&#34;http://www.mathpages.com/home/kmath414/kmath414.htm&#34;&gt;Issac Newton used to publish cyphers of his work, for later verification.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>testing a new form of peer review - again</title>
      <link>http://scholarly-comms-product-blog.com/2018/07/04/testing_a_new_form_of_peer_review_-_again_/</link>
      <pubDate>Wed, 04 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2018/07/04/testing_a_new_form_of_peer_review_-_again_/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://elifesciences.org&#34;&gt;eLife&lt;/a&gt; is trying another experiment in peer review. When they launched back in 2012 they introduced a form of peer review known as consultative peer review. They are now looking at a new iteration on the peer review idea.&lt;/p&gt;
&lt;p&gt;Trials in how peer review is done are quite rare, so I think this is going to be interesting to keep track of.&lt;/p&gt;
&lt;p&gt;The new idea is that once an article has been accepted for full review by one of the editors, the journal is going to publish the article, along with all comments. Previously an article might get rejected at this stage. Now an article that would have been rejected gets published, with all of the caveats that would haver led to its previous rejection.&lt;/p&gt;
&lt;p&gt;They capture nicely some of the pros and cons of their thinking in &lt;a href=&#34;https://elifesciences.org/inside-elife/2905802e/peer-review-elife-trials-a-new-approach&#34;&gt;this blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;They are limiting the initial trial to 300 papers, and authors can choose to enter the trial or not.&lt;/p&gt;
&lt;p&gt;Authors will also have the choice to withdraw their paper if in the peer review process actual serious flaws are discovered, but that choice will like with the authors.&lt;/p&gt;
&lt;p&gt;One of the aims of this experiment is to try to shift the “job of the journal” from being the indication of a stamp of approval, to being the container for the discussion around the quality of the work.&lt;/p&gt;
&lt;p&gt;There is also a recognition that most articles that get rejected in one location go on to eventually be published, so the idea of “gate keeping” is seen mainly as being a drag on the scholarly ecosystem.&lt;/p&gt;
&lt;p&gt;Some of the things I am interested in finding out from this trial are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Will authors care about this distinction, or will they be mainly driven by just wanting to get their work published?&lt;/li&gt;
&lt;li&gt;Will readers and funders be able to clearly distinguish between these kinds of articles and other kinds of articles, will the process matter to them?&lt;/li&gt;
&lt;li&gt;Will this represent a sustainability model that is an alternative to having a catch-all mega journal associated with a premier journal?&lt;/li&gt;
&lt;li&gt;Will post publication discussion of the merits of the paper be enhanced by access to a deeper set of notes about the review process?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is an &lt;a href=&#34;https://elifesciences.org/articles/36545&#34;&gt;editoral&lt;/a&gt; about the experiment with some nice comments (using hypothesis - so much nicer that discuss).&lt;/p&gt;
&lt;p&gt;There is another nice &lt;a href=&#34;https://eighteenthelephant.wordpress.com/2018/07/01/elifes-peer-review-experiment/&#34;&gt;blog post&lt;/a&gt; from &lt;a href=&#34;https://eighteenthelephant.wordpress.com/about/&#34;&gt;Raghuveer Parthasarath&lt;/a&gt; which highlights some other potential concerns about this trial, but along with Raghuveeer  I also applaud this experiment.&lt;/p&gt;
&lt;p&gt;I should note that I worked at eLife from 2012 — 2016, so I am far from a disinterested observer.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>notes on notes on “crossing the chasam”</title>
      <link>http://scholarly-comms-product-blog.com/2018/07/03/notes_on_notes_on_crossing_the_chasam/</link>
      <pubDate>Tue, 03 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2018/07/03/notes_on_notes_on_crossing_the_chasam/</guid>
      <description>&lt;p&gt;The following is not about scholarly communication, but is a post about one tool from the world of product development, how to think about marketing your product or service so that you can cross the chasm. Academic researchers are simultaneously the most innovative and conservative of users.&lt;/p&gt;
&lt;p&gt;There is so much pain in the process of academic research that there is a constant re-invention and invention of tools and proceeds, but at the same time there is also a huge time pressure, so for any new tool, technique or service to get wide spread traction is really hard in the academic market place.&lt;/p&gt;
&lt;p&gt;Below I pull out some thoughts from reading three blog posts that review &lt;a href=&#34;https://www.amazon.co.uk/s/?ie=UTF8&amp;amp;keywords=crossing+the+chasm&amp;amp;index=aps&amp;amp;tag=googhydr-21&amp;amp;ref=pd_sl_746iox24sn_e&amp;amp;adgrpid=53405870575&amp;amp;hvpone=&amp;amp;hvptwo=&amp;amp;hvadid=259021111146&amp;amp;hvpos=1t1&amp;amp;hvnetw=g&amp;amp;hvrand=7494881341665563457&amp;amp;hvqmt=e&amp;amp;hvdev=c&amp;amp;hvdvcmdl=&amp;amp;hvlocint=&amp;amp;hvlocphy=9045999&amp;amp;hvtargid=kwd-298995268976&#34;&gt;crossing the chasm&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://readwrite.com/2007/08/06/rethinking_crossing_the_chasm/&#34;&gt;Rethinking ‘Crossing The Chasm’ – ReadWrite&lt;/a&gt;
This is an old article from 2007 — it references Second Life! Given that it’s from then it neglects the problem of the “big four” - amazon, apple, google, facebook, and their ability to suck the air out of a new market by just buying up all of the attention around a new idea or innovation through cloning or acquisition.&lt;/p&gt;
&lt;p&gt;Nonetheless they key points this blog makes about crossing the chasm are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;go for viral spread&lt;/li&gt;
&lt;li&gt;iterate to create products of desire&lt;/li&gt;
&lt;li&gt;note that this blog claims that VC’s  care, but startups don’t. The VC is trying to manage the risk.&lt;/li&gt;
&lt;li&gt;What works for the mainstream will often be too simple for early adopters&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;http://themarketingstudent.com/crossing-the-chasm-summary/&#34;&gt;http://themarketingstudent.com/crossing-the-chasm-summary/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is better in terms of describing strategies to get early majority customers to adopt your product. I very much like trying to identify the segment of the market with most pain.&lt;/p&gt;
&lt;p&gt;You want to understand who you can serve, where their money is and how big the opportunity it. In a way this is an outward facing way to do strategy.&lt;/p&gt;
&lt;p&gt;The author also mentions the “whole product” strategy, which sounds a bit like using design thinking rather than feature thinking.&lt;/p&gt;
&lt;p&gt;I also like the idea of using a “compare and contrast” model to allow your market to quickly understand what you are offering.&lt;/p&gt;
&lt;p&gt;A template that you can use to do this is&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;*For ____*
*Who are dissatisfied with ____*
*Our product is a ____*
*That provides ____*
*Unlike ____*
*We can provide ____* 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In “Good to great” Jim Collins talks about three framing questions for creating a strategy, they are almost inside out in their approach, you should ask:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;what can be the best at the world at?&lt;/li&gt;
&lt;li&gt;what drives your economic engine?&lt;/li&gt;
&lt;li&gt;what are you deeply passionate about?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally &lt;a href=&#34;http://www.inflexion-point.com/blog/the-enduring-relevance-of-crossing-the-chasm&#34;&gt;The Enduring Relevance of “Crossing the Chasm”&lt;/a&gt; mainly focuses on some use cases of how to create a differentiated message around your product.&lt;/p&gt;
&lt;p&gt;He extended the model by adding the following refinements&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For (targeted roles) in (targeted organisations)&lt;/li&gt;
&lt;li&gt;Who are seeking to (targeted issues)&lt;/li&gt;
&lt;li&gt;But are struggling with (typical constraints)&lt;/li&gt;
&lt;li&gt;(Solution name) is a (category)&lt;/li&gt;
&lt;li&gt;That (compelling reason to act)&lt;/li&gt;
&lt;li&gt;Unlike (key alternative options)&lt;/li&gt;
&lt;li&gt;We (our unique differentiators)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think these refinements are really good, and in fact you could take information generated in a lean canvas &lt;a href=&#34;https://medium.com/@steve_mullen/an-introduction-to-lean-canvas-5c17c469d3e0&#34;&gt;lean canvas&lt;/a&gt; to create this value proposition. The next time I run a lean product canvas I’m going to try to see if I can formulate the value proposition in this way.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Distill is dedicated to making machine learning clear and dynamic</title>
      <link>http://scholarly-comms-product-blog.com/2018/06/29/distill_is_dedicated_to_making_machine_learning_clear_and_dynamic/</link>
      <pubDate>Fri, 29 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2018/06/29/distill_is_dedicated_to_making_machine_learning_clear_and_dynamic/</guid>
      <description>&lt;p&gt;Distill is an experiment in bringing interactive documents and scholarly documents together. I&amp;rsquo;m often asked what the future of publishing might look like, and were we to embrace what the web offers it might look like distill.&lt;/p&gt;
&lt;p&gt;Two things though, make it look like a nice product. Right now paper flow into this journal is very low, and secondly they have advertised a large prize to attract work in this format. Both of these indicate that there is not a trend of organic update of people wanting to create outputs for Distill.&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;a href=&#34;https://distill.pub/about/&#34;&gt;https://distill.pub/about/&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Jobs to be Done A Case Study in the NHS | Digital transformation blog</title>
      <link>http://scholarly-comms-product-blog.com/2018/06/29/jobs_to_be_done_a_case_study_in_the-nhs__digital_transformation_blog/</link>
      <pubDate>Fri, 29 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2018/06/29/jobs_to_be_done_a_case_study_in_the-nhs__digital_transformation_blog/</guid>
      <description>&lt;p&gt;This is a great post on using the jobs to be done framework. There are two specific enhancements that are discussed - how to weight those jobs, and how to use granularity of the jobs to aid the design process (less granularity gives more freedom in the design phase).&lt;/p&gt;
&lt;p&gt;The authors wanted to use surveys to get a sense of importance of the under served needs, but in the end had to resort to a simple variant of card sorting. They &lt;em&gt;still&lt;/em&gt; ended up with too much information.&lt;/p&gt;
&lt;p&gt;One of the big powers of the job stories was in using them to situate the use case (moving from the general to the specific).&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;a href=&#34;http://transformation.blog.nhs.uk/jobs-to-be-done-a-case-study-in-the-nhs&#34;&gt;http://transformation.blog.nhs.uk/jobs-to-be-done-a-case-study-in-the-nhs&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>blockchain in STEM - part 2</title>
      <link>http://scholarly-comms-product-blog.com/2018/06/26/blockchain_in_stem_-_part_2/</link>
      <pubDate>Tue, 26 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2018/06/26/blockchain_in_stem_-_part_2/</guid>
      <description>&lt;p&gt;In my last post I gave an &lt;a href=&#34;http://scholarly-comms-product-blog.com/2018/06/19/blockchain_in_stem_-_part_1/&#34;&gt;overview of what blockchain is&lt;/a&gt; (while also confusing &lt;a href=&#34;https://www.youtube.com/watch?v=XhzpxjuwZy0&#34;&gt;House of Pain&lt;/a&gt; and &lt;a href=&#34;https://www.youtube.com/watch?v=RijB8wnJCN0&#34;&gt;Cypress Hill&lt;/a&gt;. (These posts are probably best read whilst listening to either of those songs).&lt;/p&gt;
&lt;p&gt;In this post I’m going to look at potential use cases for blockchain in STEM and scholarly publishing.&lt;/p&gt;
&lt;h1 id=&#34;scholarly-communications-use-cases&#34;&gt;Scholarly communications use cases.&lt;/h1&gt;
&lt;p&gt;When thinking about any use cases in STEM I think the questions we need to answer are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What are the kinds of records that we want to place on a public log, and if those records deal with “transactions” what kind of transactions make sense within the context of scholarly comms?&lt;/li&gt;
&lt;li&gt;Who might take part in the verification network for a given transaction type?&lt;/li&gt;
&lt;li&gt;How are those participants going to see value in the tokens of exchange sot that they are incentivised to participate?&lt;/li&gt;
&lt;li&gt;Crucially, for any given transaction type, how is a blockchain based system better than the system we already have?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There is also another key element at play in the world of scholarship, and that is that we already have a global decentralised, self correcting, append only log file that has been fairly robust against attack since the middle of the seventeenth century, and thats the published scholarly record. (Some people are claim that this is under threat due to the growth of poorly reviewed papers, or non reproducible research, but in the long run as Feynamn said&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;For a successful   technology, reality must take precedence over public relations, for nature cannot be fooled&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;OK let’s enumerate a few potential scholarly use cases and apply our four key questions above to them. I’m know that this list is not exhaustive but it should give us fairly broad coverage for what are probably typical use cases, and so should be instructive about whether blockchain could fit in somewhere.&lt;/p&gt;
&lt;h3 id=&#34;priority-claims&#34;&gt;Priority Claims&lt;/h3&gt;
&lt;p&gt;We can kind of think of knowledge claims as a transaction of sorts. Certainly when we talk about “standing on the shoulders of giants” we naturally think of new discoveries as adding to an existing logbook of knowledge. We know that there are sometimes problems with verifying claims of priority, so let’s look at some claims-based use cases.&lt;/p&gt;
&lt;h4 id=&#34;claims-about-authorship-of-a-paper&#34;&gt;Claims about authorship of a paper&lt;/h4&gt;
&lt;p&gt;Currently we use CrossRef as a centralised repository for information around publication, with a close link to authorship claims (made tighter if authors use an ORCID). To replace, or even supplement, this system with a blockchain based system we have a few options on who might  participate to verify the blockchain- publishers, authors, funders, institutions or a mix of these.&lt;/p&gt;
&lt;p&gt;The current system defers the writing of records in to the system to publishers only, so getting authors involved too could be good, however the current gatekeepers provide validation onto the record though existing peer review and I think there would need to be some kind of post-claim review system in place for a blockchain based system to be useful.&lt;/p&gt;
&lt;p&gt;A robust post claim system and post publication review system would also add value to the existing system, and their existence is not dependant the existence of a blockchain system, however I do think that the blockchain system is dependant on some post pub system before it could be valuable, as we would essentially allow any writes to the log, and look for verification later.&lt;/p&gt;
&lt;p&gt;(We might be seeing that emerge with the way discovery is happening around preprint servers, so maybe we don’t have to wait that long at all, but right now it remains a punt).&lt;/p&gt;
&lt;p&gt;One reason why you need to build that in to your blockchain design is the current system has evolved workarounds to do with how we retract papers or how we tombstone them, and though these systems can be a bit messy, the affordance to do this is there. That would have to be designed in to a new system, probably from the get go, as the record, once created, cannot be altered, you can’t have a do-over in terms of the key metadata that you record for your early transactions, though you could have “correcting“ transactions later (you can see I’m trying to work my way through this as I write).&lt;/p&gt;
&lt;p&gt;Unless the system is almost frictionless to setup (in terms of pushing claims onto this chain) I don’t see any significant numbers of academics taking part. That friconlessness will need to look something like a copy of the work or claim being routed to the public chain as part of the authoring or submission software. That requires either all authors to adopt one similar plugin or extension to their writing environments, or all submissions vendors to offer this as an affordance to authors in a way that does not add complexity to the existing article submissions process. Probably the default outcome is that it will just be institutions and publishers who might take part, and they will probably do so by adopting a platform or tech solution offered to them by a vendor.&lt;/p&gt;
&lt;p&gt;That vendor interaction is probably going to constrain the creation of a truly open network, as the given vendor is probably going to deploy and run “independent nodes” on behalf of their clients. Any yet theses clients already have trust in each other and enhance that trust through participating in standards like ORICD and CrossRef.&lt;/p&gt;
&lt;p&gt;Any vendor solution has to be built on top of an open protocol, because distributed trust, remember, so some independents could run nodes. What then is the “token” that we exchange for writing and verifying claims of ownership. If its a general token that can be used for tracking any transaction then how do we constrain this network to only be abut this one STEM use case? If its a token related solely to claims about ownership how do we get people to value that? Were this system to scale it would need to be able to keep tabs on several million transactions per year.&lt;/p&gt;
&lt;p&gt;Lets say I am an independent and I run my node, it adds records about verified authorship claims and I get given what? A token I can use to submit my paper? Do I stop running nodes when I stop being an active academic? Do I only run the node when I’m getting ready to submit? Does my research grant come with a % that can be used to solve proof of work algorithms so allow me to get published, what if I hack my local university supercomputer in order to get “published” faster? Does that lock out people from publishing from “energy poor” and “compute poor” institutions?&lt;/p&gt;
&lt;p&gt;I’m not seeing this system as cleanly solving some of the existing problems that we have in expanding access to claims of ownership, and I’m not seeing this as being an area that is currently a low trust one.&lt;/p&gt;
&lt;p&gt;In addition I think there is a potential to lock in an existing danger. We trump priority claims in science, and yet what we know about how science operates tells us that it is ultimately a collaborative activity. The existing publish or perish system puts too much pressure on priority, many people have recognised that we should be working harder to eliminate the risk of gazumping in publishing, yet I fear that by encoding this kind of behaviour onto a global public unchangeable record we will only exacerbate that problem.&lt;/p&gt;
&lt;h4 id=&#34;reviews-of-articles&#34;&gt;Reviews of articles&lt;/h4&gt;
&lt;p&gt;There is a problem around being able to verify whether a reviewer is real. A blockchain system could make a public record of all published reviews, but this runs into a couple of issues. Reviewers are still mostly unwilling to sign and publish reviews (that issue could be significantly helped by requiring all reviewers to have an ORCID, and that would probably help more than having a crypto token of the review pushed into a public log).&lt;/p&gt;
&lt;p&gt;For review transfers if a publisher want to send a manuscript to another publisher, a public log around metadata of reviews might help, but can you add to that log for a paper that is not published yet? For a blockchain like system to work for review transfers we would probably first need to be in a situation where all authors are willing to have metadata around their manuscripts made public before publication, so a kind of cloaked crypto version of F1000 Research. Hmmm. I’ll just leave that thought there and gently step away from it.&lt;/p&gt;
&lt;h4 id=&#34;tracking-article-versions-from-preprint-to-publication&#34;&gt;Tracking article versions from preprint to publication&lt;/h4&gt;
&lt;p&gt;If the artefact is in a preprint server then we could use a blockchain to track metadata about the version history of the article. On the other hand, instead of trying to figure out who should participate in running nodes, we could just use the existing support for preprints that &lt;a href=&#34;https://www.crossref.org/categories/preprints/&#34;&gt;Crossref provides&lt;/a&gt;, so here our choice is to build an unknown architecture at scale that faces significant adoption problems, or use an existing scalable solution that is already being used by everyone.&lt;/p&gt;
&lt;h4 id=&#34;claims-about-generation-of-data&#34;&gt;Claims about generation of data&lt;/h4&gt;
&lt;p&gt;I think there might be a potential use case here. It could be of value for labs to be able to advertise that they have kinds of data,  or have generated kinds of data, without making the data itself available.&lt;/p&gt;
&lt;p&gt;Where there is a need to be able to make claims around priority of data then being able to put a hash of your data set into a public ledger and then verify later that you have the underlying data, even if your papers come out after papers from other groups who might have the same data, could be beneficial.&lt;/p&gt;
&lt;p&gt;There are a couple of problems with this though. Data management is at a stage where if you are dealing with biological data it is almost impossible for two groups to end up with exactly the same data. The hashes that you produce as a representation of your data will be different to a hash of even almost indistinguishably different data, so even if there are data records on the blockchain, as a lab I’ll have no way of knowing if another group has already published a similar data set to mine :(.&lt;/p&gt;
&lt;p&gt;Given that it will be hard to check I won’t check so there won’t be much utility in this chain for the purpose of finding connections between research groups. Also, some labs will jump on the shiney new and start posting lots of entries to the chain, while many labs won’t.&lt;/p&gt;
&lt;p&gt;What a record on the chain will tell us is that a given lab was the first to add to the chain not the first to discover a specific data set. For this to work it would need to be blessed by a funder who would need to make appending to this chain be a requirement of funding.&lt;/p&gt;
&lt;p&gt;If we had lab equipment that did this anyway, then that might help, but we don’t have this kind of equipment. Moreover who is going to check? Those who check are most likely to be running centralised systems (i.e. funders who are trying to tack outputs) rather than this being a check being constantly done by the community. The funder could just ask for hashes of the data to be sent to them directly, or better yet, the actual data being deposited in a dark archive until publication.&lt;/p&gt;
&lt;h4 id=&#34;linking-research-artefacts-together&#34;&gt;Linking research artefacts together&lt;/h4&gt;
&lt;p&gt;If I want to create a record of all of my contributions, and create links between them (publishing the network of scholarly artefacts that I generate as a researcher) why would I not put those links into the public domain on to a blockchain? This does not require the entire ecosystem to come along with me, so we don’t have a cold start problem. I think the problem here is that this is just a derivative issue of the core issue around trust in the act of publication. So if you trust the authors on the key published elements you will trust them on these network claims, but publishing these network claims will not solve the problem of whether you trust them on the key publishing claims, so you are building a trust network where the verification of that trust for these links is largely meaningless as you have skipped over the core problem of trust.&lt;/p&gt;
&lt;h4 id=&#34;claims-about-facts-and-micro-statements&#34;&gt;Claims about facts and micro statements&lt;/h4&gt;
&lt;p&gt;What if I want to make claims about parts of a paper, make public claims on statements, facts or micro-publications? Why would I not push these onto a blockchain? Though here the use case is an interesting one, and is about creating a publishing system for something that we don’t handle well at this point in time in our infrastructures, I’m again finding it hard to understand what I would use the tokens for that get generated by the nodes who are validating these writes. Where this become powerful is in finally allowing fine grained credit to be assigned, a grand automation of the research object, however there are currently no incentive systems in place for doing this, and so adding a technological barrier to publishing something where the incentive becomes even more abstract is possibly not going to solve that incentive problem.&lt;/p&gt;
&lt;h3 id=&#34;resources&#34;&gt;Resources&lt;/h3&gt;
&lt;p&gt;The second category I want to think about is resource tracking and sharing. Where has a resource been used? Who has used it in the past? How might a set of resources be pooled and bid on? There are potentially a lot of ways that a public ledger could help with these kinds of questions.&lt;/p&gt;
&lt;h4 id=&#34;access-to-compute-time&#34;&gt;Access to compute time&lt;/h4&gt;
&lt;p&gt;If we have a distributed set of computing resources, people who hold those resources could offer them out and compute time could be exchanged for tokens. In this case the actual history contained in the ledger is probably only of interest in terms of ensuring that the tokens being exchanged are validated, and I could image that those tokens could be used in the future for requesting compute time.&lt;/p&gt;
&lt;p&gt;I think there are a couple of problems with this approach. The first is that compute time within the academy is already well provisioned, with many nations putting resource in creating supercomputing networks. Also cloud providers provide cheap scalable compute resources.&lt;/p&gt;
&lt;p&gt;Creating an open market place where there is a public history of transactions, but where that history is probably of no intrinsic interest seems wasteful when the existing alternatives seem to be getting the job done. Why create a “token” of exchange for compute time, when the exiting token of exchange — money — does very well. If we say that people have plenty of spare computing time sitting around, and a blockchain system will allow that spare capacity to be used, well, I’d need to be convinced that our core resource shortage is compute time, and I don’t believe that this is the case. (For what it;’s worth I think the key resource constraint that we are gong to be facing soon is going to be human attention time, and for now I’m unwilling to tokenise parts of my brain).&lt;/p&gt;
&lt;h4 id=&#34;access-to-lab-time&#34;&gt;Access to lab time&lt;/h4&gt;
&lt;p&gt;This is a similar use case to the previous one. We are just using a public ledger to create a kind of currency. I don’t see much reason to replace existing currencies with a crypto currency here.&lt;/p&gt;
&lt;h3 id=&#34;tracking-of-physical-reagents&#34;&gt;Tracking of physical reagents&lt;/h3&gt;
&lt;p&gt;I think there really might be some kind of use case here. Blockchain is finding a lot of interest in the world of international logistics, and indeed where experiments are using reagents, or other equipment, being able to produce a provenance file of where everything came from could be really useful. Moreover, if as a lab you create a new resource for public consumption, being able to register that resource while at the same time showing how it is connected to elements in the supply chain that came before it might be interesting. This is a use case that I can speculate on, but I also recognise that I really don’t know much about this area of lab work.&lt;/p&gt;
&lt;p&gt;I guess what participants in this network are hoping for is that by making their materials more traceable, and by having others sign up to doing the same, they can have an expectation of eventually receiving more reliable and more traceable elements themselves, so there is a gradient that could push more and more people to participate.&lt;/p&gt;
&lt;p&gt;What we are almost looking for here is a hope that various players will move towards transparency and standardisation and use the blockchain as a technology for converging on to drive that standardisation.&lt;/p&gt;
&lt;p&gt;So what then is the unit of transaction, the token? How does that play in this use case?&lt;/p&gt;
&lt;p&gt;I’m genuinely at a loss to answer that question.&lt;/p&gt;
&lt;h3 id=&#34;rights&#34;&gt;Rights&lt;/h3&gt;
&lt;p&gt;The last broad use case is about rights management. How do I know that I have the rights to an object, and how do I know that I can transfer those rights? At the moment we need to base this on contracts and central clearing houses, however could blockchain greatly simplify this for the academic market?&lt;/p&gt;
&lt;h4 id=&#34;rights-transfers-around-copyright-articles-or-journals&#34;&gt;Rights transfers around copyright, articles or journals&lt;/h4&gt;
&lt;p&gt;When authors transfer copyright to journals could this be pushed to a blockchain? I think we can be sure that the authors won’t do this, so it will be up to the publishers to do this. Other actors supporting the network of validation will be the publishers, so there has to be a sufficient cost saving for publishers to do this in terms of how they currently advertise rights to libraries and to each other.&lt;/p&gt;
&lt;p&gt;The key thing here is the former - I think. The current system of how rights and holdings files, and access to content via IP restriction is filled with bogey men, and as I understand it from speaking to colleagues the system is somewhat complex. So if we had all of that information in a public ledger then granting access to rights might be something that could be automated — but, and I think this is a big but — one of the reasons the current system is complicated is that there are many players, so gaining adoption is going to be hard.&lt;/p&gt;
&lt;p&gt;We would have to run this “rights blockchain” in parallel for some time. Initially you would have a small number of participants. What you would need to do is create a small collaboration of publishers who would sign up to keeping this chain up to date, would commit to ensuring the blockchain is valid, and would be willing to invest in getting good data into that system.&lt;/p&gt;
&lt;p&gt;You would ideally like some pilot librarians to come on-board, but they would probably need to be bank rolled by this “STM blockrighs consortium” for the initial phase of roll out. I mean, I think this would be interesting, but I think it’s moving against the grain where libraries, institutions and funders are seeking universal open access, so I would not put my money or resources into a project like this.&lt;/p&gt;
&lt;p&gt;The following two tweets capture some of the problems with blockchain for rights&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/tommorris/status/1006478693273882624&#34;&gt;Tom Morris  🏳️‍🌈 on Twitter: “”We’ll put ownership of IP rights on a blockchain”Translation: “we want to privatise the IP system, create a monopoly who will charge high, unpredictable fees, and remove any judicial oversight from a system that relies on that to curb the worst abuse”.Make it stop.”&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/edent/status/1006248586395508737&#34;&gt;꧁Terence Eden꧂ ⏻ on Twitter: “I don’t understand the blockchain hype.A startup has certified my artwork &amp;amp; placed their verification on the bitcoin blockchain.Now art dealers &amp;amp; auctioneers can feel secure that I am the original artist.One small problem… I am not Leonardo da Vinci!https://t.co/9219OcPsVW… https://t.co/3huflle9La”&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;an-addendum-on-pessimism&#34;&gt;An addendum on pessimism&lt;/h3&gt;
&lt;p&gt;As I write up these use cases, I recognise that I am being deliberately pessimistic, and perhaps that’s not fair. I’m seeing problems and barriers, and the truth is that for a new technology to mature to utility it does have to overcome a lot of barriers, and it’s the dreamers and enthusiasts that drive forward and find solutions. One the other hand, it’s got to be good to have an open debate about the kinds of problems that these technologies might need to face.&lt;/p&gt;
&lt;p&gt;In his &lt;a href=&#34;https://www.ben-evans.com/benedictevans/2017/11/29/presentation-ten-year-futures&#34;&gt;presentation on ten ear futures&lt;/a&gt;  Benedict Evans talks about how we often think about how a new technology will be applied to existing use cases at the start of the adoption curve, but at the end of the adoption curve if a new platform or technology is truly transformative then the use cases that end up being used are totally different to ones we imagined. My take on blockchain in STEM is broadly that there are no existing use cases that are a compelling fit for the tech, but that is not to say that there won’t be other new use cases in the future.&lt;/p&gt;
&lt;p&gt;In my next post I’m going to compare some of my own thinking with what other people are saying abut blockchain in this space and wrap up with some final thoughts, I should get that post out sometime next week.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OA negotiation manifesto from university of California.</title>
      <link>http://scholarly-comms-product-blog.com/2018/06/21/oa_negotiation_manifesto_from_university_of_california./</link>
      <pubDate>Thu, 21 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2018/06/21/oa_negotiation_manifesto_from_university_of_california./</guid>
      <description>&lt;p&gt;This is a really interesting initiative from the university of California. If the scholarly landscape looked like this then publishers would have to generate revenue entirely from services and derivative open products, rather than from content licensing.&lt;/p&gt;
&lt;p&gt;Most of the points is the manifesto are fairly unsurprising but two points stood out as interesting to me.&lt;/p&gt;
&lt;p&gt;Point 10 asks for all metadata to be made available including usage metadata. Are Counter reports sufficient for this, or is anything else needed?&lt;/p&gt;
&lt;p&gt;Point 11 is really radical. It asks for universities to be compensated for the time that their academics spend doing peer review. I leave it as an exercise to the reader to figure out how that might work in practice (hint - it would be really hard to get full accounting for this time, so some kind of semi-arbitrary figure wold be needed, but that could start to marketise a kind of effort that might be best left outside of the marketplace).&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;a href=&#34;https://senate.universityofcalifornia.edu/_files/committees/ucolasc/scholcommprinciples-20180425.pdf&#34;&gt;https://senate.universityofcalifornia.edu/_files/committees/ucolasc/scholcommprinciples-20180425.pdf&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Preprints growth rate ten times higher than journal articles - Crossref</title>
      <link>http://scholarly-comms-product-blog.com/2018/06/21/preprints_growth_rate_ten_times_higher_than_journal_articles_-_crossref/</link>
      <pubDate>Thu, 21 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2018/06/21/preprints_growth_rate_ten_times_higher_than_journal_articles_-_crossref/</guid>
      <description>&lt;p&gt;Great post looking into stats on preprints in crossref. Headline takeaways, preprint registration into crossref is 10x that of article growth, but it&amp;rsquo;s hard to read a lot into that as the absolute numbers are so different at the moment 2.4M per month (published articles) vs 10k per month (preprints). There is also some interesting data on preprint citations, preprint citations come in at best at a level of about 10% of the citations to the subsequently published article. Would be interesting to do some longitudinal studies on this.&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;a href=&#34;https://www.crossref.org/blog/preprints-growth-rate-ten-times-higher-than-journal-articles/&#34;&gt;https://www.crossref.org/blog/preprints-growth-rate-ten-times-higher-than-journal-articles/&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scholia</title>
      <link>http://scholarly-comms-product-blog.com/2018/06/21/scholia/</link>
      <pubDate>Thu, 21 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2018/06/21/scholia/</guid>
      <description>&lt;p&gt;Scholia is an amazing interface into scholarly information held inside id WikiData. It includes information about authors, articles and a very large chunk of the citation graph. You can see an article pate here: &lt;a href=&#34;https://tools.wmflabs.org/scholia/work/Q24595162&#34;&gt;https://tools.wmflabs.org/scholia/work/Q24595162&lt;/a&gt;. The tool extracts topic information on articles, shows cites and citing articles (and how many citations each of these articles has)&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;a href=&#34;https://tools.wmflabs.org/scholia/&#34;&gt;https://tools.wmflabs.org/scholia/&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Blockchain in STEM - part 1</title>
      <link>http://scholarly-comms-product-blog.com/2018/06/19/blockchain_in_stem_-_part_1/</link>
      <pubDate>Tue, 19 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2018/06/19/blockchain_in_stem_-_part_1/</guid>
      <description>&lt;p&gt;A lot of people are taking about “blockchain for science” and “blockchain for publishing”, but I’m skeptical. Some of the people taking about this are really smart, so I could be wrong.&lt;/p&gt;
&lt;p&gt;If we think of scholarly publishing as being like the connective tissue of science, and we accept that this idea is gaining purchase within our community that we have to realise that we are looking at a case of&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;blockchain in the membrane&lt;br&gt;
blockchain in the brain&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So, are we going to &lt;code&gt;jump jump&lt;/code&gt; to a better future, or are we just looking at entering a house of pain? In this short series of posts I’ll explore some of my own thinking about this topic.&lt;/p&gt;
&lt;p&gt;This is the first of a three part series where I discuss my understanding of what blockchain is.&lt;/p&gt;
&lt;p&gt;In the &lt;a href=&#34;http://scholarly-comms-product-blog.com/2018/06/26/blockchain_in_stem_-_part_2/&#34;&gt;second post&lt;/a&gt; I discuss some use cases and in the third post in the series I&amp;rsquo;ll look at what some other people are saying about the technology in this sector.&lt;/p&gt;
&lt;h3 id=&#34;what-do-i-think-blockchain-is&#34;&gt;What do I think blockchain is?&lt;/h3&gt;
&lt;p&gt;Heres my understanding of what blockchain is.&lt;/p&gt;
&lt;p&gt;Blockchain is a public “append only“ logfile (or ledger), copies of which can be held by anyone. We can call the collection of people who are participating a network, and we can call any single participant a node in this network.&lt;/p&gt;
&lt;p&gt;There is no central source of truth, but there is an algorithm that aims to allow all copies of this log file to become “eventually consistent”.
This log file is not a database, nor a key-value store. The core log file tends to be a fairly dumb flat file with no support for things like indexing or joins over different parts of the data that might be contained in the file.&lt;/p&gt;
&lt;p&gt;When I talk about a &lt;code&gt;log&lt;/code&gt; file or a a &lt;code&gt;ledger&lt;/code&gt; I’m really talking about nothing more sophisticated than a flat file that has time stamped entries on it.&lt;/p&gt;
&lt;p&gt;When I say &lt;code&gt;append only&lt;/code&gt; what I mean is that given a copy of this file the system &lt;em&gt;should&lt;/em&gt; not allow anyone to go in and change any of the things that are already in the file, it should only allow you to add new things to the end of the file.&lt;/p&gt;
&lt;p&gt;By describing this file as being &lt;code&gt;eventually consistent&lt;/code&gt; what we mean is that two nodes who whose copies get out of sync can find a way to agree which of the two different copies should be discarded in favor of the other one (since all the log files can be tampered with you have to throw one away rather than rewrite it).&lt;/p&gt;
&lt;p&gt;Differences in copies can happen because  one node might get information about changes before a second node, and that second node could write some other data onto the blockchain before the changes that the first node made are seen by the second node.&lt;/p&gt;
&lt;p&gt;this process of picking winners when there are discrepancies works to make sure that in time all copies are the same, and we call this eventual consistency.&lt;/p&gt;
&lt;p&gt;There are some well known important append only data stores (wikipedia, for example, keeps track of every change to every page and makes this data publicly available. A linked data version of wikipedia — &lt;a href=&#34;https://wiki.dbpedia.org&#34;&gt;DBPedia&lt;/a&gt; — has become a central resource in the linked data world).&lt;/p&gt;
&lt;p&gt;There are plenty of databases out there that operate at scale, can be decentralised and have different mechanisms to allow them to get to consistency, however this is not an easy engineering problem at scale (see &lt;a href=&#34;https://jepsen.io&#34;&gt;Jespen&lt;/a&gt; for example). These distributed databases are usually run by a single organisation that can be spread over a large number of machines in variety of different locations, rather than as a truly open distributed system.&lt;/p&gt;
&lt;p&gt;There are many open distributed peer to peer networks that people participate in, the Tor and Onion networks are good examples, as are the bit torrent and &lt;a href=&#34;https://freenetproject.org/author/freenet-project-inc.html&#34;&gt;freenet&lt;/a&gt; networks, but these networks are more for data sharing rather than making a system that creates a public log file that anyone can inspect.&lt;/p&gt;
&lt;p&gt;What does blockchain get you that some of these other systems don’t?&lt;/p&gt;
&lt;p&gt;By making this file public on the network, and allowing anyone to take part in storing a copy, and take part in running the algorithm that sets eventual consistency across all copies when new writes come in to the log file, if you get enough people taking part on that open network, you can remove the need for any central source of truth or central authority or gatekeeper on the writes that are added to the log file.&lt;/p&gt;
&lt;p&gt;So getting rid of a central authority, for some use cases, can sound like a really intriguing idea, but what are the costs associated with running a node on this kind of network?&lt;/p&gt;
&lt;p&gt;Well, you need to ensure that your consistency algorithm is immune to attack otherwise there can be little trust in the veracity of the records written to this log file. In order to do this the blockchain protocol involves a thing called “proof of work”. Proof of work makes it artificially slow for any single node to verify the new writes onto the blockchain, and by so doing it allows all of the nodes on the network to have enough time to stay in-sync without a number of them being taken over by a bad 3rd party. This proof of work is computationally expensive and if you run a node on a large blockchain network where the log file is quite big, you end up having a large electricity bill. There is an actual significant cost.&lt;/p&gt;
&lt;p&gt;So you need a mechanism to get people to take part in being on the network, for if there are not enough people on the network, the network could be attacked easily.&lt;/p&gt;
&lt;p&gt;The blockchain protocol does this by rewarding nodes with tokens, and those tokens in turn become the route to adding new entires to the log file. Now rather than just having a log file with dates and some data we have a log file that tracks dates, use of the tokens for writing to the log file, and the data associated with the write.&lt;/p&gt;
&lt;p&gt;If the data we associate with the writes is data about exchanging goods or services, then the tokens become a kind of unit of exchange, and so we often talk about the writes to the blockchain as being transactions. The writes can also just be about moving the ownership of these tokens around the network.&lt;/p&gt;
&lt;p&gt;What do we gain, and what do we lose, by opting for using blockchain over some other data storage mechanism? Well we gain the ability for verified transactions to occur without a central authority — so long as the consistency algorithm is robust. We probably lose the ability to scale quickly, and we certainly lose the ability to verify transactions in a time and cost efficient manner.&lt;/p&gt;
&lt;p&gt;In a way we are trading time and energy for trust.&lt;/p&gt;
&lt;p&gt;What do we need to have in place for a blockchain system to work? Well we need enough independent nodes on our network to make our blockchain tamper proof. We also need all of these nodes to be incentivised to participate, so either the transaction costs have to be really low (making it easy to attack, so not good), or the tokens granted for participation have to be seen to be of sufficient value that those running nodes will continue to do so, even as time and energy costs rise.&lt;/p&gt;
&lt;p&gt;In low trust ecosystems (where there is systemic corruption, or regular payment systems are badly degraded or structural bias exists to prevent people from getting credit), then I can see potential value for blockchain like systems, but I’m not sure if that value has been fully realised yet as the barrier to entry to these systems means that the technical bar itself acts as a kind of bias against the use of these systems by the weak and marginalised.&lt;/p&gt;
&lt;p&gt;In systems where there are many existing traditional actors who have a forrest of standards and interoperability issues I can see value in all players converging on one way to do things as a means to reduce frictions in those systems, but the explosion of ICOs and pennant for any largish blockchain system to end up forking indicates to me that blockchain is not yet the holder of the secret to creating industry-wide standardisation for any systems.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Digital Radar - STM landscape</title>
      <link>http://scholarly-comms-product-blog.com/2018/06/07/digital_radar_-_stm_landscape/</link>
      <pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2018/06/07/digital_radar_-_stm_landscape/</guid>
      <description>&lt;p&gt;Thoughtworks have created a tool to allow you to build your own &amp;ldquo;Digital Radar&amp;rdquo;. The one linked to here was put together a few years ago by people at the BMJ to look at the technology landscape in STM publishing (There are some really interesting things in there in some interesting locations).&lt;/p&gt;
&lt;p&gt;&amp;lt;a href=https://radar.thoughtworks.com/?sheetId=https%3A%2F%2Fdocs.google.com%2Fspreadsheets%2Fd%2F1zQPRcn76XHKxex7ytdmTKC9nXMPPK0Tv0-hMlSY6xVU%2Fpubhtml&amp;gt;https://radar.thoughtworks.com/?sheetId=https%3A%2F%2Fdocs.google.com%2Fspreadsheets%2Fd%2F1zQPRcn76XHKxex7ytdmTKC9nXMPPK0Tv0-hMlSY6xVU%2Fpubhtml&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Strategy in a Lean Enterprise - lean value tree.</title>
      <link>http://scholarly-comms-product-blog.com/2018/06/07/strategy_in_a_lean_enterprise_-_lean_value_tree./</link>
      <pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2018/06/07/strategy_in_a_lean_enterprise_-_lean_value_tree./</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been working with lean value tree as a framework for some time now, but there are few online resources about this. The linked presentation does a great job of giving an overview of the tool. In particular I like how they call out the need to describe the promise of value to the customer, something that we could definitely do more of.&lt;/p&gt;
&lt;p&gt;Another presentation on the same topic is this one: &lt;a href=&#34;https://www.slideshare.net/steve236/lean-value-tree-overview-82783795&#34;&gt;https://www.slideshare.net/steve236/lean-value-tree-overview-82783795&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;a href=&#34;https://www.slideshare.net/OllieStevensonGoldsm/strategy-in-a-lean-enterprise&#34;&gt;https://www.slideshare.net/OllieStevensonGoldsm/strategy-in-a-lean-enterprise&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>scholarly comms product meetup - V2 - announcement</title>
      <link>http://scholarly-comms-product-blog.com/2018/04/18/scholarly_comms_product_meetup_-_v2_-_announcement_/</link>
      <pubDate>Wed, 18 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2018/04/18/scholarly_comms_product_meetup_-_v2_-_announcement_/</guid>
      <description>&lt;p&gt;Back in November of last year we ran the first scholarly comms meetup with a focus on product management. There are lots of great meetups out there for people who work in scholarly comms, but we felt that there might be an unscratched need to have a meeting where the focus was not explicitly on community building, or on new technologies, or on public outreach, or on new trends and technologies, but solely on the product management work that is required to develop these kinds of tools.&lt;/p&gt;
&lt;p&gt;We hoped we could get people together to chat about war stories, share experiences, and in a friendly and encouraging environment open up about the common difficulties that we encounter in attempting to create high quality services for academics. The event went really well, and after it happened we said we hoped that we would do it again.&lt;/p&gt;
&lt;p&gt;We it’s back!, and this time we have a focussed topic: GDPR and it’s implications on product management in the scholarly communication space.&lt;/p&gt;
&lt;p&gt;If you are in London please join us on the &lt;strong&gt;1st of May&lt;/strong&gt; at the SAGE offices. The event is super informal, a few speakers cover some topic of interest, and then we chat over pizza and beer.&lt;/p&gt;
&lt;p&gt;We are space limited, so you can get a ticket by signing up at our &lt;a href=&#34;https://www.meetup.com/londons-scholarly-tech/events/249629141/&#34;&gt;meetup page&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ian&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>about</title>
      <link>http://scholarly-comms-product-blog.com/about/</link>
      <pubDate>Mon, 01 Jan 2018 17:40:52 -0700</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/about/</guid>
      <description>&lt;p&gt;This blog has a focus on product development in scholarly communications. There is going to be a large focus on scholarly publishing. Product development covers a huge range of topics. I&amp;rsquo;m interested in &lt;strong&gt;tools for product development&lt;/strong&gt;, &lt;strong&gt;technologies that impact how we design and build products&lt;/strong&gt;, &lt;strong&gt;business models and landscape changes&lt;/strong&gt; and &lt;strong&gt;machine learning and AI&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve been blogging on a variety of topics for some time. This blog brings together some of my posts on the topic of scholarly comms.&lt;/p&gt;
&lt;p&gt;If you have an idea you would like to see me blog about, or you would like to submit a post then please ping me on &lt;a href=&#34;https://twitter.com/ianmulvany&#34;&gt;twitter&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://mulvany.net&#34;&gt;Ian Mulvany&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The robots are coming, the promise and perils of AI - questions</title>
      <link>http://scholarly-comms-product-blog.com/2017/11/09/the_robots_are_coming_the_promise_and_perils_of_ai_-_questions/</link>
      <pubDate>Thu, 09 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2017/11/09/the_robots_are_coming_the_promise_and_perils_of_ai_-_questions/</guid>
      <description>&lt;p&gt;I&amp;rsquo;m at the Charleston conference, my first time, and we &lt;a href=&#34;https://2017charlestonconference.sched.com/event/CCxm/all-the-robots-are-coming-the-promise-and-the-peril-of-ai&#34;&gt;had a panel discussion this morning talking about AI&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;On the panel were:&lt;/p&gt;
&lt;p&gt;Heather Staines
Director of Partnerships, Hypothes.is&lt;/p&gt;
&lt;p&gt;Peter Brantley
Director of Online Strategy, UC Davis&lt;/p&gt;
&lt;p&gt;Elizabeth Caley
Chief of Staff, Meta, Chan Zuckerberg Initiative&lt;/p&gt;
&lt;p&gt;Ruth Pickering
Co-founder and Chief Strategy Officer, Yewno&lt;/p&gt;
&lt;p&gt;and myself. It was a pleasure to be on a panel with these amazing people.&lt;/p&gt;
&lt;p&gt;There was a lot of interest from the audience, and we didn&amp;rsquo;t get anywhere close to talking through all of the questions that we had discussed as a panel ahead of the session, so I&amp;rsquo;m going to blog the questions that we had prepared. Importantly below are mostly &lt;strong&gt;my answers&lt;/strong&gt; to those questions. The answers below don&amp;rsquo;t necessarily represent the views of my fellow panellists, and in fact I&amp;rsquo;m pretty sure we hold divergent opinions on some of these questions, but that&amp;rsquo;s OK, it&amp;rsquo;s a complex topic. Some of the best answers below do actually belong to the other panelists, but google doc tells me that author was anonymous.&lt;/p&gt;
&lt;p&gt;Before diving in to running over the questions I want to make a quick statement around one topic that came up a bit in the Q&amp;amp;A. We got to talking about how &amp;ldquo;AI&amp;rdquo; can solve problems, would &amp;ldquo;AI&amp;rdquo; have prevented the financial crisis were we to have had better insight into the market? and so on.&lt;/p&gt;
&lt;p&gt;This framing of the conversation makes it sound like AI has agency in the solution of these kinds of problems, but that is not the case at all. It is us as individuals and together as a society that has agency. AI is a tool that we build, and the builders must be accountable for the usage of these kinds of tools. Many of the problems that we talked about in the session will neither be solved nor made worse by AI itself, but rather by the actions we collectively take as a society. In one of the examples discussed in the Q&amp;amp;A around whether AI would have prevented the financial meltdown of 2007 I believe that many people acted in ways that contributed to the magnitude of that crisis, in spite of knowing the risks involved, and no amount of better information would have changed their minds.&lt;/p&gt;
&lt;p&gt;OK, on to the questions!&lt;/p&gt;
&lt;h1 id=&#34;what-is-ai-and-what-is-not&#34;&gt;What is AI and what is not?&lt;/h1&gt;
&lt;p&gt;AI is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Just a set of simple computations - but lots of them&lt;/li&gt;
&lt;li&gt;Entirely built by humans.&lt;/li&gt;
&lt;li&gt;Old, current models were originally proposed around 40 years ago&lt;/li&gt;
&lt;li&gt;Suddenly cheap to run&lt;/li&gt;
&lt;li&gt;Is not “one algorithm” - different specific tools for different types of data&lt;/li&gt;
&lt;li&gt;Surprisingly effective given a large training data set at:
&lt;ul&gt;
&lt;li&gt;pattern matching&lt;/li&gt;
&lt;li&gt;Feature detection in images / audio&lt;/li&gt;
&lt;li&gt;Inference&lt;/li&gt;
&lt;li&gt;Prediction&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;AI is not:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cognisant&lt;/li&gt;
&lt;li&gt;Self aware&lt;/li&gt;
&lt;li&gt;General&lt;/li&gt;
&lt;li&gt;Dangerous (currently)&lt;/li&gt;
&lt;li&gt;Going to take your job (possibly)&lt;/li&gt;
&lt;li&gt;Good at providing explanations or explaining causation&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;what-is-a-narrow-ai-vs-a-general-ai-_answer-is-from-the-other-panelists_&#34;&gt;What is a narrow AI vs a general AI? &lt;em&gt;(answer is from the other panelists)&lt;/em&gt;&lt;/h1&gt;
&lt;blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;Narrow AI accomplishes one task, or is focused on a specific application, whereas a General AI can provide insights to a broader range of challenges by integrating across a larger array of sources. Narrow AI is language translation, autonomous driving, image recognition. A General AI might be an omnipresent personal assistant . . we are not there yet.&lt;/p&gt;
&lt;h1 id=&#34;how-does-machine-learning-fit-into-ai-_answer-is-from-the-other-panelists_&#34;&gt;“How does machine learning fit into AI?” &lt;em&gt;(answer is from the other panelists)&lt;/em&gt;&lt;/h1&gt;
&lt;blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;Machine learning is a technical approach towards AI, a statistical process that utilises a body of data and tries to derive a rule or procedure that explains the data or can predict future data. This is, e.g., in contrast to older “expert systems” where humans tried to train or teach a proto-AI how to solve problems. Deep learning is a subfield of machine learning that uses “tiers” or layers, in which processing is handed from one tier to another for further derivation.&lt;/p&gt;
&lt;h1 id=&#34;why-is-the-availability-of-large-amounts-of-data-almost-mandatory-to-design-a-successful-ai&#34;&gt;Why is the availability of large amounts of data almost mandatory to design a successful AI?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;You need a large enough of a train g data set to be able to capture the features of the general case
&lt;ul&gt;
&lt;li&gt;Eg object recognition&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;For supervised training getting a corpus of high quality labelled training data can be a challenge.
&lt;ul&gt;
&lt;li&gt;Eg recapcha&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;For unsupervised learning the more data you have the higher the likely hod that you will be able to find “true” features of the data over “ghosts” of the process
&lt;ul&gt;
&lt;li&gt;E.g. using AI to extract new knowledge from scientific instruments.&lt;/li&gt;
&lt;li&gt;Most clustering algorithms need enough data to be able to identify real clusters.&lt;/li&gt;
&lt;li&gt;There may be something here about information bottleneck but I’m not sure.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;For simulation driven approaches you may want to run a lot of simulations and this leads to the generation of a lot of data as a by product.
&lt;ul&gt;
&lt;li&gt;E.g. teaching a machine how to play go, how to drive cars in video games.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;what-are-the-problems-of-bias-in-ai-how-might-they-affect-publishers-and-users&#34;&gt;What are the problems of bias in AI? How might they affect publishers and users?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Prediction and inference is based on the data we feed in. We can end up locking in existing bias is we don&amp;rsquo;t look critically at the way we train our algorithms. E.g. if we feed in data about previous convictions data to predict future crime, and convictions happened in an unfair and biased way, then those biases will cary over into the model.&lt;/li&gt;
&lt;li&gt;If we try to predict future “impact” of scholarly work we may bias for individual, institution and previous publication record rather than looking at the content of the ideas. My favourite work on productivity on the sciences come from Dashun Wang, and he shows that the “super productive” period is not correlated with time in career, so unless you can identify that someone is in the middle of this kind of phase in their career looking at past impact is a poor predictor of future impact. &lt;a href=&#34;https://www.youtube.com/watch?v=goO3dWrtWc4&#34;&gt;IC2S2 2017: Dashun Wang - YouTube&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;should-these-tools-be-regulated-how-might-that-happen&#34;&gt;Should these tools be regulated, how might that happen?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;This is critical, and is sometimes referred to algorithmic accountability, but it is challenging.&lt;/li&gt;
&lt;li&gt;A clear way to do this is to make the people who make these systems accountable for their outcomes. There is a discussion around allowing AIs to be granted synthetic personhood (&lt;a href=&#34;https://link.springer.com/article/10.1007/s10506-017-9214-9&#34;&gt;Of, for, and by the people: the legal lacuna of synthetic persons | SpringerLink&lt;/a&gt;) This is a bad idea, we have to remember that these systems have been built by people and need to remain accountable.&lt;/li&gt;
&lt;li&gt;Another route is data privacy, and in the EU there is a new framework coming into place from next year called the General Data Protection Regulation (GDPR). I know that SAGE is doing a lot of work right now to make sure we are compliant, and more importantly that everyone in the company is aware of the implications of the act so that we don’t become accidentally non compliant.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;how-has-the-widespread-availability-of-software-like-googles-tensorflow-helped-to-foster-ai-experimentation&#34;&gt;How has the widespread availability of software like Google’s TensorFlow helped to foster AI experimentation?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Matato Koike used tensorflow to build a &lt;a href=&#34;https://cloud.google.com/blog/big-data/2016/08/how-a-japanese-cucumber-farmer-is-using-deep-learning-and-tensorflow&#34;&gt;machine vision system to help his mothers&amp;rsquo; cucumber farm auto classify cucumbers&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Jacques Mattheij built a &lt;a href=&#34;https://jacquesmattheij.com/sorting-two-metric-tons-of-lego&#34;&gt;lego sorting machine&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;These all show that these tools are at the point where they can be used by the interested hobbyist.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;is-ai-is-most-useful-for-production-workflows-or-user-needs&#34;&gt;Is AI is most useful for production workflows or user needs?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Yes
&lt;ul&gt;
&lt;li&gt;Adam Matthew have rolled it out to enable &lt;a href=&#34;http://www.amdigital.co.uk/m-editorial-blog/handwritten-text-recognition/&#34;&gt;hand writing recognition in digitised collections&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;IET are using machine learning to auto add ontology terms and indexing terms to articles that are indexed in Inspec (&lt;a href=&#34;http://inspecdirect.theiet.org&#34;&gt;http://inspecdirect.theiet.org&lt;/a&gt;). This supports their taxonomists in their work.&lt;/li&gt;
&lt;li&gt;Springer Nature are using some techniques to create the classification and information hierarchy for a new portal they are working on about nano technology.&lt;/li&gt;
&lt;li&gt;Sage is not using it in production but is using it to help with business analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;could-ai-enhanced-writing-tools-help-guide-us-toward-writing-better-articles&#34;&gt;Could AI-enhanced writing tools help guide us toward writing better articles?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Yes, but it’s very early days and it’s so so hard to get people to switch tools. I thought google wave would be able to do this, but it didn’t take off :(&lt;/li&gt;
&lt;li&gt;Two examples though:
&lt;ul&gt;
&lt;li&gt;Biomed Central BMC are using a system called StatReviewer for auto checking for things like CONSORT statements and reported p-values.&lt;/li&gt;
&lt;li&gt;There is a small startup in london - &lt;a href=&#34;https://www.penelope.ai&#34;&gt;Penelope&lt;/a&gt;, that provides early feedback to authors on their manuscript on a whole host of things in the article (&lt;a href=&#34;https://www.penelope.ai/faq#checks-section)&#34;&gt;https://www.penelope.ai/faq#checks-section)&lt;/a&gt;. It’s being used by a few hundred academic a month at the moment.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;how-can-an-ai-help-with-determining-what-to-publish&#34;&gt;How can an AI help with determining what to publish?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;I think this is really hard at the moment, and I think it’s problematic.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;how-might-ai-improve-recommending-services-for-journals-for-monographs&#34;&gt;How might AI improve recommending services? For journals? For monographs?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;It’s not clear to me that this approach can get you, for example, an order of magnitude lift on engagement with recommendations over other mechanisms for doing this. I’d prefer to see work continue around trying to extract semantics from the literature and auto-identify conflicting statements or statistically weak results. Chris Hartgerink recently used text mining over 160K papers to &lt;a href=&#34;http://www.mdpi.com/2306-5729/1/3/14&#34;&gt;review the way statistics are being reported across an entire discipline&lt;/a&gt;. This kind of work can only be done through machine assisted methods, and I think will provide a lot more value than using AI to drive just paper recommendations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;would-it-be-possible-to-draw-in-other-external-information-about-a-users-likespatterns-eg-google-can-figure-out-a-reader-was-just-in-vienna-maybe-they-would-like-to-read-a-new-historical-murder-mystery-set-in-austria-hungary-that-just-came-out&#34;&gt;Would it be possible to draw in other external information about a user’s likes/patterns? (e.g. Google can figure out a reader was just in Vienna, maybe they would like to read a new historical murder mystery set in Austria Hungary that just came out?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;So yes, but to get all of this data and integrate it you need to be convinced that this is going to give you the best return on this bet. Getting this kind of data is hard and a bit creepy.&lt;/li&gt;
&lt;li&gt;I was an author on a white paper on this topic about 10 years ago, and we focussed on trying to understand how to &lt;a href=&#34;http://oro.open.ac.uk/21188/&#34;&gt;engineer serendipity&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;how-could-ais-help-derive-knowledge-that-might-not-be-apparent-to-scholars-in-the-data-by-making-inferences&#34;&gt;How could AIs help derive knowledge that might not be apparent to scholars in the data, by making inferences?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;See above about looking for systematic weakness in the full corpus.&lt;/li&gt;
&lt;li&gt;At the moment getting consistent semantics out of research papers to be able to create networks of inference remains beyond what AI can do, but we should expect this to become possible.
&lt;ul&gt;
&lt;li&gt;Watson’s forays into the biomedical literature have &lt;a href=&#34;https://www.forbes.com/forbes/welcome/?toURL=https://www.forbes.com/sites/matthewherper/2017/02/19/md-anderson-benches-ibm-watson-in-setback-for-artificial-intelligence-in-medicine/&amp;amp;refURL=https://www.google.com/&amp;amp;referrer=https://www.google.com/&#34;&gt;not planned out as expected&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.victorpankratius.com&#34;&gt;Victor Prankratius at MIT&lt;/a&gt; is one researcher working on machine generated discovery, but in his early forays into this field he found that working with the research literature to be too problematic as accessing full text is hard, text formats are inconsistent, and extracting meaning from papers is currently also very difficult. His group have made progress in generating new knowledge by looking at the raw data that comes out of scientific instruments, and in particular seismographs, so AI can do this kind of work today, but the real benefit will come when we can do this on the literature.&lt;/li&gt;
&lt;li&gt;People in the semantic web community have proved that you can construct these inferential networks eg &lt;a href=&#34;http://www.massgeneral.org/neurology/researcher_profiles/clark_timothy.aspx&#34;&gt;Tim Clarke&lt;/a&gt; has done a lot of work on this but their work depended on manually tagging a lot of micro publications, and that doesn’t scale right now. (I need to catch up with Tim to see where they are at with this kind of work.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;how-might-chatbots-influence-scholarly-communication&#34;&gt;How might chatbots influence scholarly communication?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;I have no opinion on this.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;right-now-it-might-seem-that-our-interaction-with-ais-is-occasional-episodic-how-might-ais-evolve-to-seem-to-be-always-with-us-guiding-our-work&#34;&gt;Right now, it might seem that our interaction with AIs is occasional, episodic. How might AIs evolve to seem to be always with us, guiding our work?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;I’d want to be very careful about engaging with a system on that level. There is a great short story by Bruce Sterling that beautifully describes the &lt;a href=&#34;https://adactio.com/extras/manekineko/&#34;&gt;comedic dystopia&lt;/a&gt; that could result . When we talk about guiding our work we have to have a sense of what we value in life and the risk to be avoided is that we have attention sinks entering our lives that are maximising for a set of values that are not ours. That said, our connected selves has increased the burden of communication along with the volume of information that we interact with. I would want an AI that could reply to status request emails.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;lets-end-on-a-positive-note-how-can-you-be-prepared-for-ai&#34;&gt;Let’s end on a positive note: how can you be prepared for AI?&lt;/h1&gt;
&lt;p&gt;At an individual level I’d recommend people take the time to learn some of the code that makes these things work.&lt;br&gt;
At an institutional level I would recommend that people review their data readiness and look for tasks that are time sinks that could be handed off to AIs, like cataloging, document conversion, copy editing. The answer below from someone else on the panel is fantastic:&lt;/p&gt;
&lt;blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;Riotously embed metadata, and insert data collection and harvesting breakpoints in services and platforms.
Ask others for data collected in transactions that involve you, if you can partner with them to improve services.
Think about organisational points of intersection or near-intersection and how AI might improve workflow or interactions with staff, contributors, or users.&lt;br&gt;
AI often ties together inputs and outputs by making inferences about outcomes: what are the things that you want to improve? For whom?&lt;/p&gt;
&lt;h1 id=&#34;how-can-we-position-ourselves-so-that-ai-benefits-are-not-the-sole-territory-of-google-fb-and-other-large-technology-players-aggregating-information&#34;&gt;How can we position ourselves so that AI benefits are not the sole territory of Google, FB, and other large technology players aggregating information?&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;O’Reilly media got their entire catalog into a knowledge graph that sits in memory on their Webserver. If you have high quality data and metadata you don’t need to be at google scale to start to take advantage.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Futurepub10</title>
      <link>http://scholarly-comms-product-blog.com/2017/03/16/future-pub-10/</link>
      <pubDate>Thu, 16 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2017/03/16/future-pub-10/</guid>
      <description>&lt;p&gt;This week I attended &lt;a href=&#34;https://www.digital-science.com/blog/events/overleafs-futurepub-10-coming-london-march-13th/&#34;&gt;futurepub10&lt;/a&gt;, I love these events, I&amp;rsquo;ve been to a bunch, and the format of short talks, and lots of time to catchup with people is just great.&lt;/p&gt;
&lt;p&gt;# A new Cartography of Collaboration - Daniel Hook, CEO Digital Science (work with Ian Calvert).&lt;/p&gt;
&lt;p&gt;Digital science have produced a &lt;a href=&#34;https://www.digital-science.com/resources/portfolio-reports/connected-culture-collaboration/&#34;&gt;report on collaboration&lt;/a&gt;, and this talk was covering one of chapters from that.&lt;/p&gt;
&lt;p&gt;I was interested to see what the key takeaways are that you can describe in a five minute talk. This talk looked at what could be inferred around collaboration by looking at co-authors actually using the Overleaf writing tool. It&amp;rsquo;s clear that there is an increasing amount of information available, and it&amp;rsquo;s also clear that if you have a collaborative authoring tool you are going to get information that was not previously available by just looking at the publication record.&lt;/p&gt;
&lt;p&gt;Daniel confirmed they can look at the likely journals for submission, based on the article templates, how much effort in time and content that each author is providing to the collaboration, how long it takes to go from initial draft to completed manuscript, which manuscripts end up not being completed. There is a real treasure trove of information here. (I wonder if you can call the documents that don&amp;rsquo;t get completed the &lt;code&gt;dark collaboration graph&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;In addition to these pieces of metadata there are the more standard ones, institute, country, subject matter.&lt;/p&gt;
&lt;p&gt;In spite of all of the interesting real-time and fine grained data that they have, for the first pass info they looked at the country - country relations. A quick eyeballing shows that the US does not collaborate across country boundaries as much as the EU does. The US is highly collaborative within the US.&lt;/p&gt;
&lt;p&gt;Looking at the country to country collaboration stats for countries in the EU I&amp;rsquo;d love to see what that looks like scaled per researcher rather than weighted by researchers per country, are there any countries that are punching above their weight per capita?&lt;/p&gt;
&lt;p&gt;In the US when you look at the State to State relations California represents a superstate in terms of collaboration. South Carolina is the least collaborative!!&lt;/p&gt;
&lt;p&gt;The measures of centrality in the report is based on document numbers related to collaborations.&lt;/p&gt;
&lt;h3 id=&#34;question-time&#34;&gt;Question Time!&lt;/h3&gt;
&lt;p&gt;The data that generates the report is updated in real time, but it seems like they don&amp;rsquo;t track it in real time yet. (It seems to me that this would really come down to a cost benefit analysis, until you have a key set of things that you want to know about this data you probably don&amp;rsquo;t need to look at real time updates.). Daniel mentions that they might be able to begin to look at the characteristic time scale to complete a collaboration within different disciplines.&lt;/p&gt;
&lt;p&gt;In terms of surprise there was the expectation in the US that collaboration would be more regional than they saw (my guess is that a lot of the national level collaboration is determined by centres of excellence for different research areas, a lot driven by Ivy League).&lt;/p&gt;
&lt;p&gt;Someone asks if these maps can be broken out by subject area. It seems that it&amp;rsquo;s probable that they can get this data, but the fields will be biased around the core fields that are using by Overleaf.&lt;/p&gt;
&lt;p&gt;This leads to an interesting question, how many users within a discipline do you need to get to get representative coverage for a field (when I was at Mendeley I recall we were excited to find that the number might be in the single digit percentages, but I can&amp;rsquo;t recall if that still holds any more, nor why it might.).&lt;/p&gt;
&lt;p&gt;Someone asks about the collaboration quality of individual authors. Daniel mentions that this is a tricky question, owing to user privacy. They were clear that they had to create a report the didn&amp;rsquo;t expose any personally identifiable information.&lt;/p&gt;
&lt;p&gt;### Comment&lt;/p&gt;
&lt;p&gt;I think that they are sitting on a really interesting source of information, and for any organisation to have information at this level, especially with the promise of real time updates, that&amp;rsquo;s quite exciting, however I&amp;rsquo;m not convinced that there is much extra information here than you would get by just looking at the collaboration graphs based on the published literature. This is what I&amp;rsquo;d love to see, can you evidence that the information you get from looking at real time authoring is substantively different than what you would get by mining the open literature? Doing this kind of real time analysis is probably only going to happen if Overleaf see a direct need to understand their user base in that way, and doing that is always going to need to be traded off against other development opportunities. Perhaps if they can find a way to cleanly anonymise some of this info, they could put it into the public domain and allow other researchers to have a shot at finding interesting trends?&lt;/p&gt;
&lt;p&gt;The other papers in the report also look interesting and I&amp;rsquo;m looking forward to reading through them. The network visualisations are stunning and I&amp;rsquo;m guessing that they used &lt;a href=&#34;https://gephi.org&#34;&gt;gephi&lt;/a&gt; to derive them.&lt;/p&gt;
&lt;p&gt;# Open Engagement and Quality Incentives in Peer Review, Janne Tuomas Seppänen, founder of &lt;a href=&#34;https://www.peerageofscience.org&#34;&gt;Peerage of Science&lt;/a&gt;. &lt;a href=&#34;https://twitter.com/janneseppanen?lang=en&#34;&gt;@JanneSeppanen&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Peerage of science provides a platform to allow researchers to get feedback on their manuscripts from others (reviewing) before submission, and allows them to get feedback on how useful their reviews are to others. A &lt;a href=&#34;https://www.peerageofscience.org/journals/&#34;&gt;number of journals&lt;/a&gt; participate to allow easy submission of a manuscript along with review for consideration for publication.&lt;/p&gt;
&lt;p&gt;Janne is emphasising that the quality of the peer review that is generated in his system is high. These reviews are also peer evaluated, on a section by section base.&lt;/p&gt;
&lt;p&gt;Reviewers need to provide feedback to each other. This is a new element to the system, and according to Janne the introduction of this new section in their system has not negatively affected the time to complete the review by any significant factor.&lt;/p&gt;
&lt;p&gt;75% of manuscripts submitted to their system end up eventually published. 32% are published directly in the journals that are part of the system. 27% are exported to non-participating journals.&lt;/p&gt;
&lt;p&gt;### Questions&lt;/p&gt;
&lt;p&gt;The reason why people take part in reviewing is that they can get a profile on how good their reviews are from their colleagues, building up their reviewing profile.&lt;/p&gt;
&lt;p&gt;Is there any evidence that the reviews actually improve the paper? The process always involves revisions on the paper, but there is no suggestion that there is direct evidence that this improves the paper.&lt;/p&gt;
&lt;p&gt;### Comment&lt;/p&gt;
&lt;p&gt;Really, anything that helps to improve the nature of peer review has to be welcomed. I remember when this service first launched, and I was skeptical back then, but they are still going, and that&amp;rsquo;s great. In the talk I didn&amp;rsquo;t catch how much volume they are processing. I&amp;rsquo;m keen to see many experiments like this one come to fruition.&lt;/p&gt;
&lt;h1 id=&#34;discover-whats-been-missing-vicky-hampshireyenowhttpyewnocom&#34;&gt;Discover what&amp;rsquo;s been missing, Vicky Hampshire, &lt;a href=&#34;http://yewno.com&#34;&gt;Yenow&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Yenow uses machine learning to extract concepts from a corpus, and then provides a nifty interface to show people the correlation between concepts. These correlations are presented as a concept graph, and the suggestion is that this is a nice way to explore a space. Specific snippets of content are returned to the searcher, so this can be used as a literature review tool.&lt;/p&gt;
&lt;p&gt;I had the pleasure of spending an hour last week at their headquarters in Redwood California having a look at the system in detail, and I&amp;rsquo;ll throw in some general thoughts at the bottom of this section. It was nice to see it all presented in a five minute pitch too. They do no human curating of the content.&lt;/p&gt;
&lt;p&gt;They incorporated in 2014, is now based in California, but the technology was created in Kings in London. As I understand it the core technology was originally used in the drug discovery realm and one of their early advisors &lt;a href=&#34;https://profiles.stanford.edu/michael-keller&#34;&gt;Mike Keller&lt;/a&gt; had a role in alerting them to the potential for this technology in the academic search space.&lt;/p&gt;
&lt;p&gt;The service is available through institutional subscription and it&amp;rsquo;s been deployed at a number of institutions such as Berkeley, Stanford and the state library of Bavaria (where you can try it out for &lt;a href=&#34;https://yewno-1com-1yewno.emedia1.bsb-muenchen.de&#34;&gt;yourself&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;To date they have indexed 100M items of text and they have extracted about 30M concepts.&lt;/p&gt;
&lt;p&gt;### Questions&lt;/p&gt;
&lt;p&gt;Are they looking at institutions and authors? These are things that are on their roadmap, but they have other languages higher up in their priorities. They system won&amp;rsquo;t do translation, but they are looking for cross-language concept identification. They are interested in using the technology to identify images and videos.&lt;/p&gt;
&lt;p&gt;They do capture search queries, and they have a real time dashboard for their customers to see what searchers are being made. They also make this available for publishing partners. This information is not yet available to researchers who are searching.&lt;/p&gt;
&lt;p&gt;They are also working on auto-tagging content with concepts, and there is a product in development for publishers to help them auto-categorise their corpus.&lt;/p&gt;
&lt;p&gt;They are asked what graph database they are using. They are using DynamoDB and elasticsearch, but Vicky mentioned that the underlying infrastructure is mostly off the shelf, and the key things are the algorithms that they are applying.&lt;/p&gt;
&lt;p&gt;At the moment there is no API, the interface is only available to subscribing institutions. The publisher system that they are developing is planned to have an API.&lt;/p&gt;
&lt;p&gt;### Comment&lt;/p&gt;
&lt;p&gt;There is a lot to unpack here. The scholarly kitchen recently had a nice overview of services that are assembling &lt;a href=&#34;https://scholarlykitchen.sspnet.org/2017/02/23/who-has-all-content/&#34;&gt;all of the scholarly content&lt;/a&gt;, and I think there is something here of great importance for the future of the industry, but what that is is not totally clear to me yet.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m aware of conversations that have been going on for some years now about wanting to see the proof of the value of open access through the development of great tools on top of open content, and as we get more and more open access content the collection of all of that content into one location for further analysis should become easier and easier, however yenow, along with other services like meta and google scholar, have been building out by working on access agreements with publishers. It&amp;rsquo;s clear that the creation of tools built on top of everything is not dependent on all of the content being open, it&amp;rsquo;s dependent on the service you are providing being not perceived as threatening to the business model of publishers.&lt;/p&gt;
&lt;p&gt;That puts limits on the nature of the services that we can construct from this strategy of content partnerships. It&amp;rsquo;s also the case that for every organisation that wants to try to create a service like this, they have to go through the process of setting up agreements individually, and this probably creates a barrier to innovation.&lt;/p&gt;
&lt;p&gt;Up until now many of the kinds of services that have been built in this way have been discovery or search services, and I think publishers are quite comfortable with that approach, but as we start to integrate machine learning, and increase the sophistication of what can be accomplished on top of the literature, will that have the potential to erode the perceived value of publisher as a destination? Will that be a driver to accelerate the unbundling of the services that publishers provide. In the current world I may use an intermediate search service to find the content that may interest me, and then engage with that content at the publisher site. In a near future world if I create a natural language interface into the concept map, perhaps I&amp;rsquo;ll just ask the search engine for my answer directly. Indeed I may ask the search engine to tell me what I ought to be asking for. Owing to the fact that I don&amp;rsquo;t have full overview of the literature I&amp;rsquo;m not in a position to know what to ask for myself, so I&amp;rsquo;ll rely on being told. In those scenarios we continue to disrupt the already tenuous relationship between reader and publisher.&lt;/p&gt;
&lt;p&gt;There are some other interesting things to think about too. How many different AI representations of the literature should be hope for? Would one be just too black boxed to be reliable? How may we determine reproducibility of search results? how can we ensure representation of correlations that are not just defined by the implicit biases of the algorithm? should we give the reader algorithmic choice? Should there be algorithmic accountability? Will query results be dependent on the order in which the AI reads the literature? Many many many interesting questions.&lt;/p&gt;
&lt;p&gt;The move to do this without any human curation is a bold one. Other people in this space hold the opinion that this approach currently has natural limits, but it&amp;rsquo;s clear that the Yenow folk don&amp;rsquo;t see it that way. I don&amp;rsquo;t know how to test that, but maybe as searches on the platform become more focussed, that&amp;rsquo;s the moment where those differences could come to light.&lt;/p&gt;
&lt;p&gt;I do have some comments on the product itself. I spent a little time today using the demo site available from the state library of Bavaria. It strikes me that I would quite like to be able to choose my own relevance criteria so that I can have a more exploratory relationship with the results. I did find a few interesting connections through querying against some topics that I was recently interested in, but I had the itch to want to be able to peel back the algorithm to try to understand how the concepts were generated. It&amp;rsquo;s possible that this kind of search angst was something that I experience years ago with keyword search, but that years of practice have beaten the inquisitiveness out of me, but for now that is definitely something that I noticed while using this concept map, almost a desire to know what lies in the spaces between the connections.&lt;/p&gt;
&lt;p&gt;At the moment they are looking to sell a subscription into libraries. It&amp;rsquo;s almost certain that this won&amp;rsquo;t totally replace current search interfaces (that sentence might come back to haunt me!). The challenge they face in this space is that they are Yet Another Discovery Interface, and people using these tools probably don&amp;rsquo;t invest a huge amount of time learning their intricacies. On the other hand the subscription model can be monetized immediately, and you don&amp;rsquo;t have to compete with Google head to head.&lt;/p&gt;
&lt;p&gt;On a minor note looking at their interface there is an option to sign in, but It&amp;rsquo;s not clear to me why I should. I imagine that it might save my searches, that it might provide the opportunity for me to subscribe to some kind of updating service, but I just can&amp;rsquo;t tell from the sign up page.&lt;/p&gt;
&lt;h1 id=&#34;crossref-event-data---joe-wass---joewass&#34;&gt;CrossRef Event Data - Joe Wass - @JoeWass&lt;/h1&gt;
&lt;p&gt;By this stage in the evening the heat was rising in the room, and the jet lag was beginning to kick in, so my notes start to thin out a lot. Joe presented some updates on the CrossRef &lt;a href=&#34;https://www.crossref.org/services/event-data/&#34;&gt;event data service&lt;/a&gt;. It was great to see it live, and I&amp;rsquo;d love to see it being incorporated into things like altmetric. Perhaps they need a bounty for encouraging people to build some apps on top of this data store?&lt;/p&gt;
&lt;p&gt;At the moment they are generating about 10k events per day. They have about 0.5M events in total.&lt;/p&gt;
&lt;p&gt;They provide the data as CC0, and for every event in the data store they give a full audit trail&lt;/p&gt;
&lt;h1 id=&#34;musicians-and-scientists---eva-amson---easternblothttpstwittercomeasternblotlangen&#34;&gt;Musicians and Scientists - Eva Amson - &lt;a href=&#34;https://twitter.com/easternblot?lang=en&#34;&gt;@easternblot&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Eva gave a beautiful little talk about the relationship between scientists and musicians, and that there are a disproportionally high number of scientists who play instruments than in the general population. She has been collecting stories for a number of years now and the overlap between these two activities is striking. You can read more about the &lt;a href=&#34;http://easternblot.net/category/musicians-and-scientists/&#34;&gt;project on her site&lt;/a&gt; and you can catch Eva playing with &lt;a href=&#34;http://www.londoneuphonia.com&#34;&gt;http://www.londoneuphonia.com&lt;/a&gt; on Saturday at St Paul&amp;rsquo;s Church Knightsbridge.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Three posts about product development</title>
      <link>http://scholarly-comms-product-blog.com/2017/03/15/three_posts_product_dev/</link>
      <pubDate>Wed, 15 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2017/03/15/three_posts_product_dev/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://partiallyattended.com/images/lean_value_tree.jpg&#34; alt=&#34;lean value tree&#34;&gt;&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m catching up on some reading at the moment. Trying to make headway on some other work while jet lagged is proving a challenge. Anyway, here are a couple of nice posts about product development that popped up in my feed (hat tip to &lt;a href=&#34;http://www.mindtheproduct.com/product-management-newsletter/?utm_source=Mind+the+Product+Newsletter&amp;amp;utm_campaign=af178f4c34-mtp_newsletter_2017_02_27&amp;amp;utm_medium=email&amp;amp;utm_term=0_babd9cfe61-af178f4c34-62248757&#34;&gt;Mind the Product Weekly Newsletter&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;## What do people do in the spaces in between?&lt;/p&gt;
&lt;p&gt;When thinking about what people do with your product, also &lt;a href=&#34;https://ux.useronboard.com/product-people-mind-the-gap-da363018cc57#.pr0k54nma&#34;&gt;think about what they don’t do, and how to help them get to where they are going&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The takeaway from this post is that by mapping out these interstitial moments you can get to a better understanding of your users needs, and better map the requirements of what you need to build.&lt;/p&gt;
&lt;p&gt;## We have been getting MVP wrong all this time, the point is to validate, not to delight for it&amp;rsquo;s own sake.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://hackernoon.com/the-mvp-is-dead-long-live-the-rat-233d5d16ab02#.9mk1phln5&#34;&gt;Forget “MVP”, focus on testing your biggest assumptions&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The key point in this post is that when deciding what to ship, use each iteration as an opportunity to test your riskiest assumptions, and understand what you expect to learn with each release. If you don&amp;rsquo;t know what those assumptions are, or what you are going to learn, why are you shipping a feature? I imagine that this post is mostly directed towards products that are still exploring the market-fit space, however even established products live within spaces that are evolving so some of this thinking carries over too.&lt;/p&gt;
&lt;p&gt;It reminds me of the Popperian view that you can&amp;rsquo;t prove hypothesis, but you can reject them, so each experiment to be most valuable should be constructed to try to reject the most critical hypothesis.&lt;/p&gt;
&lt;p&gt;I think there is at least one counter argument to the main point in this post, but you know things are complex, so that&amp;rsquo;s OK. If you are in a space where you understand your users well, and you have considerable experience to hand, it is probably OK to just do what you know to be right in terms of benefitting the user.&lt;/p&gt;
&lt;h2 id=&#34;burn-the-roadmaps&#34;&gt;Burn the roadmaps!!&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/techstars/why-most-product-roadmaps-are-a-train-wreck-and-how-to-fix-this-12617e3adabc#.96btccsa2&#34;&gt;Throw out the product roadmap, usher in the validation roadmap!&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This post was very welcome reading for me as I have a terrible relationship with product roadmaps, I just think that in a fast moving environment you don&amp;rsquo;t know what you are going to be doing in 12 months, and god forbid if you are tied down already to what you are going to be doing in 18 months, then you are probably not exploring a new space. Of course when you get to scale, and when you get to work on projects at scale, those kinds of timelines do in fact make sense, but I still like the idea of flipping the roadmap into one that is constructed around confirming/testing our understanding of the world in contrast to constructing how we want to roll our our features.&lt;/p&gt;
&lt;h2 id=&#34;lean-value-tree-and-constant-experimentation&#34;&gt;Lean value tree, and constant experimentation&lt;/h2&gt;
&lt;p&gt;The image at the top of this post is a representation of a tool called the lean value tree (see slide 30 from &lt;a href=&#34;https://www.slideshare.net/OllieStevensonGoldsm/strategy-in-a-lean-enterprise&#34;&gt;this deck&lt;/a&gt;. We have been using it a bit in the last two months at my current role, and I&amp;rsquo;m finding a lot of value in it. One of the things that ties all three of the posts that I have linked here together is the idea of experimentation. Understand your missing assumptions, test rigorously, be led in decision making about what you can learn. Something like the lean value tree can sit above these imperatives and help you make decisions around which experiments to spin up, and how to balance opportunities. Having worked it pretty hard in the past few weeks I can see that it has a lot of &lt;em&gt;value&lt;/em&gt;, but it still does not beat open conversation in an open team.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PLOS are looking for a new CEO</title>
      <link>http://scholarly-comms-product-blog.com/2017/01/10/plos-ceo/</link>
      <pubDate>Tue, 10 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2017/01/10/plos-ceo/</guid>
      <description>&lt;p&gt;So I hear that PLOS are looking for a new CEO. They are making the process fairly open, so if you are interested you can read more  &lt;a href=&#34;http://www.nature.com/naturejobs/science/jobs/601523-chief-executive-officer&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I got to thinking about some of the challenges and opportunities facing PLOS over the weekend. Over the years I&amp;rsquo;ve gotten to know a lot of PLOS folk, and I think it&amp;rsquo;s an amazing organisation. It has proved the viability of open access, and their business model is being copied by a lot of other publishers. At the same time they have had a fairly high frequency of turn over of senior staff in the last couple of years. So what are the likely challenges that a new CEO will face, and what should they do about them? (Time for some armchair CEO&amp;rsquo;ing).&lt;/p&gt;
&lt;p&gt;The condensed view of PLOS&amp;rsquo;s mission that they want to to accelerate progress in science and medicine. At the heart of their mission is the belief that knowledge is a public good, and leading on from that, that the means for transmitting that knowledge should also be a public good (specifically research papers).&lt;/p&gt;
&lt;p&gt;It &lt;a href=&#34;https://en.wikipedia.org/wiki/PLOS&#34;&gt;was founded in 2001&lt;/a&gt; by three visionaries, and it was configured to be a transformational  organisation that could catalyse radical change in the way that knowledge is created and disseminated, initially in particular in contrast to the subscription model for distributing scholarly content.&lt;/p&gt;
&lt;p&gt;Since launching PLOS has found massive success with the introduction of PLOS one, currently the largest journal in the world. That rapid growth led to a period of significant scaling and adjustment for the organisation, where it had to keep running at full pace in order to stay just about on top of the flood of manuscripts that were coming its way. This also created a big revenue driver for the organisation that has led to PLOS one being the engine that drives the rest of the PLOS.&lt;/p&gt;
&lt;p&gt;So now we have the strategic crux facing any incoming CEO. The organisation has an obligation to be radical in it&amp;rsquo;s approach to further it&amp;rsquo;s mission, but at the same time the engine that drives the organisation operates as such scale that changes to the way it works introduce systemic risks to the whole organisation. You also have to factor in that the basic business model of PLOS one is non defensible, and market share is being eroded by new entrants, in particular Nature Communications, so it is likely that no changes also represents a risky strategy.&lt;/p&gt;
&lt;p&gt;So what to do?&lt;/p&gt;
&lt;p&gt;There are probably many routes to take, and there are certainly a large number of ongoing activities that PLOS is engaged in as part of the natural practice of any organisation. I think the following perspectives might have some bearing on where to go. As with any advice, it&amp;rsquo;s much easier to throw ideas across the wall when you don&amp;rsquo;t have any responsibility for them, but I&amp;rsquo;m going to do it anyway in the full awareness that much of what I say below might not actually be useful at all.&lt;/p&gt;
&lt;h1 id=&#34;changing-plos-does-not-change-scientists&#34;&gt;Changing PLOS does not change scientists&lt;/h1&gt;
&lt;p&gt;PLOS has shown that Open Access can succeed, and it&amp;rsquo;s existence has been critical to confirm the desire of researchers who want to research conducted as an open enterprise. That has allowed those researchers to advocate for something real, rather than something imagined. However, there remain a large number of researchers for whom the constraints of the rewards system they operate under outweigh any interest they may have in open science. I think it is important to recognise that no matter what changes PLOS introduces, those changes on their own will not be sufficient to change the behaviour of all (or even of a majority) of researchers. Being able to show plausible alternatives to the existing system is important, but it is also important to continue to work closely with other key actors in the ecosystem to try to advance systemic change. What that tells me is that the bets that PLOS ought to take on to create change do have to be weighed against their likelihood to affect all researchers, and the risks they introduce to the current business model of PLOS.&lt;/p&gt;
&lt;p&gt;On the other hand you do want to progressively make it possible for people to be more open in how they conduct science. We talked a lot at eLife about supporting good behaviours, and you could imagine using pricing or speed mechanisms as a way of driving that change (e.g. lower costs for publishing articles that have been placed on a preprint server, for instance). One does have to be careful with pricing in academic circles as usually costs to publication are rarely a factor in the decision of an academic around where to publish, but generally I&amp;rsquo;m in favour of providing potentially different routes through a product to different users, and making the routes that promote the behaviours I support be easier/cheaper. (Github do this brilliantly by make open code repositories free to host, and only making you pay if you want to keep your code private).&lt;/p&gt;
&lt;h1 id=&#34;how-do-you-balance-risk&#34;&gt;How do you balance risk?&lt;/h1&gt;
&lt;p&gt;One of the things that is consistent in innovation is that we mostly don&amp;rsquo;t know what is going to succeed. I expect that the success of PLOS one probably took PLOS by surprise. It was a small change to an existing process, but it had a dramatic effect on the organisation.&lt;/p&gt;
&lt;p&gt;It seems to me that what you want to do is to have a fair number of bets in play. If we accept that we mostly won&amp;rsquo;t know what is going to succeed in the first place, then the key thing is to have a sufficient number of bets in place that you get coverage over the landscape of possibilities, and you iterate and iterate and iterate on the ones that start working well, and you have the resolve to close down the ones that are either making no progress or are getting stuck in local minima.&lt;/p&gt;
&lt;h1 id=&#34;product-horizons&#34;&gt;Product Horizons&lt;/h1&gt;
&lt;p&gt;I like the idea of creating a portfolio of product ideas around the &lt;a href=&#34;https://www.google.co.uk/search?q=project+portfolio+three+horizons&amp;amp;client=safari&amp;amp;rls=en&amp;amp;source=lnms&amp;amp;tbm=isch&amp;amp;sa=X&amp;amp;ved=0ahUKEwiIx4Gq4LfRAhUFLsAKHXMBD_AQ_AUICCgB&amp;amp;biw=1365&amp;amp;bih=816&#34;&gt;three horizons&lt;/a&gt; principle. There are lots of ways of determining if your bets are paying off. One of the things that I think PLOS needs to do is to ensure that at least a certain minimum of it&amp;rsquo;s financial base is being directed towards this level of innovation.&lt;/p&gt;
&lt;p&gt;I don&amp;rsquo;t think that is a problem at all for the organisation in terms of creating tools like ALM and their new submissions and peer review system, but I&amp;rsquo;m not clear on whether they have being doing this strategically across all of the bases where they want to have an impact. That&amp;rsquo;s not an easy thing to do, balancing ongoing work, new ideas, being disciplined to move on, being disciplined enough to keep going with the realisation that real success sometimes takes you by surprise.&lt;/p&gt;
&lt;h1 id=&#34;plos-may-need-diversification&#34;&gt;PLOS may need diversification&lt;/h1&gt;
&lt;p&gt;As I referred to above, the business model of PLOS, as it&amp;rsquo;s currently configured, is not easily defensible. Many other publishers have created open access journals with publishing criteria based on solidity of the science rather than impact. The Nature branded version of this is now attracting a huge number of papers (one imagines driven by the brand overflow from the main Nature titles). This speaks to me that there is some value in looking at diversifying the revenue streams that PLOS generates. This could be around further services to authors, to funders or to other actors in the current scholarly ecosystem. Here are three ways to potentially look at the market.&lt;/p&gt;
&lt;p&gt;One; what will the future flow of research papers look like, how does one capture an increasing share of that? Will increased efficiencies of time to publication, and improved services around the manuscript be sufficient, how might the peer review system be modified to make authors happier.&lt;/p&gt;
&lt;p&gt;Two; ask how will funding flow to support data and code publishing, will there be funding for creating new systems for assessment? Can any services that benefit PLOS be extended to benefit others in the same way?&lt;/p&gt;
&lt;p&gt;Three; if you are creating platforms and systems that can be flexible and support the existing scale of PLOS, what might the marginal investment be to extend those platforms so that others could use them (societies, small groups of academics that want to self-publish, national bodies or organisations from emerging research markets).&lt;/p&gt;
&lt;p&gt;The key here is not to suggest that PLOS has to change for it&amp;rsquo;s own sake, but rather to be clear about exploring these kinds of options strategically. It might be that you can create streams of revenue that make innovation be self-supporting, it might be that you hit on a way to upend the APC model. These efforts could be seen as investment in case the existing driver of revenue continues to come under increasing pressure in the future.&lt;/p&gt;
&lt;p&gt;Ultimately you want to build a sustainable engine for innovation.&lt;/p&gt;
&lt;h1 id=&#34;who-does-all-of-the-work&#34;&gt;Who does all of the work?&lt;/h1&gt;
&lt;p&gt;In the end all of the work is done by real people, and the key thing any new CEO is going to have to do is to bring a clarity of purpose, and to support the staff who are in the thick of things. What I&amp;rsquo;ve seen cause the most dissatisfaction in staff (aside from micromanagement - a plague on the houses of all micro-mangers), is a lack of ability to ship. This usually comes down to one of two causes, either priorities chance too quickly, or unrealistic deadlines are set that lead to the introduction of technical debt, that causes delays in shipping. It&amp;rsquo;s key to try to identify bottlenecks in the organisation, and (as contradictory as it might sound) to try to create slack in people&amp;rsquo;s schedules to allow for true creative work to happen.&lt;/p&gt;
&lt;h1 id=&#34;if-everyone-is-going-open-access-why-should-plos-exist-has-it-now-succeeded-in-some-way&#34;&gt;If everyone is going open access why should PLOS exist, has it now succeeded in some way?&lt;/h1&gt;
&lt;p&gt;Given that almost all new journal launches are now open access journal launches, has PLOS effectively won? Could the existing PLOS as it exists essentially go away? I think within one area of how we get to an open research ecosystem that might actually be true, however that only speaks to access to the published literature. Open science requires so much more than that. It needs transparency around review, efficiency in getting results into the hands of those who need them, data and code that are actionable and reusable, a funding system that abandons it&amp;rsquo;s search for the chimera of impact, an authoring system that is immediately interoperable with how we read on the web today.&lt;/p&gt;
&lt;p&gt;So what to do with PLOS as it&amp;rsquo;s currently configured? I see the current PLOS, with it&amp;rsquo;s success, as being an opportunity to generate the revenues to continue to explore and innovate in these other areas, but I think that the current system should be protected to ensure that this is possible.&lt;/p&gt;
&lt;h1 id=&#34;in-the-end-of-the-day-what-does-a-ceo-do&#34;&gt;In the end of the day, what does a CEO do?&lt;/h1&gt;
&lt;p&gt;I can&amp;rsquo;t remember where I read it now, but one post from a few years back struck me as quite insightful. It said that a CEO has three jobs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;make sure the lights stay on&lt;/li&gt;
&lt;li&gt;set the vision for the organisation&lt;/li&gt;
&lt;li&gt;ensure that the best people are being hired, and supported&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PLOS is in a great position at the moment. It has a business model that is working right now, and is operating at a scale that gives any incoming CEO a good bit of room to work with. It&amp;rsquo;s a truly vision led organisation, whose ultimate goal is one that can benefit all of society. It has great great people working for it.&lt;/p&gt;
&lt;p&gt;I don&amp;rsquo;t think that the job is in anyway going to be a gimme, but it&amp;rsquo;s got to be one of the most interesting challenges out there in the publishing / open science landscape at the moment.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reverse DOI lookups with Crossref</title>
      <link>http://scholarly-comms-product-blog.com/2017/01/10/crossref-reverse-lookups/</link>
      <pubDate>Tue, 10 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2017/01/10/crossref-reverse-lookups/</guid>
      <description>&lt;p&gt;Today I had a need to think about how to do a reverse lookup of a formatted citation to find a DOI.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/crossreforg&#34;&gt;@CrossrefOrg&lt;/a&gt; helped out and pointed me to the &lt;code&gt;reverse&lt;/code&gt; api endpoint. It workes like this:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;http://api.crossref.org/reverse&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Created a json payload file “citation.json” formatted as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;[
  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;	Curtis, J. R., Wenrich, M. D., Carline, J. D., Shannon, S. E., Ambrozy, D. M., &amp;amp; Ramsey, P. G. (2001). Understanding physicians’ skills at providing end-of-life care: Perspectives of patients, families, and health care workers. Journal of General Internal Medicine, 16, 41-49.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  &amp;#34;&lt;/span&gt;
]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Call the API using CURL (you need to set the Content-Type header to application/json)&lt;/p&gt;
&lt;blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;$ curl -vX POST &lt;a href=&#34;http://api.crossref.org/reverse&#34;&gt;http://api.crossref.org/reverse&lt;/a&gt; -d @citation.json &amp;ndash;header &amp;ldquo;Content-Type: application/json&amp;rdquo;&lt;/p&gt;
&lt;p&gt;I then got the following response:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;{&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;status&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ok&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;message-type&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;work&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;message-version&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1.0.0&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;message&amp;#34;&lt;/span&gt;:{&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;indexed&amp;#34;&lt;/span&gt;:{&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;date-parts&amp;#34;&lt;/span&gt;:[[&lt;span style=&#34;color:#ae81ff&#34;&gt;2016&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;25&lt;/span&gt;]],&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;date-time&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2016-10-25T11:17:12Z&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;timestamp&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;1477394232160&lt;/span&gt;},&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;reference-count&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;21&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;publisher&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Springer Nature&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;issue&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;content-domain&amp;#34;&lt;/span&gt;:{&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;domain&amp;#34;&lt;/span&gt;:[],&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;crossmark-restriction&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;},&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;short-container-title&amp;#34;&lt;/span&gt;:[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;J Gen Intern Med&amp;#34;&lt;/span&gt;],&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;cited-count&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;published-print&amp;#34;&lt;/span&gt;:{&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;date-parts&amp;#34;&lt;/span&gt;:[[&lt;span style=&#34;color:#ae81ff&#34;&gt;2001&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]]},&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;DOI&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;10.1111\/j.1525-1497.2001.00333.x&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;type&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;journal-article&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;created&amp;#34;&lt;/span&gt;:{&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;date-parts&amp;#34;&lt;/span&gt;:[[&lt;span style=&#34;color:#ae81ff&#34;&gt;2004&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;]],&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;date-time&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2004-06-09T16:44:02Z&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;timestamp&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;1086799442000&lt;/span&gt;},&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;page&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;41-49&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;source&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;CrossRef&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;title&amp;#34;&lt;/span&gt;:[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Understanding Physicians&amp;#39; Skills at Providing End-of-Life Care. Perspectives of Patients, Families, and Health Care Workers&amp;#34;&lt;/span&gt;],&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;prefix&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;http:\/\/id.crossref.org\/prefix\/10.1007&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;volume&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;16&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;author&amp;#34;&lt;/span&gt;:[{&lt;span style=&#34;color:#f92672&#34;&gt;&amp;#34;given&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;J. Randall”,&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;family&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;Curtis&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;affiliation&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:[]},{&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;given&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;Marjorie&lt;/span&gt; &lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;D.&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;family&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;Wenrich&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;affiliation&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:[]},{&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;given&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;Jan&lt;/span&gt; &lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;D.&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;family&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;Carline&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;affiliation&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:[]},{&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;given&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;Sarah&lt;/span&gt; &lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;E.&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;family&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;Shannon&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;affiliation&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:[]},{&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;given&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;Donna&lt;/span&gt; &lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;M.&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;family&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;Ambrozy&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;affiliation&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:[]},{&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;given&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;Paul&lt;/span&gt; &lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;G.&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;family&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;Ramsey&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;affiliation&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:[]}],&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;member&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;http&lt;/span&gt;:&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;\/\/id.crossref.org\/member\/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;297&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;container-title&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:[&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;Journal&lt;/span&gt; &lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;of&lt;/span&gt; &lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;General&lt;/span&gt; &lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;Internal&lt;/span&gt; &lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;Medicine&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;],&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;original-title&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:[],&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;deposited&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:{&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;date-parts&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:[[2011,8,10]],&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;date-time&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2011-08-10&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;T&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;39&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;02&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;Z&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;timestamp&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:1312990742000},&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;score&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:120.61636,&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;subtitle&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:[],&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;short-title&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:[],&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;issued&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:{&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;date-parts&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:[[2001,1]]},&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;alternative-id&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:[&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10.1111&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;\/j.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1525&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;-1497.2001&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;00333&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;.x&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;],&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;URL&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;http&lt;/span&gt;:&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;\/\/dx.doi.org\/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;10.1111&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;\/j.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1525&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;-1497.2001&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;00333&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;.x&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;ISSN&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:[&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0884-8734&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1525-1497&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;],&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;citing-count&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:21,&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;subject&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;:[&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;Internal&lt;/span&gt; &lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;Medicine&amp;#34;]&lt;/span&gt;}&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;From this we can see that crossref suggests the following DOI lookup with a score of “120”
http://dx.doi.org/10.1111/j.1525-1497.2001.00333.x&lt;/p&gt;
&lt;p&gt;There is some backslash escaping going on here, so the actual lookup url is:
&lt;a href=&#34;&#34;&gt;http://dx.doi.org/10.1111/j.1525-1497.2001.00333.x&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This directs us the the following &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.1525-1497.2001.00333.x/abstract;jsessionid=12FD9F17A25B963F3E7D03C42AB137A7.f03t03?systemMessage=Wiley+Online+Library+Journal+subscribe+and+renew+pages+for+some+journals+will+be+unavailable+on+Wednesday+11th+January+2017+from+06%3A00-12%3A00+GMT+%2F+01%3A00-07%3A00+EST+%2F+14%3A00-20%3A00+SGT+for+essential+maintenance.+Apologies+for+the+inconvenience&#34;&gt;article&lt;/a&gt;, which does seem to be the one that we are interested in.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What we mean when we talk about preprints</title>
      <link>http://scholarly-comms-product-blog.com/2016/12/13/when-we-talk-about-preprints/</link>
      <pubDate>Tue, 13 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2016/12/13/when-we-talk-about-preprints/</guid>
      <description>&lt;p&gt;Cameron Neylon, Damian Pattinson, Geoffrey Bilder, and Jennifer Lin have just posted a cracker of a preprint onto biorxiv.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://biorxiv.org/content/early/2016/12/09/092817&#34;&gt;On the origin of nonequivalent states: how we can talk about preprints&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Increasingly, preprints are at the center of conversations across the research ecosystem. But disagreements remain about the role they play. Do they &amp;ldquo;count&amp;rdquo; for research assessment? Is it ok to post preprints in more than one place? In this paper, we argue that these discussions often conflate two separate issues, the history of the manuscript and the status granted it by different communities. In this paper, we propose a new model that distinguishes the characteristics of the object, its &amp;ldquo;state&amp;rdquo;, from the subjective &amp;ldquo;standing&amp;rdquo; granted to it by different communities. This provides a way to discuss the difference in practices between communities, which will deliver more productive conversations and facilitate negotiation on how to collectively improve the process of scholarly communications not only for preprints but other forms of scholarly contributions.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The opening paragraphs are a treat to read, and provide a simple illustration of a complex issue. They offer a model of &lt;em&gt;state&lt;/em&gt; and  &lt;em&gt;standing&lt;/em&gt;, that provides a clean way of talking about what we mean when we talk about preprints.&lt;/p&gt;
&lt;p&gt;There are a couple of illustrations in the paper of how this model applies to different fields, in particular, physics, biology, and economics.&lt;/p&gt;
&lt;p&gt;I think it would be wonderful to extend this work to look at transitions in the state/standing model within disciplines over time. I suspect that we are in the middle of a transition in biology at the moment.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SSRN and Elsevier</title>
      <link>http://scholarly-comms-product-blog.com/2016/06/19/ssrn-elsevier/</link>
      <pubDate>Sun, 19 Jun 2016 22:53:47 +0100</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2016/06/19/ssrn-elsevier/</guid>
      <description>&lt;p&gt;So Elsevier has bought SSRN, a private company that fills a role for many academics that is an exemplar of the power and utility of a true commons for scholarship.&lt;/p&gt;
&lt;p&gt;There is much wringing of hands, gnashing of teeth, and the obligatory call for the community to pony up and create a true open piece of infrastructure.&lt;/p&gt;
&lt;p&gt;Well, it turns out building things is pretty hard, and building things that people will actually use is even harder. In the long run if the infrastructure of scholarly communication is going to be as open as we would like it to be, it’s going to require serious buy-in from funders, institutions and researchers. Its telling that one of the most fundamental pieces of scholarly infrastructure — CrossRef, is essentially funded by publishers, and another Google Scholar is owned and run by a small group within a very large company.&lt;/p&gt;
&lt;p&gt;We may wake up one morning in magic pony land where all of the infrastructure we want is sufficiently funded, and researchers are self-enlightened enough about the odd incentive schemes at play within the race for funding, that they take the collective action to all abandon journals and self publish and review work in an open transparent and reproducible manner, but until that day things like the Elsevier acquisition of SSRN are going to keep happening (I wouldn’t be surprised if ResearchGate is next up to start acquiring things soon, a ResearchGate acquisition of Academeia.edu anyone?).
So what is the optimistic outcome, and what is the pessimistic outcome for this acquisition?&lt;/p&gt;
&lt;p&gt;To understand that I I think it’s helpful to understand the landscape that Elsevier is operating in. They are a publicly listed, massively profitable company, but they are working in an industry where all of the writing on the wall is telling them that funders want to move to fully OA models, and want pricing for OA to come down. Libraries have been bled dry and there is no more profit expansion to be had in that direction. Generally markets like to see the capacity for growth, and the capacity for growth in Elsevier’s traditional revenue streams looks small. That’s generally not good for a publicly listed company.&lt;/p&gt;
&lt;p&gt;I think internally Elsevier has two options, one is to double down on its existing model — one which has tended to extract far more value from the academic landscape than it has created. The other option is to move to services, and seek growth in services. I like to think that perhaps there is a great debate inside the company with some people on the side of the traditional model, and some good souls trying to move to the company to a services model, but that’s a naive reading, and the truth is almost certainly much more complex. All of the people that I have met who work in Elsevier are good people who seem to really have the interests of the researchers at heart, but historically Elsevier made it’s great profits through monopsonomic business practices.&lt;/p&gt;
&lt;p&gt;So, the pessimistic view on what is going to happen is that Elsevier discovers that it can’t sufficiently replace it’s current revenue streams with services, and as it hits market push-back it shutters things like SSRN (perhaps folds it into Mendeley, but then leaves it’s product side under invested in, while eventually turning Mendeley into something much less fully featured than it is now). It uses the captured market that is has on the SSRN side to put tighter copyright controls in place, and researchers are forced by practice, habit and lack of options to demand that their institutions buy whatever product Elsevier puts into play. I’ll be honest I don’t really see this as a likely outcome.&lt;/p&gt;
&lt;p&gt;The Optimistic outcome is that Elsevier get’s to a place where it is happy to abandon it’s current profit margins, is willing to massively downsize, and somehow finds a way to make the service offering that it puts in play be actually able to pay for the massive burn rate that Mendeley, and other infrastructure plays, must be costing it at the moment. In this world Elsevier is no longer a journal publisher, and all content coming from Elsevier is openly available, and we all get a nice happy feeling about their brand, and everything they do actually accelerates research, rather than holding it back. Somehow I don’t see this as a likely outcome either.&lt;/p&gt;
&lt;p&gt;Probably they are going to try to integrate SSRN usage and users into whatever infrastructure they have for user data across Elsevier. I expect that SSRN will get some product updates. I expect that the actual burn rate of operating SSRN is so small that not much is going to change on the SSRN side for the next few years, but what I would expect to see happening is Elsevier wanting to build analytics and profiles around all of the interests and research for all researchers who currently interact with SSRN. The services they may be hoping to get out of this may be services that can be sold to advertisers, to product companies, in effect they may be looking to make a pivot from the content being what they sell, to the researchers being what they sell. That would be very interesting, and a bit worrying.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A(peeling) Peer Review, a proposal.</title>
      <link>http://scholarly-comms-product-blog.com/2015/05/17/peer-review-idea/</link>
      <pubDate>Sun, 17 May 2015 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2015/05/17/peer-review-idea/</guid>
      <description>&lt;p&gt;eLife&amp;rsquo;s &lt;a href=&#34;https://vimeo.com/49775707&#34;&gt;peer review&lt;/a&gt; process is really good. One of the key attributes of this is that reviewers are not blind to one another, and they have to consult with one another. This largely removes the &lt;a href=&#34;http://scholarlykitchen.sspnet.org/2012/07/31/the-referee-that-wasnt-there-the-ghostly-tale-of-reviewer-3-3/&#34;&gt;third reviewer problem&lt;/a&gt;. We also publish the decision letters and the author responses to the decision letter.&lt;/p&gt;
&lt;p&gt;Reviewers have the option of revealing themselves to authors. As with most review systms our reviewers know who the authors are. We are not at the point where our review process is fully open, this is the kind of thing that is community driven. My own hope is that we can move towards fully open review in time.&lt;/p&gt;
&lt;p&gt;Even in fully open review, where there is no blinding between authors and reviewrs, I think there is a case to be made for making the reviewers blind to whom the authors are. They will find out eventually, when the paper is published.&lt;/p&gt;
&lt;p&gt;You can argue that this is pointless because in a small field everyone knows who everyone else is anyway, indeed the evidence on small scale studies is mixed with some &lt;a href=&#34;http://dx.doi.org/10.1001/jama.1990.03440100079012&#34;&gt;evidence in favor&lt;/a&gt;, and some &lt;a href=&#34;http://dx.doi.org/10.1001/jama.280.3.240&#34;&gt;evidience against&lt;/a&gt; the thesis that this masking will help improve the quality of review.&lt;/p&gt;
&lt;p&gt;With the growth of reserach in the BRICS nations there are increasing numbers of papers coming in from labs that might not be that well known, and that might suffer from this potential bias. Researchers from these nations certainly fear this kind of bias and when you construct teh study in a ceratin way there is some evidence to support this feeling.&lt;/p&gt;
&lt;p&gt;There have been a few case studies which I am unable to dig up at the moment of writing this post, but gist of what they did was that they took a selection of already published papers, and resubmitted them with author names and institute names replaced by those that would apprear to be from less prestidgious labs and countries. Most of the papers thus re-submitted, were rejected.&lt;/p&gt;
&lt;p&gt;There seems to be no evidicne that I&amp;rsquo;m aware of that suggetss that this blinding decreases the quality of review, or increases biases in review.&lt;/p&gt;
&lt;p&gt;So if we do introduce blinding the authors from the reviews at the review stage, it&amp;rsquo;s not likely to hurt, it is likeley to increase the feeling of confidence in the system from BRICS researchers.&lt;/p&gt;
&lt;p&gt;What would be great though. is if after this bliding, and after the publicaion, if at that point we could reveal all of the idntities of those involved, with everyone knowing that this was going to happen up front. We could, so to say, peer away the anonymity of the review process, layer by layer. We might have an appealing model of peer review, and one in which the incidence of appeals was reduced and the eventualy transparency could lead to better decisions.&lt;/p&gt;
&lt;p&gt;So that&amp;rsquo;s my proposal for a review system, one in which we peel back our layers of shielding at the end.&lt;/p&gt;
&lt;p&gt;It may well be that this is already happening, I don&amp;rsquo;t know journals that are doing it exactly like this off of the top of my head, do let me know!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FuturePub Jan 2015 - Lens Next</title>
      <link>http://scholarly-comms-product-blog.com/2015/01/28/futurepub4-lens-next/</link>
      <pubDate>Wed, 28 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2015/01/28/futurepub4-lens-next/</guid>
      <description>&lt;p&gt;On 2015-01-27 I gave one of the short talks at the FuturePub event. My slidedeck is &lt;a href=&#34;https://speakerdeck.com/ianmulvany/futurepub-jan-2015-lens&#34;&gt;here&lt;/a&gt;. I wanted to give a quick update on where the Lens viewer for research articles is heading. Lens is a great platform for experimentation, and we have been iterating on some ideas towards the end of 2014 that have now made it into the 2.0 release.&lt;/p&gt;
&lt;p&gt;The main update is that Lens can now be configured to accept information from a 3rd party source and display that information in the right hand resources pane. Lens converts an XML version of a research article into JSON, and then renders the JSON in it&amp;rsquo;s distinct two column layout. Up until now Lens was only able to get information from the article XML source file. We have added the ability for lens to check and API, and if there is a result returned, then Lens can be configured to show that information. As part of this you can define a new right-hand column for the specific source of information that you are interested in showing next to the article.&lt;/p&gt;
&lt;p&gt;Here you see a screenshot of an image with a related articles feed, and you can check out the &lt;a href=&#34;http://elife-static-web-host-test.s3-website-eu-west-1.amazonaws.com/future-pub-talk-jan-2015/lens-next-linked-article-examples/doc.html?url=https://s3.amazonaws.com/elife-cdn/elife-articles/03251/elife03251.xml&#34;&gt;example&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://farm8.staticflickr.com/7403/16195259380_41af48db01_z_d.jpg&#34; alt=&#34;related articles pane example&#34;&gt;&lt;/p&gt;
&lt;p&gt;Here you can see an example of version of Lens that taps into the altmetric.com API, and you can &lt;a href=&#34;http://bit.ly/lens-alm-dist&#34;&gt;play with the example&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://farm8.staticflickr.com/7306/15762642783_f56e21424e_z_d.jpg&#34; alt=&#34;altmetrics pane example&#34;&gt;&lt;/p&gt;
&lt;p&gt;You can get started with this new version of Lens right now, and there is a starter repository with an example customisation ready to play with.&lt;/p&gt;
&lt;p&gt;In addition to this major change, there has been a big improvement to the way Lens handles mathematics (thanks to contributions from the American Mathematical Society), and there have been a number of other smaller improvements too.&lt;/p&gt;
&lt;h1 id=&#34;other-speakers-in-the-evening&#34;&gt;Other speakers in the evening&lt;/h1&gt;
&lt;h2 id=&#34;christopher-rabotin-from-spahhro&#34;&gt;Christopher Rabotin from Spahhro&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;- they currently have 1.7 M pieces of content in the platform at the moment 
- they are launching a publisher API, so that people can push content into their platform, and see usage of that data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I&amp;rsquo;m looking forward to seeing this, I&amp;rsquo;m very interested in seeing what usage eLife content gets out of this platform.&lt;/p&gt;
&lt;h2 id=&#34;kaveah---rivervalley-technologies&#34;&gt;Kaveah - Rivervalley Technologies&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;- Kaveah demonstrates his XML end to end workflow
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This tool has come along nicely since the last time I looked at it. This is definitely the future of the interaction between authors and production, but the big gap remains in getting the original manuscript into this form. There are some moves in that direction, people moving to tools like authorea and writelatex, there are a number of other typesetters offering this online proofing environment, it&amp;rsquo;s an area of fairly rapid iteration at the moment, and I wish Kaveh good luck with this toolchain.&lt;/p&gt;
&lt;h2 id=&#34;scientific-literacy-tool---keren-limor-waisberg&#34;&gt;Scientific literacy tool - Keren Limor-Waisberg&lt;/h2&gt;
&lt;p&gt;This is a chrome extension that helps readers understand the context of the article that they are reading. You can check it out &lt;a href=&#34;http://www.literacytool.com&#34;&gt;here&lt;/a&gt;. I&amp;rsquo;m going to take it for a spin over the next week.&lt;/p&gt;
&lt;h2 id=&#34;overleaf-offline---winston-li&#34;&gt;Overleaf offline - Winston Li&lt;/h2&gt;
&lt;p&gt;This was technically the most impressive demonstration of the evening. A group of students have worked with Overleaf to connect it to git. You can now git clone your paper and work on it offline, and send a git push to update your online version. There are a few limitations, but this is a huge step for the product, and these students did it in about 3 months. What can you do with it? As Winston reminded us&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;As Turing tells us, you can do anything that is computable, it&amp;rsquo;s the command line!, it&amp;rsquo;s git!&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>thoughts on the ERC data workshop</title>
      <link>http://scholarly-comms-product-blog.com/2014/09/24/erc-workshop-thought/</link>
      <pubDate>Wed, 24 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2014/09/24/erc-workshop-thought/</guid>
      <description>&lt;p&gt;On Thursday and Friday of last week I attended a European Research Council workshop on managing research data. It was well attended with about 130 participants brining views from across the academic disciplines. I&amp;rsquo;ve blogged my raw notes from &lt;a href=&#34;http://partiallyattended.com/2014/09/21/erc-workshop-day1&#34;&gt;day one&lt;/a&gt; and &lt;a href=&#34;http://partiallyattended.com/2014/09/21/erc-workshop-day2&#34;&gt;day two&lt;/a&gt;. In this post I reflect on the points I noticed that were raised over the two days. People have been talking about the increasing importance of research information for many years now, and a hope was raised in the opening comments that we might be able to provide solutions to the problems posed by the issues of research data, by the end of the workshop. I was skeptical about our chances of doing that. The risk at a meeting like this is that the same points and problems get regurgitated, problems are listed at too high a level, everyone calls on everyone else, or at least someone else, to step in and solve the problem. There were aspects of all of these issues, but there were also highly encouraging signs too, and signs of real progress in solving some of the perennial existential questions of research data. Over the course of the two days I made a note, when I noticed it, of when specific named issues, potential solutions, or novel points, were made.&lt;/p&gt;
&lt;p&gt;By the end of the first day the problems list far outweighed the solutions list, but by the end of the second day that ratio had reversed. I&amp;rsquo;m going to briefly drill into each one in a moment, but before doing that I&amp;rsquo;ll touch on the highlights coming out of the meeting.&lt;/p&gt;
&lt;p&gt;By the end of the meeting the chair put it well when he said that overall the feeling coming out of the meeting was one of unity, a shared desire and understanding that data should be open, and a shared understanding that some culture change is necessary. We have many parties interested in this issue, and we all want to move faster on the issue.&lt;/p&gt;
&lt;p&gt;There were signs of real progress too. LERU have a working paper on research data, and the take home message is that university chancellors almost universally think that research data should be made open, and that this will be a high priority issue for them - once they figure out what they are doing about open access.&lt;/p&gt;
&lt;p&gt;How to cite data is now solved, in principle. The FROCE 11 data citation principles solve this, what remains is implementation (already in progress in the life sciences), and then adoption. Adoption is going to be where the largest challenges lie, because if we have a mechanism for citing data, and researchers continue to turn up to meetings like this say &lt;em&gt;how do I cite data&lt;/em&gt;, then obviously there is work to do. We have to continue this work until researchers turn up to meetings like this one and say &lt;em&gt;this is how I cite data&lt;/em&gt;. We want data citations everywhere.&lt;/p&gt;
&lt;p&gt;A working solution to how researchers can make claims on what data they have produced was demonstrated by Sünje Dallmeier‐Tiessen with the The &lt;a href=&#34;http://odin-project.eu&#34;&gt;ODIN&lt;/a&gt; project. Again there is work needed here to promote adoption, and work to do on usability and interoperability.&lt;/p&gt;
&lt;p&gt;It wasn&amp;rsquo;t all light and harmonious music though, there were a few telling shadows, a few indicators that the problem remains a deep and challenging one. It was notable that no LERU university has any reward system or prize system in place for good use or reuse of research data, or any mechanism in place for rewarding excellence in the support systems for research data. There is a Dutch prize on this topic, but it&amp;rsquo;s clear that more can be done.&lt;/p&gt;
&lt;p&gt;In fact, often a need in culture change was mentioned. It should be obvious where this change can best be affected - in the grant rewarding process and in the hiring process. The EU, indeed all funders, are wary of sticks, but let&amp;rsquo;s sow the fields of Europe&amp;rsquo;s rich plots of data with an abundance of carrots. Let&amp;rsquo;s make available specific funding to support bottom up approaches to training for data management. There is already an appetite with initiatives like software carpentry, the creation of figshare, the growth of data dryad. Goodness, we could even invest in library infrastructure for this purpose. Let&amp;rsquo;s set up a research track for pure data re-use with grants awarded to those who have projects that reuse the data of others, and give them the time and resources to clean up that data. Let&amp;rsquo;s make clear that data are a real research output that counts in assessment. There would be no requirement to do this, but researchers who did would have their work recognised where it matters most. There were a few calls that anywhere between 5% or 15% of all research funding should go to data management, but I think it would be better to look at how we can alter behaviours on the ground from the bottom up. Data is important. After all, the data is the science, or at the very least it is the embodiment of our articulation of how we have grappled with reality, and it is the trail that shows our direct engagement with nature.&lt;/p&gt;
&lt;p&gt;Having real options for data management careers in research could also help in the short term, and in the medium term could help create a workforce that is skilled in the management of big data.&lt;/p&gt;
&lt;p&gt;OK, so let&amp;rsquo;s now look at each of the points that I captured from the two days at the meeting. I&amp;rsquo;ll list these as either questions, problems or solutions. I&amp;rsquo;m grouping them into topics that seem to make sense to me, so my groupings don&amp;rsquo;t reflect the order in which these topics arose, but I hope by doing this I can provide a horizontal view across the breakout sessions from the meeting to get the common themes that emerged. I&amp;rsquo;ll list the solutions as they were proposed. They stand here for your consideration. I add my own commentary at the bottom of each section.&lt;/p&gt;
&lt;h1 id=&#34;moolah-money-cash&#34;&gt;Moolah, money, cash,&lt;/h1&gt;
&lt;h3 id=&#34;problems-or-questions-raised&#34;&gt;Problems or questions raised&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;- Question: who pays, what do they pay for?
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;solutions-proposed&#34;&gt;Solutions proposed&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;- Solution: provide funding for data sharing.  
- Solution: take a percent, say 15%, and set that aside in every grant for data sharing, curation and storage . 
- Solution: do bulk purchasing from providers, and distribute compute and storage credits to researchers.  
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;my-comments&#34;&gt;My comments&lt;/h3&gt;
&lt;p&gt;The inference seemed to be that it should mostly be funders supplying the cash through some mechanism. The idea of doing bulk purchasing for infrastructure, and then giving researchers credits is an appealing one. Such approaches will be good for big data, but will have little impact on the majority of instances of data that is created, things like individual excel files on an individuals computer.&lt;/p&gt;
&lt;h2 id=&#34;infrastructure-and-support&#34;&gt;Infrastructure and support&lt;/h2&gt;
&lt;h3 id=&#34;problems-or-questions-raised-1&#34;&gt;Problems or questions raised&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;- Problem: hiring domain experts, e.g. DBAs, on a temporary basis is hard.  
- Problem: formats need to be updated, needs to work in the long-term, needs intelligent curation, structures to support this does not exist.   
- Problem: manual labour is required, the current credit system does not support that.   
- Question: how might we provide more training.   
- Problem: data is useless without the computational infrastructure behind the data.  
- Question: how might we provide better infrastructure for data.     
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;solutions-proposed-1&#34;&gt;Solutions proposed&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;Solution: have a bank of data domain experts ready in the library/institution that can be seconded out or hired on short term contracts by researchers  
Solution: create a profession of data curators  
Solution: The EU should take care of infrastructure EU-wide to promote a level playing field.  
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;my-comments-1&#34;&gt;My comments&lt;/h3&gt;
&lt;p&gt;All of these solutions are good in principle, however they will require real will to create these kinds of incentives. It was often mentioned throughout the meeting that these kinds of skills could be provided through the private sector, and there was real concern that such an approach might lead to restrictions on the data if that data becomes controlled by a private company. Academic publishers were mentioned. I find it hard to see an EU-wide rolling out of an army of data curators, I think that has to come bottom up, from within disciplines. I could see libraries making a case to equip themselves for this task, but I don&amp;rsquo;t see them as being the natural inheritors of that task. It seems that institution-wide facilities, or national facilities might be good places for these kinds of roles to reside.&lt;/p&gt;
&lt;h2 id=&#34;fundamental-definitions&#34;&gt;Fundamental definitions&lt;/h2&gt;
&lt;h3 id=&#34;problems-or-questions-raised-2&#34;&gt;Problems or questions raised&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;Problem: open data has been defined by rich labs, it&#39;s ambiguous, and currently non-inclusive.   
Question: how can we get to an agreed understanding of what Open Data is, and what currency it has in research communication.    
Question: how might we define data, per discipline.    
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;my-comments-2&#34;&gt;My comments&lt;/h3&gt;
&lt;p&gt;No solutions were proposed for this topic, but a great point was made that what we are calling data sharing is really effectively data dissemination, as those making their data available are not usually waiting for some reciprocal piece of data (although to be fair that was the example for the motivation behind sharing geneomic data).&lt;/p&gt;
&lt;h2 id=&#34;management-and-interoperability&#34;&gt;Management and interoperability&lt;/h2&gt;
&lt;h3 id=&#34;problems-or-questions-raised-3&#34;&gt;Problems or questions raised&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;Question: how do we combine heterogeneous data from within one discipline or study.   
Question: how do you deal with large unstructured data sets?    
Question: who sets the metadata structures in different communities? 
Question: where do we put our data?  
Problem: need coordination between different data repositories and related services.    
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;solutions-proposed-2&#34;&gt;Solutions proposed&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;Solution: deposit you data into existing structured DBs where they are available.  
Solution: convince people to copy good data management plans (and follow them).    
Solution: create an open marketplace of good data management plans.    
Solution: make data management plans be a living document.    
Solution: include the data scientist at the point of experimental design.    
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;my-comments-3&#34;&gt;My comments&lt;/h3&gt;
&lt;p&gt;So these topics, where do I put my data, how, how does it interoperate. To make anything happen we &lt;strong&gt;have&lt;/strong&gt; to look at things on a discipline by discipline level. Many disciplines have this nailed, and we must must must must must work as hard as we can to get appropriate data into it&amp;rsquo;s appropriate repository. If it goes anywhere else that piece of data might as well not exist. I had a detailed conversation about the feasibility of federating computation over these kinds of data sets across repositories, but at the moment there is no infrastructure or will to support an approach like that, however we don&amp;rsquo;t have to, because the primary home for that data exists.&lt;/p&gt;
&lt;p&gt;For small scale - one off files- data that is important on a paper by paper basis, there are now stable DOI coining repositories. If we can get people to use them then the question of where this data should go seems to have a clear answer.&lt;/p&gt;
&lt;p&gt;Two areas that represent significant challenges are domains that are just now stumbling into the era of unmanageably large data owing to tooling or data sources suddenly being able to produce more data than these domains had previously needed to work with. Domains that look at web scale data, and domains whose experimental equipment has vastly increased it&amp;rsquo;s output volume are examples, such as the digital humanities and microscopy. These research fields need to work hard on building up a shared infrastructure and data formats, and those efforts need to be supported.&lt;/p&gt;
&lt;p&gt;The other area that represents a challenge is data which is heterogeneous, but needs to be integrated in order to tell a story. Before this workshop I&amp;rsquo;d not appreciated that this kind of complexity can live even within one experiment. Perhaps a research objects like approach, or an approach of adopting tools and methods from schema-less data stores and key-value stores in web applications, could be applied to these problems, I just don&amp;rsquo;t know enough to have a solid opinion on this yet.&lt;/p&gt;
&lt;h2 id=&#34;incentives-trust-and-ethics&#34;&gt;Incentives, trust and ethics&lt;/h2&gt;
&lt;h3 id=&#34;problems-or-questions-raised-4&#34;&gt;Problems or questions raised&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;Question: how do you reward data dissemination? how do you provide incentives.  
Question: how do we deal with data fraud.  
Question: can you trust the data in a repository?
Question: how might we provide a proper citation and reward system.  
Question: how do we make data donation habitual?
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;solutions-proposed-3&#34;&gt;Solutions proposed&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;Solution: give DOIs, or similar, to data.  
Solution: cite data in reference lists, use the [FORCE 11 data citation principles](https://www.force11.org/datacitation).  
Solution: reward data contributions .   
Solution: appropriately label datasets to support fine-grained attribution.  
Solution: develop a culture of acknowledgement.  
Solution: use embargoes as a mechanism to incentivise researcher to make timely use of their own data.  
Solution: give a prize for examples of good use of data (it&#39;s mention that there is a data prize in The Netherlands).  
Solution: provide certification for digital repositories.  
Solution: Funders should mandate open data.  
Solution: enforce data policies.  
Solution: create a code of conduct teaching young researchers about the ethical issues around data.  
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;my-comments-4&#34;&gt;My comments&lt;/h3&gt;
&lt;p&gt;A lot of people talked about how we can cite the data. Yo, HELLO!!, we already have a solution to this, you just have to cite the data. A functioning example of how a researcher had connected a data output from one paper to their ORCID profile was even demonstrated during the meeting. For the vast majority of use cases, this is technically solved, we just need to let people know that it&amp;rsquo;s solved. Indeed the following blog post from CrossRef describes how to &lt;a href=&#34;http://crosstech.crossref.org/2014/09/linking-data-and-publications.html&#34;&gt;intertwine data and literature citations&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are, of course, some subtitles, what do you do with a data source or DB that has multiple contributors, what if your data source is evolving over time. There are potential solutions to these issues on the horizon, I&amp;rsquo;m excited to see where the &lt;a href=&#34;http://dat-data.com&#34;&gt;DAT&lt;/a&gt; project gets to. The aim of this project is to allow the creation of fine-grained identifiers (along with contributor info) for every record in a DB. It&amp;rsquo;s basically git for data.&lt;/p&gt;
&lt;p&gt;On the broader topic of giving credit for data outputs where those outputs can be identified, that is where the cultural change needs to come, and ideas such as setting up prizes or named chairs for data reuse are really good ones. In fact looking over the proposed solutions, most are indeed carrot-like rather than stick-like.&lt;/p&gt;
&lt;p&gt;On the topic of ethical responsibility and data fraud, I love the idea that by making your data available, that is a huge dis-incentive to fraud. Reproducing experiments is hard, so even if your data is made available, your experiment might not be that easy to reproduce, but fake data tends to have a statistically significantly different signature to real data, and so the act of making your data available is an act of ethical responsibility.&lt;/p&gt;
&lt;h2 id=&#34;legal-and-commercial-issues&#34;&gt;Legal and commercial issues&lt;/h2&gt;
&lt;h3 id=&#34;problems-or-questions-raised-5&#34;&gt;Problems or questions raised&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;Question: how does the distance to the commercial market affect acceptance of and practices in data sharing.  
Question: can we introduce licences that can be interoperable for data?  
Question: who owns the data/a bacterium?  
Question: legal and ethical issues affect the use of such data.  
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;solutions-proposed-4&#34;&gt;Solutions proposed&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;Solution: move towards an internationally level playing field on ethics for research.  
Solution: update the EU copyright directive.  
Solution: create an EU-wide directive on data policy for scientific research.  
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;my-comments-5&#34;&gt;My comments&lt;/h3&gt;
&lt;p&gt;It seems there are movements within the vast machine of EU legality to try to get to some state of normalisation on these issues, and I wish them the very best. One response that was clear from the floor of the conference around commerical involvement was the clear call to not give ownership of the data away to the private sector in the same way that ownership of the literature has been ceded to commerical publishers. In contrast to this the fact that data that is currently commercially held can be of high value to research was mentioned eloquently by the speaker who is building climate change models, and I think that position strengthens the arguments of the Open Data movement &amp;ndash; even commerical data providers should be encouraged EU-wide to move to thinking about getting open data certification for their data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ERC data management workshop, day 1</title>
      <link>http://scholarly-comms-product-blog.com/2014/09/21/erc-workshop-day1/</link>
      <pubDate>Sun, 21 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2014/09/21/erc-workshop-day1/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#initial-thoughts-about-the-workshop&#34;&gt;initial thoughts about the workshop.&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#opening-remarks&#34;&gt;Opening remarks.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#setting-the-scene&#34;&gt;Setting the scene.&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#sabrina-leonelli-the-epistemology-of-data-intesive-science&#34;&gt;Sabrina Leonelli - the epistemology of data-intesive science.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[Dr Hans Pfeiffenberger - Open Science &amp;ndash; opportunities, challenges &amp;hellip; &lt;a href=&#34;https://twitter.com/DataScienceFeed&#34;&gt;@datasciencefeed&lt;/a&gt;.](#dr-hans-pfeiffenberger-open-science-opportunities-challenges-datasciencefeedhttpstwittercomdatasciencefeed)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bernd-pulverer-finding-and-accessing-the-data-behind-figures&#34;&gt;Bernd Pulverer - finding and accessing the data behind figures.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dr-roar-sk%C3%A5lin-norwegian-researchers-want-to-share-but-are-afraid-of-jeopardising-their-career&#34;&gt;Dr Roar Skålin - Norwegian researchers want to share, but are afraid of jeopardising their career.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary-of-points-from-the-scene-setting&#34;&gt;Summary of points from the scene setting.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#afternoon-breakout-session-life-sciences&#34;&gt;Afternoon breakout session - Life Sciences.&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#short-comments&#34;&gt;Short comments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#iris-hovatta-group-leader-department-of-biosciences-university-of-helsinki&#34;&gt;Iris Hovatta, Group Leader, Department of Biosciences, University of Helsinki.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dr-bouke-de-jong-head-of-unit-mycobacteriology-at-the-institute-oftropical-medicine-antwerp&#34;&gt;Dr Bouke de Jong, Head of Unit Mycobacteriology at the Institute ofTropical Medicine, Antwerp.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sebastian-luyssaert-user-and-provider-perspectives-on-data-sharing-at-the-interface-between-life-and-earth-sciences&#34;&gt;Sebastian Luyssaert - user and provider perspectives on data sharing at the interface between life and earth sciences.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#discussion&#34;&gt;Discussion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary-of-issues-from-breakout-session&#34;&gt;Summary of issues from breakout session.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary-of-solutions-from-breakout-sessions&#34;&gt;Summary of solutions from breakout sessions.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary-of-issues-from-the-workshop&#34;&gt;Summary of issues from the workshop.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary-of-solutions-proposed-by-the-workshop&#34;&gt;Summary of solutions proposed by the workshop.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#hallway-conversations&#34;&gt;Hallway conversations.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bingo-card-terms&#34;&gt;Bingo card terms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;These are notes from the first day of the European Research Council Research Data Management &amp;amp; Sharing Workshop. I&amp;rsquo;ve also &lt;a href=&#34;http://partiallyattended.com/2014/09/21/erc-workshop-day2/&#34;&gt;posted notes from the second day&lt;/a&gt;, and I&amp;rsquo;ll shortly add another post examining problems and potential solutions raised over the course of the workshop. Jennifer Lin from PLOS has also posted &lt;a href=&#34;https://www.evernote.com/shard/s215/sh/76269856-3b62-4103-81ba-2068dca1b470/9ac27bc2968e5e9ea3e245eb44f9dff2&#34;&gt;some excellent notes&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;These notes are bit jagged, but I thought there was more value in getting them out in a rough form ahead of the RDS meeting that starts tomorrow, rather than waiting to get them into better shape, but missing that event. My apologies up front for errors, and incomplete sentences.&lt;/p&gt;
&lt;p&gt;# initial thoughts about the workshop.&lt;/p&gt;
&lt;p&gt;The opening document, that was distributed a few days before the workshop, highlights the great heterogeneity in how data is used, understood and licensed, across different disciplines. It&amp;rsquo;s a big old gordian knot. I advocate doing small simple things that move us, step by step, into a better future.&lt;/p&gt;
&lt;p&gt;I will be keeping an ear out for tools that are in use in real workflows, and I&amp;rsquo;ll be keeping an ear out for any comments that float up during the course of the meeting that resonate for one reason or another. In principle this meeting is about the EU listening to the research community, and other stakeholders, and hearing what it is that we want as an appropriate future for how data should be managed.&lt;/p&gt;
&lt;p&gt;(I&amp;rsquo;ll see if the docs can be posted somewhere, for the purposes of this blog.)&lt;/p&gt;
&lt;h2 id=&#34;opening-remarks&#34;&gt;Opening remarks.&lt;/h2&gt;
&lt;p&gt;In addition to the normal remakes on heterogeneity, Professor Nicholas Canny made the excellent point that within the EU business model viability is also a real issue within the EU. What works in one country does not always work in another.&lt;/p&gt;
&lt;p&gt;The nub of Prof. Canny&amp;rsquo;s remarks is that sustainability is the key issue, not only in terms of storage, but also in terms of verification and validation of the data at a later point in time.&lt;/p&gt;
&lt;p&gt;A use case of interest about digital preservation and sharing is the Boston archive of the IRA recollections. This was collected with the promise that no material would be released until later, but then &amp;hellip;. &lt;a href=&#34;http://bostoncollegesubpoena.wordpress.com&#34;&gt;they were subpoenaed&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are problems, and this conference is addressing those problems. The hope is that we can provide solutions to those problems.&lt;/p&gt;
&lt;p&gt;The next speaker also focusses on publications, as the main route towards data. It&amp;rsquo;s mentioned that some disciplines have been very self-organising, however some disciplines are even lacking recognition that there is currently an issue. It would be interesting to me to find out which disciplines are lagging the most.&lt;/p&gt;
&lt;p&gt;Sustainability also touches on the software, so it&amp;rsquo;s all about wares - hardware for storage, software for interoperability and wetware for expertise.&lt;/p&gt;
&lt;p&gt;We are informed that the agenda is both heavy and efficient. I suppose like an elephant. It&amp;rsquo;s mentioned again that there is a hope that this workshop will provide solutions. I am doubtful that we will manage that, however  if we can identify some small roadblocks, perhaps that might be sufficient, perhaps that will give us a few points against which we can apply some levers.&lt;/p&gt;
&lt;p&gt;## Setting the scene.&lt;/p&gt;
&lt;p&gt;(It might be good for me to try to capture specially interesting questions that emerge in these opening sessions, we can then review later and see if there are common themes.)&lt;/p&gt;
&lt;h3 id=&#34;sabrina-leonelli---the-epistemology-of-data-intesive-science&#34;&gt;Sabrina Leonelli - the epistemology of data-intesive science.&lt;/h3&gt;
&lt;p&gt;(who is very very awesomely speaking with her very young baby on her shoulder, which is just awesome).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Q: how do we make data donation habitual?

Point: manual labour is required, the current credit system does not support that.

Point: formats need to be updated, needs to work in the long-term, needs intelligent curation, structures to support this does not exist.

Q: how might we create structures and systems to support data curation, and intelligent curation.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(on the question of how to update data, &lt;a href=&#34;http://dat-data.com&#34;&gt;dat&lt;/a&gt; is a very nice potential solution, as are &lt;a href=&#34;http://dataprotocols.org/data-packages/&#34;&gt;data packages&lt;/a&gt;, but these are both in an embryonic state. The reason that Dat is appealing is that it matches the model of git, and we know that git is successful, we know that git supports more items of code, in a way that is sustainable, reusable, and shareable, at a scale that currently dwarfs that number of researchers that are curating their own data sets, so if we had a system that could do the same for data that was provably as robust as git, there is an inference that we could make that such a system might be fit for purpose for the scholarly world).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Point: open data has been defined by rich labs, it&#39;s ambiguous, and currently non-inclusive

Q: how can we get to an agreed understanding of what Open Data is, and what currency it has in research communication
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sabrina makes the very interesting point that people are not very clear in their understanding of what is being shared, when they share their data, what do they get back in return. Sharing should be a reciprocal activity and what we do with research data is not a reciprocal activity. She prefers the term dissemination.&lt;/p&gt;
&lt;h3 id=&#34;dr-hans-pfeiffenberger---open-science----opportunities-challenges--datasciencefeedhttpstwittercomdatasciencefeed&#34;&gt;Dr Hans Pfeiffenberger - Open Science &amp;ndash; opportunities, challenges &amp;hellip; &lt;a href=&#34;https://twitter.com/DataScienceFeed&#34;&gt;@datasciencefeed&lt;/a&gt;.&lt;/h3&gt;
&lt;p&gt;Everyone is afraid of data publishing, but who should be afraid? The people who make data up should be afraid of sharing their data. (the bottom line here is that researchers with shit practices should be afraid of data sharing, the inverse inference is that if you are afraid of sharing your data you might be considered to be a shit research, however that&amp;rsquo;s a stretch, and I want to be clear that Dr. Pfeiffenberger does not make this inference).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Q: how might we define data, per discipline

Q: how do you reward data dissemination? how do you provide incentives
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Mentions &lt;a href=&#34;http://www.argo.net&#34;&gt;Argo&lt;/a&gt; as a great example of an open data project. (I&amp;rsquo;d not seen argo before, it&amp;rsquo;s amazing). Dr. Pfeiffenberger gives examples of where making the data open had doubled the research output.&lt;br&gt;
(The royal society report on science as an open enterprise is mentioned again).&lt;/p&gt;
&lt;p&gt;NSF have started to ask for a list of five products from researchers, where the citable product can include a data set, and not only a research paper.&lt;/p&gt;
&lt;p&gt;DFG rules have been amended to say that you can be an author of a paper if you contributed to the creation of some data.&lt;/p&gt;
&lt;p&gt;A recommendation is made to not sign bad contracts. A recommendation is also made to fight against numerical assessment practices (this relates to &lt;a href=&#34;http://www.ascb.org/dora/&#34;&gt;DORA&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;bernd-pulverer---finding-and-accessing-the-data-behind-figures&#34;&gt;Bernd Pulverer - finding and accessing the data behind figures.&lt;/h3&gt;
&lt;p&gt;The data behind figures is critical, it&amp;rsquo;s currently hard to get to, &lt;a href=&#34;http://emboj.embopress.org&#34;&gt;EMBO&lt;/a&gt; is working on improving this situation.&lt;/p&gt;
&lt;p&gt;Bernd emphasis that not all data is useful. Raw and unstructured data age rapidly.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Q: how do we deal with scooping?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(here are some &lt;a href=&#34;http://www.slideshare.net/DataDryad/pulverer-embo-sourcedatanfdp13&#34;&gt;slides&lt;/a&gt; on the source data project). A key thing they are working on is tagging and identifying information in papers at the figure panel level to identify methods, entities and authors of individual components of a figure. This will allow horizontal navigation based on data rich and resource rich facets.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Q: how do we deal with data fraud
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Do we open the gates to heaven or to hell?&lt;/p&gt;
&lt;h3 id=&#34;dr-roar-skålin---norwegian-researchers-want-to-share-but-are-afraid-of-jeopardising-their-career&#34;&gt;Dr Roar Skålin - Norwegian researchers want to share, but are afraid of jeopardising their career.&lt;/h3&gt;
&lt;p&gt;They surveyed researchers, and got responses from 1474 researchers, a response rate of just over 30%. This was statistically representative. A large number of researchers actively decided to opt out from the survey.&lt;/p&gt;
&lt;p&gt;40 - 50% of researchers state that data is available, but on request.&lt;/p&gt;
&lt;p&gt;Researchers in this system broadly reflect the concerns that we have seen from other studies, concerns about scooping, about misinterpretation, and the time and effort required to&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Q: how might we provide a proper citation and reward system

Q: how might we provide more training

Q: how might we provide better infrastructure for data

Q: how is infrastructure organised, nationally, via publishers, via institutions, independent entities

Q: bottom up or top down?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Money is not the number one concern, more concern over infrastructure and training.&lt;/p&gt;
&lt;p&gt;Most researchers are in agreement that data should be provided on publication.&lt;/p&gt;
&lt;h3 id=&#34;summary-of-points-from-the-scene-setting&#34;&gt;Summary of points from the scene setting.&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;Q: how do we make data donation habitual?

Point: manual labour is required, the current credit system does not support that.

Point: formats need to be updated, needs to work in the long-term, needs intelligent curation, structures to support this does not exist.

Q: how might we create structures and systems to support data curation, and intelligent curation.

Point: open data has been defined by rich labs, it&#39;s ambiguous, and currently non-inclusive

Q: how can we get to an agreed understanding of what Open Data is, and what currency it has in research communication

Point: who pays, what do they pay for?

Q: how might we define data, per discipline

Q: how do you reward data dissemination? how do you provide incentives

Q: how might we define data, per discipline

Q: how do you reward data dissemination? how do you provide incentives

Q: how do we deal with data fraud

Q: how might we provide a proper citation and reward system

Q: how might we provide more training

Q: how might we provide better infrastructure for data
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;afternoon-breakout-session---life-sciences&#34;&gt;Afternoon breakout session - Life Sciences.&lt;/h2&gt;
&lt;h3 id=&#34;short-comments&#34;&gt;Short comments&lt;/h3&gt;
&lt;p&gt;These are going to be quite short, and then we will kick into discussions, so I&amp;rsquo;m going to aim to only outline these comments.&lt;/p&gt;
&lt;h3 id=&#34;iris-hovatta-group-leader-department-of-biosciences-university-of-helsinki&#34;&gt;Iris Hovatta, Group Leader, Department of Biosciences, University of Helsinki.&lt;/h3&gt;
&lt;p&gt;She studies anxiety, and uses mice to study gene regulatory networks.&lt;/p&gt;
&lt;p&gt;Most of their data is stored in excel files. Standardisation seems to be challenging. Mice in different labs seem to behave differently, even if genetically identical.&lt;/p&gt;
&lt;p&gt;They also product RNA-seq data. This kind of data is well supported, many journals require it&amp;rsquo;s availability.&lt;/p&gt;
&lt;p&gt;(On the excel data, I wonder whether they have experimented with any plugins, and how did they find them?).&lt;/p&gt;
&lt;p&gt;They want to integrate their behavioural and expression data together. There is a lack of expertise in their lab for database construction. Hiring DBAs on a monthly basis is hard.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Point: hiring domain experts, e.g. DBAs, on a temporary basis is hard

Solution: have a bank of data domain experts ready in the library/institution that can be seconded out or hired on short term contracts by researchers

Q: how do we obtain informed consent?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;### Dr Bouke de Jong, Head of Unit Mycobacteriology at the Institute ofTropical Medicine, Antwerp.&lt;/p&gt;
&lt;p&gt;Studying TB, and TB transmission. They look at infection by comparing the genotypes of the bacteria from different patients within a study area. The geneotypic clustering is a proxy for recency of infection.&lt;/p&gt;
&lt;p&gt;Challenges again reside around complex and large data sets, combining demographic and genomic data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Q: how do we combine heterogeneous data from within one discipline or study
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;They have non-automated data collection issues. Cleaning the data is labor intensive. They are creating dedicated DBs.&lt;/p&gt;
&lt;p&gt;If doing again they would like to enter data in the filed using bar-code methods to avoid double entry.&lt;/p&gt;
&lt;p&gt;They have clustering at multiple levels.&lt;/p&gt;
&lt;p&gt;(publishable unit is mentioned for the first time).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Q: where do we put our data?

Q: who owns the data/a bacterium?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In routine diagnostics patients have not given explicit consent. You might say that with anonymised data this might be OK, but the question of ownership of the bacteria remains an open question.&lt;/p&gt;
&lt;p&gt;How do you provide service to the community like this (of giving data away) count as a key performance indicator when it comes to evaluation?&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Q: what&#39;s the incentive?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Narrative seems to be an issue, if the researcher can construct that narrative within a paper and we can tightly link back to those narratives from the data, would that help?)&lt;/p&gt;
&lt;p&gt;Jo McEntyre asks a question about why not deposit some of this interesting MD with the core &amp;ldquo;genetic data&amp;rdquo;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We have a quick conversation around what we do with &amp;ldquo;unstructured&amp;rdquo; data at the point of publication. If we got that working, that might be a route to help with this issue, but it might lead to more noise. This needs to be worked out a bit more.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There is a question about anonymising the data&lt;/p&gt;
&lt;p&gt;### Sebastian Luyssaert - user and provider perspectives on data sharing at the interface between life and earth sciences.&lt;/p&gt;
&lt;p&gt;He looks at managing forestry, with a view of looking at how that can affect greenhouse gases and climate change. They have a model with about 500k lines of code. They run this model on a big computational infrastructure.&lt;/p&gt;
&lt;p&gt;One of the data sets that they use is forest inventory data from the EU. There are about 400k data points. This is economic data and is therefore hard to get access to. This data is held at the national level. They need to contact 30 bodies. It is very labor intensive to get this data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Problem: data is useless without the computational infrastructure behind the data

Solution: share data and operational algorithms (data cubes)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;He mentions &lt;a href=&#34;http://fluxnet.ornl.gov&#34;&gt;fluxnet&lt;/a&gt; as an example of a community effort. Data sharing is doing via a fair-use policy, where the data is made available on request, but not all data are shared, so you still need to contact the PI to get the data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Solution: make data sharing mandatory, like in [ICOS](http://www.icos-infrastructure.eu) &amp;amp; [NEON](http://www.neoninc.org/science/data)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;He mentions that the old-school method has the advantage the it forces conversations between researchers, and collaborations emerge.&lt;/p&gt;
&lt;p&gt;Where data is taken from an environment where there is a lot of competition, it is better for you to share your data, as if you don&amp;rsquo;t people will work with the others who have similar data that are sharing it. If you have data from a location that is hard to get this data, then it is better to not share your data, as people will contact you anyway, and you will get to be a co-author.&lt;/p&gt;
&lt;p&gt;ESA recently realised that no-one was using their data. The reason is that NASA data was free and ESA data was expensive. ESA data is now free.&lt;/p&gt;
&lt;p&gt;Data is so large that to do data sharing they buy disks and post it around.&lt;/p&gt;
&lt;p&gt;There is no way to talk to people at NASA or ESA about how the data was produced.&lt;/p&gt;
&lt;p&gt;After they improve their models they become more of a software provider than a data provider. At this point he is struggling a lot with conversations in the&lt;/p&gt;
&lt;p&gt;130 person months will end up in one paper, in doing a large extension to the underlying software model. There is no tradition in sharing software, however the only way people will be able to do experiments in the future is that they will be able to use the software that this group created. They want to get credit for this work.&lt;/p&gt;
&lt;p&gt;They are looking to distribute the model with a fair use policy. They have this software under version control, but how does this match up to IP? They are thinking of breaking the model up into components, and each component could come with a list of data and contributors to that component.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Q/Problem how do you give credit for software
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;He is worried that in the next year he is going to have to take up a lot of time in cleaning up his data and code, in order to be in a position of getting his next grant. He is more interested in doing science than spending time on cleaning his data sets.&lt;/p&gt;
&lt;h3 id=&#34;discussion&#34;&gt;Discussion&lt;/h3&gt;
&lt;p&gt;We get into a really good conversation about tools, competency and training, &lt;a href=&#34;http://software-carpentry.org&#34;&gt;Software Carpentry&lt;/a&gt; and &lt;a href=&#34;http://software-carpentry.org/v4/data/&#34;&gt;Data Carpentry&lt;/a&gt; are mentioned.&lt;/p&gt;
&lt;p&gt;We gather a list of potential issues with data in this domain. Someone mentions the issue of access to materials, or even accurate description of materials, such as reagents.&lt;/p&gt;
&lt;p&gt;Data management plans are mentioned as potentially problematic in terms of overhead. The worry is that a data management plan is something people write at the start, but then they ignore them after getting the money. Thinking about them as a step for submitting proposals could be a problem. Leaving them to the level of institutions can be problematic. Could one think of a system where the plan is discussed before and after submission, and the funder plays a role of coordinating data management at a super-institutional level. It&amp;rsquo;s mentioned that researchers are now being asked to review data management plans, but often don&amp;rsquo;t feel qualified to make these peer review decisions. It&amp;rsquo;s mentioned that in the UK the humanities council has a special committee for reviewing technically heavy applications.&lt;/p&gt;
&lt;p&gt;A question is raised about licensing of facts, it&amp;rsquo;s pointed out that you can&amp;rsquo;t licence facts.&lt;/p&gt;
&lt;p&gt;All of the UK funders have said that it is acceptable to ask for funding for data provisions, but you can&amp;rsquo;t ask  for a blank cheque, you have to justify the request in the grant. This automatically interacts with the institutional level, as you will eventually end up interacting with whatever resources are available at your institutions.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s mentioned that it&amp;rsquo;s a mistake to consider infrastructure as only hardware. It needs to expand it&amp;rsquo;s definition to include skills.&lt;/p&gt;
&lt;p&gt;The issue of rewards and incentives is mentioned. Bernd mentions that making data available can help with discoverability. Making data required at point of publication is mentioned as a mechanism (but to be honest the researchers do not seem convinced).&lt;/p&gt;
&lt;p&gt;We ask the researchers what incentives they need to see to become more open to the idea of sharing, we get a variety of answers&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- seeing that people who share are more successful
- knowing that a shared body of knowledge can provide more power in terms of making scientific advances (when I can see more data through the act of sharing my data)
- already has benefitted from sharing, got 80% good experience, 20% bad experience, but is mostly concerned that if he has to make all of his data available it will be too much of a burden, will take time away from doing science.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(transmitting data is mentioned again, I wonder about sending computation to the data, rather than the other way around).&lt;/p&gt;
&lt;p&gt;A comment is made that big data sets can be expensive to store, up to 30K for two years of storage. This can freeze out younger researchers. (Jo makes mentions again that we have places to put some data, but our systems do not cover all data types at this point in time).&lt;/p&gt;
&lt;p&gt;We devolve into writing a power point slide via committee.&lt;/p&gt;
&lt;p&gt;### Summary of issues from breakout session.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Point: hiring domain experts, e.g. DBAs, on a temporary basis is hard

Q: how do we obtain informed consent?

Q: how do we combine heterogeneous data from within one discipline or study

Q: where do we put our data?

Q: who owns the data/a bacterium?

Problem: data is useless without the computational infrastructure behind the data

Q/Problem how do you give credit for software

Problem: access and description of materials is often poor
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;summary-of-solutions-from-breakout-sessions&#34;&gt;Summary of solutions from breakout sessions.&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;Solution: have a bank of data domain experts ready in the library/institution that can be seconded out or hired on short term contrasts by researchers

Solution: share data and operational algorithms (data cubes)

Solution: make data sharing mandatory, like in [ICOS](http://www.icos-infrastructure.eu) &amp;amp; [NEON](http://www.neoninc.org/science/data)

Solution: docker or vagrant
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;summary-of-issues-from-the-workshop&#34;&gt;Summary of issues from the workshop.&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;Point: hiring domain experts, e.g. DBAs, on a temporary basis is hard

Q: how do we make data donation habitual?

Point: manual labour is required, the current credit system does not support that.

Point: formats need to be updated, needs to work in the long-term, needs intelligent curation, structures to support this does not exist.

Q: how might we create structures and systems to support data curation, and intelligent curation.

Point: open data has been defined by rich labs, it&#39;s ambiguous, and currently non-inclusive

Q: how can we get to an agreed understanding of what Open Data is, and what currency it has in research communication

Point: who pays, what do they pay for?
Q: how might we define data, per discipline

Q: how do you reward data dissemination? how do you provide incentives

Q: how might we define data, per discipline

Q: how do you reward data dissemination? how do you provide incentives

Q: how do we deal with data fraud

Q: how might we provide a proper citation and reward system

Q: how might we provide more training

Q: how might we provide better infrastructure for data

Q: how do we combine heterogeneous data from within one discipline or study

Q: where do we put our data?

Q: who owns the data/a bacterium?

Problem: data is useless without the computational infrastructure behind the data
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;summary-of-solutions-proposed-by-the-workshop&#34;&gt;Summary of solutions proposed by the workshop.&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;Solution: have a bank of data domain experts ready in the library/institution that can be seconded out or hired on short term contracts by researchers

Solution: share data and operational algorithms (data cubes)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;hallway-conversations&#34;&gt;Hallway conversations.&lt;/h2&gt;
&lt;p&gt;I chatted briefly with an engineer over coffee. She described some of the data that they deal with when looking at modelling the potential effects of building tunnels under a city, and that effect on the buildings on the ground.&lt;/p&gt;
&lt;h2 id=&#34;bingo-card-terms&#34;&gt;Bingo card terms&lt;/h2&gt;
&lt;p&gt;Lot&amp;rsquo;s of topics come up again and again, so I&amp;rsquo;ve quickly created a small set of &lt;a href=&#34;https://www.dropbox.com/s/uf3iv6h13j7s6qh/UOjgo0-print-bingo-com.pdf?dl=0&#34;&gt;data sharing bingo cards&lt;/a&gt;. I&amp;rsquo;ve used the following terms:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;who pays&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;relation to open access ?&lt;/li&gt;
&lt;li&gt;royal society report&lt;/li&gt;
&lt;li&gt;privacy concerns&lt;/li&gt;
&lt;li&gt;the humanities are different&lt;/li&gt;
&lt;li&gt;data standards&lt;/li&gt;
&lt;li&gt;embargos&lt;/li&gt;
&lt;li&gt;how do we cite data&lt;/li&gt;
&lt;li&gt;data quality&lt;/li&gt;
&lt;li&gt;big data&lt;/li&gt;
&lt;li&gt;unreproducable science&lt;/li&gt;
&lt;li&gt;legal restrictions&lt;/li&gt;
&lt;li&gt;licensing&lt;/li&gt;
&lt;li&gt;I&amp;rsquo;ll get scooped&lt;/li&gt;
&lt;li&gt;no time to share&lt;/li&gt;
&lt;li&gt;my data will be misunderstood&lt;/li&gt;
&lt;li&gt;there is no infrastructure&lt;/li&gt;
&lt;li&gt;my data is sensitive&lt;/li&gt;
&lt;li&gt;bottom up&lt;/li&gt;
&lt;li&gt;top down&lt;/li&gt;
&lt;li&gt;sustainability&lt;/li&gt;
&lt;li&gt;PLOS&lt;/li&gt;
&lt;li&gt;discoverability&lt;/li&gt;
&lt;li&gt;incentives&lt;/li&gt;
&lt;li&gt;data citation&lt;/li&gt;
&lt;li&gt;publishable unit&lt;/li&gt;
&lt;li&gt;supercomputer&lt;/li&gt;
&lt;li&gt;anonymise the data&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>ERC data management workshop, day 2</title>
      <link>http://scholarly-comms-product-blog.com/2014/09/21/erc-workshop-day2/</link>
      <pubDate>Sun, 21 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2014/09/21/erc-workshop-day2/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#life-sciences-breakout-key-points&#34;&gt;Life sciences breakout - key points.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#physical-sciences-breakout-key-points&#34;&gt;Physical sciences breakout - key points.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#humanities-breakout-key-points&#34;&gt;Humanities breakout - key points.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#open-discussion-on-morning-presentations&#34;&gt;Open discussion on morning presentations.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#breakout-session-on-incentives&#34;&gt;Breakout session on incentives.&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;[Paul Ayris - Implementing the Future: the &lt;a href=&#34;http://leru.org&#34;&gt;LERU&lt;/a&gt; roadmap for research data.](#paul-ayris-implementing-the-future-the-leruhttpleruorg-roadmap-for-research-data)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#s%C3%BCnje-dallmeiertiessen-incentives-for-open-science-attribution-recognition-collaboration&#34;&gt;Sünje Dallmeier‐Tiessen - Incentives for Open Science Attribution, Recognition, Collaboration.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#veerle-van-den-eynden-and-libby-bishop-incentives-for-sharing-research-data-evidence-from-an-eu-study&#34;&gt;Veerle Van den Eynden and Libby Bishop - Incentives for sharing research data, evidence from an EU study.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#open-discussion-after-breakout-session&#34;&gt;Open discussion after breakout session.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reporting-session-from-working-groups&#34;&gt;Reporting session from working groups.&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-management-and-sharing&#34;&gt;Data management and sharing.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#storage-curation-and-interoperability&#34;&gt;Storage, curation and interoperability.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-discoverability-access-and-reuse&#34;&gt;Data discoverability access and reuse.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rewards-and-incentives-for-good-data-management-the-carrot-session&#34;&gt;Rewards and incentives for good data management (the carrot session).&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#breakout-session-post-summing-discussion&#34;&gt;Breakout session - post summing - discussion.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#concluding-discussion-session&#34;&gt;Concluding discussion session.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#closing-remarks&#34;&gt;Closing remarks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#issues-and-questions-that-came-up-today&#34;&gt;Issues and questions that came up today.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#suggested-solutions-to-issues-that-came-up-today&#34;&gt;Suggested solutions to issues that came up today.&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Well, here we are at day two. My notes on the first day are &lt;a href=&#34;http://partiallyattended.com/2014/09/21/erc-workshop-day1/&#34;&gt;here&lt;/a&gt;.
We will open up with a short overview of the breakout sessions yesterday.&lt;/p&gt;
&lt;h3 id=&#34;life-sciences-breakout---key-points&#34;&gt;Life sciences breakout - key points.&lt;/h3&gt;
&lt;p&gt;The only point that came up that I hadn&amp;rsquo;t really covered in my notes from yesterday was that the view was that scientists should not become experts in data management, but some training should help.&lt;/p&gt;
&lt;h3 id=&#34;physical-sciences-breakout---key-points&#34;&gt;Physical sciences breakout - key points.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;open access to data shows the true richness of the data&lt;/li&gt;
&lt;li&gt;can validate the ownership of data&lt;/li&gt;
&lt;li&gt;can attract collaborators from other fields&lt;/li&gt;
&lt;li&gt;advantages of data sharing outweigh the disadvantages&lt;/li&gt;
&lt;li&gt;process of data sharing starts at the level of instrumentation and common data formats&lt;/li&gt;
&lt;li&gt;there should be the possibility of DOI-type labelling of data packages&lt;/li&gt;
&lt;li&gt;how do you deal with large unstructured data sets?&lt;/li&gt;
&lt;li&gt;legal and ethical issues affect the use of such data&lt;/li&gt;
&lt;li&gt;there is a difference between observational and experimental data sets&lt;/li&gt;
&lt;li&gt;how does the distance to the commercial market affect acceptance of and practices in data sharing&lt;/li&gt;
&lt;li&gt;who makes the first move? &amp;ndash; researchers, institutions, funders, societies?&lt;/li&gt;
&lt;li&gt;do we need a new profession of data curator?&lt;/li&gt;
&lt;li&gt;appropriately label datasets to support fine-grained attribution&lt;/li&gt;
&lt;li&gt;develop a culture of acknowledgement&lt;/li&gt;
&lt;li&gt;provide funding for data sharing&lt;/li&gt;
&lt;li&gt;Embargoes are complex, different embargoes are needed for different levels, PI&amp;rsquo;s need some time to work with the data, data collected at the national level should be made open immediately.
&lt;ul&gt;
&lt;li&gt;an embargo can act as an incentive for the timely use of the data for researchers (they need to get that paper out before their data is released)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;set aside 15% of each grant for data curation and storage&lt;/li&gt;
&lt;li&gt;that old chestnut &amp;ldquo;standardisation vs interoperability&amp;rdquo;&lt;/li&gt;
&lt;li&gt;update the EU copyright directive&lt;/li&gt;
&lt;li&gt;who sets the metadata structures in different communities?&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;humanities-breakout---key-points&#34;&gt;Humanities breakout - key points.&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;&#34;&gt;DigiPal&lt;/a&gt; was mentioned.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;management must be done at the discipline level, not at domain level&lt;/li&gt;
&lt;li&gt;needs to be done above the institutional level&lt;/li&gt;
&lt;li&gt;sustainability is crucial for SSH&lt;/li&gt;
&lt;li&gt;could SSH learn how to deal with Ethical issues from the life sciences?
&lt;ul&gt;
&lt;li&gt;need flexible sciences&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ownership of data is discipline depended, one rule does not fit all&lt;/li&gt;
&lt;li&gt;creation of infrastructures in not an ERC mandate (it makes one wonder why we might be here today)&lt;/li&gt;
&lt;li&gt;need career recognition&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;### Open discussion on morning presentations.&lt;/p&gt;
&lt;p&gt;Data management starts before the first data point is acquired.&lt;/p&gt;
&lt;p&gt;Data and publications need to be tied together.&lt;/p&gt;
&lt;p&gt;We need to get the right tools to researchers.&lt;/p&gt;
&lt;p&gt;Representation of data is as important as data itself.&lt;/p&gt;
&lt;p&gt;I remind researcher to cite data in their reference lists.&lt;/p&gt;
&lt;p&gt;There is a discussion around whether raw data should be stored, of if
it&amp;rsquo;s possible to derive the data from code, could that be sufficient,
it seems agreed that this needs to be decided by the community to find
their own norms.&lt;/p&gt;
&lt;p&gt;Roles and responsibilities around costs are one of the main issues that
universities are currently discussing.&lt;/p&gt;
&lt;p&gt;(Today I learnt about the &lt;a href=&#34;http://www.dcc.ac.uk&#34;&gt;Digital Curation Centre&lt;/a&gt; in the UK, I feel
a little bad that I&amp;rsquo;d not totally been on top of that before).&lt;/p&gt;
&lt;p&gt;There is a discussion on data journals and data articles. (I&amp;rsquo;m not entirely sure that
this conversation gets us anywhere further than describing the world as we find it).&lt;/p&gt;
&lt;p&gt;There is a discussion around funding, it&amp;rsquo;s asked whether data management and storage for
research data represents a new market for the private sector. Strong reservations are expressed
by multiple people, and the idea is compared to what has happened with scientific publications.&lt;/p&gt;
&lt;h2 id=&#34;breakout-session-on-incentives&#34;&gt;Breakout session on incentives.&lt;/h2&gt;
&lt;h3 id=&#34;paul-ayris---implementing-the-future-the-leruhttpleruorg-roadmap-for-research-data&#34;&gt;Paul Ayris - Implementing the Future: the &lt;a href=&#34;http://leru.org&#34;&gt;LERU&lt;/a&gt; roadmap for research data.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;each university needs a research data management plan&lt;/li&gt;
&lt;li&gt;researchers should have data management plans&lt;/li&gt;
&lt;li&gt;LERU recognises that data should be open by default&lt;/li&gt;
&lt;li&gt;rewards and incentives for researchers need further development&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Excitingly the rectors of the universities that comprise the LERU group were
very positive about adopting an open data policy.&lt;/p&gt;
&lt;p&gt;The point in the roadmap about incentives for researchers has the optimistic view
that there will be real economic benefit from opening up data early, and that
will lead to the creation of more resources downstream that researchers can
later benefit from.&lt;/p&gt;
&lt;p&gt;A significant barrier is that data is not part of the way that research evaluation
is done. Everything still hinges on the research article.&lt;/p&gt;
&lt;p&gt;Not all journals require data to be deposited. Researchers are not going to deposit
data out of the goodness of their heart. There are few rewards for data sharing,
even concrete rewards and prizes. No LERU universities have any such prize.&lt;/p&gt;
&lt;p&gt;The recommendations on how to improve the situation include the common themes&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- cite the data
- enforce data policies
- reward data contributions
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Currently a good number of institutions have not developed a good research data policy,
or data curation systems or policies.
It&amp;rsquo;s not that it&amp;rsquo;s not important, it&amp;rsquo;s just too early in the process. Institutions are
currently more involved with looking at open access, open data has just not made
to to the top of the pile yet.&lt;/p&gt;
&lt;p&gt;Most are planning to do something, they just haven&amp;rsquo;t started yet.&lt;/p&gt;
&lt;p&gt;### Sünje Dallmeier‐Tiessen - Incentives for Open Science Attribution, Recognition, Collaboration.&lt;/p&gt;
&lt;p&gt;Questions that come up from researchers&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;How do I find data referenced in this paper.

This dataset is great! Has the author shared more?

Why should I bother to share my data, no one will see it anyway.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sünje is working with &lt;a href=&#34;http://www.datacite.org&#34;&gt;DataCite&lt;/a&gt; and &lt;a href=&#34;http://orcid.org&#34;&gt;ORCID&lt;/a&gt; on &lt;a href=&#34;http://odin-project.eu&#34;&gt;ODIN&lt;/a&gt;, a way
to link data, papers and people. This kind of infrastructure can help answer many of
the questions that people have today about data.&lt;/p&gt;
&lt;p&gt;Again The Data Citation principles are mentioned.&lt;/p&gt;
&lt;p&gt;She gives a great example of how &lt;a href=&#34;http://orcid.org/0000-0002-5769-7094&#34;&gt;Kyle Cranmer&lt;/a&gt; uses his ORCID profile to show how he has contributed
to data creation on the ATLAS experiment.&lt;/p&gt;
&lt;p&gt;(It looks to me that this question of data citation is now well within the realm of having been technically solved, so we need to move
to advocacy, and we need to teach researchers how to do this. The question of &amp;ldquo;how can I cite data&amp;rdquo; has a clear answer. Getting people
to find out about the answer is the next challenge).&lt;/p&gt;
&lt;h3 id=&#34;veerle-van-den-eynden-and-libby-bishop---incentives-for-sharing-research-data-evidence-from-an-eu-study&#34;&gt;Veerle Van den Eynden and Libby Bishop - Incentives for sharing research data, evidence from an EU study.&lt;/h3&gt;
&lt;p&gt;They looked at case studies from a number of EU countries across a number of different disciplines. There are a diverse range of methods for data sharing.
The report will be online next week and the interviews will go into their university repository and will also be available (Open Data FTW!!).&lt;/p&gt;
&lt;p&gt;The incentives that these researchers identified were:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- direct benefit
    - collaborations are more robust
    - career visibility
    - get wiser
    - is better for science

- norms
    - default in the research group
    - hierarchical sharing throughout their research career
    - conservative non-sharing cultures represent a challenge
    - openness benefits research, but individual researchers reluctant to take lead

- external drivers
    - funders
    - data support services
    - publishers
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These external drivers are not the main drivers, but they do help to shift the landscape.&lt;/p&gt;
&lt;p&gt;The big fear remains being scooped. We need to create a level playing field for sharing. Sharing failed experiments were mentioned in biology and chemistry
was mentioned as being very important (but still people do not do this yet).&lt;/p&gt;
&lt;p&gt;Data citation didn&amp;rsquo;t feel that they had to be able to track reuse of their data, but they were expecting citation for reuse.&lt;/p&gt;
&lt;p&gt;Micro-publishing and micro-citation were mentioned as important, especially in the life sciences. You need to be able to provide atomic level identifiers.&lt;/p&gt;
&lt;p&gt;The report and full recommendations will be available at &lt;a href=&#34;http://knowledge-exchange.info&#34;&gt;http://knowledge-exchange.info&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;open-discussion-after-breakout-session&#34;&gt;Open discussion after breakout session.&lt;/h3&gt;
&lt;p&gt;It&amp;rsquo;s mentioned that there is an error in equating data publication with formal publication. It should be reported as a separate output. It&amp;rsquo;s also mentioned
that in the humanities when data is cited the compilers of the data is currently not included in that data citation. (I have to say that I think that the commenters
full comment is not inconsistent with the idea of actually including names in citations, even if they are not being used right now).&lt;/p&gt;
&lt;p&gt;Someone asks for a data repository with an embargo for the period of when a paper is under review. Sünje mentions that &lt;a href=&#34;http://zenodo.org&#34;&gt;Zenodo&lt;/a&gt; can support this.&lt;/p&gt;
&lt;p&gt;There is a very interesting discussion around aggregation of data, vs the original collection of the data. A specific paper is mentioned where there are about
40 authors of an aggregation paper. The data that they aggregated were not in a state to be cited, they are not, at this point in time, citable. It&amp;rsquo;s put to
one of the commenters that he could make a comment in the article on the journal platform to ask the authors to correctly cite the original data that they aggregated,
and he said that he would be worried of making a comment like that, for fear of a negative impact on his future funding prospects.&lt;/p&gt;
&lt;p&gt;I mention that research assessment needs to improve to seriously look at non-article contributions. I mention that researchers may need to look past the impact factor.
There is an uncomfortable titter of polite laughter at the recommendation in the room, and we pass quickly over the point.&lt;/p&gt;
&lt;p&gt;We do talk about the concrete steps that are out there to reward this kind of behaviour, and there are no institutions that formally recognise and reward these practices.
That&amp;rsquo;s a bit of a red flag there.&lt;/p&gt;
&lt;p&gt;We ask what is the kind of reward that would make a difference. It&amp;rsquo;s thought that money would be counter-productive. Research money would be nice. Researchers want
help to do their work. They want good services. If they can find people to work with who are professionals in managing data, that would be helpful.&lt;/p&gt;
&lt;p&gt;Tim Hunt mentions that the ORCID interface is terrible. Work on that would be very valuable. &amp;ldquo;if you don&amp;rsquo;t make a good interface, you might as well not get out of bed&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;We talk about whether software should be usable, would that increase the uptake of good behaviour, but there is no conclusion from the group on this point.&lt;/p&gt;
&lt;p&gt;We come back to to the issue of what kind of a thing the data contribution is. Do we want databases to count as patents or publications? Do we not want them to
count as databases?, actually the point is more about what kind of IP we want for the data, which actually makes a lot of sense as a question. There is a strong
call to make the data open. I have some thoughts on &lt;a href=&#34;http://partiallyattended.com/2008/07/14/patents-and-peer-review/&#34;&gt;the differences between patents and papers&lt;/a&gt;.
This also touches on the question of who is the owner of the data?&lt;/p&gt;
&lt;h2 id=&#34;reporting-session-from-working-groups&#34;&gt;Reporting session from working groups.&lt;/h2&gt;
&lt;h3 id=&#34;data-management-and-sharing&#34;&gt;Data management and sharing.&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;- Issue: need coordination between different data repositories and related services
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The key message is that a cultural change is needed when it comes to dealing with data.&lt;/p&gt;
&lt;p&gt;Collection of personal data for scientific research is considered legitimate subject to safeguards, under the view of&lt;br&gt;
EU data and privacy policies. They are moving towards a one stop shop model for these kinds of data use cases.&lt;/p&gt;
&lt;p&gt;It is considered that data protection laws will not require additional resources from institutes (though that&amp;rsquo;s an opinion
that flies in the face of common sense, so it will be interesting to see if it holds up).&lt;/p&gt;
&lt;h3 id=&#34;storage-curation-and-interoperability&#34;&gt;Storage, curation and interoperability.&lt;/h3&gt;
&lt;p&gt;There was a speaker from &lt;a href=&#34;http://www.dans.knaw.nl&#34;&gt;Data Archiving and Networked Services&lt;/a&gt;.
It was put that it would be good to&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- provide certification for digital repositories  
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A lot of technology is working now for managing data, but people don&amp;rsquo;t know about it, so we need to&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- improve advocacy around existing solutions
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Key points from this discussion were&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- can you trust the data in a repository?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To get to that we need to understand the appropriate level of curation for the data. Metadata is critical. Scientific quality
is the responsibility of both the researcher and the institute.&lt;/p&gt;
&lt;p&gt;On fraud, who is responsible for it. If it&amp;rsquo;s found, who owns it?&lt;/p&gt;
&lt;p&gt;How do you create a level playing field. It&amp;rsquo;s mentioned that the UK and the Netherlands are paying for repositories, but
that might lead to less open access, as those bodies may decide at some point to no longer make their institutional repositories
available to people outside of their institution.&lt;/p&gt;
&lt;h3 id=&#34;data-discoverability-access-and-reuse&#34;&gt;Data discoverability access and reuse.&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;- deposit you data into existing structured DBs where they are available
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href=&#34;http://www.elixir-europe.org&#34;&gt;Elixir&lt;/a&gt; is mentioned in this talk.&lt;/p&gt;
&lt;p&gt;There is a new copyright exception in the UK, but this is limited to non-commercial uses. New copyright exceptions are coming online, but they
are not perfectly fit, in their current form, to totally support Big Data reuse.&lt;/p&gt;
&lt;p&gt;There is a comment that the work Elsevier has done on article of the future, with creating in-article visualisations, involved some discussions around whether
these visualisations would be subject to copyright, as they were a derivative work of the original article.&lt;/p&gt;
&lt;p&gt;It was mentioned that we need to keep an eye on the emergence of new data types or new technologies. An eye needs to be
kept on return on investment.&lt;/p&gt;
&lt;p&gt;There is data that shows that an article that has associated data published will get cited more.&lt;/p&gt;
&lt;p&gt;If we want open data, then we should also have open access.&lt;/p&gt;
&lt;p&gt;When it comes to copyright infringement of machine copying, what should count is not that a copy is made, but the intent behind the copying.&lt;/p&gt;
&lt;h3 id=&#34;rewards-and-incentives-for-good-data-management-the-carrot-session&#34;&gt;Rewards and incentives for good data management (the carrot session).&lt;/h3&gt;
&lt;p&gt;I&amp;rsquo;ve written up this session earlier in this blog post, so I&amp;rsquo;m going to pass over the summing up of the session.&lt;/p&gt;
&lt;h3 id=&#34;breakout-session---post-summing---discussion&#34;&gt;Breakout session - post summing - discussion.&lt;/h3&gt;
&lt;p&gt;There is a comment that we need to support the skills for interpreting the data in addition to the skills for creating data. Time for a quick coffee.&lt;/p&gt;
&lt;p&gt;That discussion session was fairly low key, I think we have hit maximum overlap on the issues, and we are definitely recycling both issues, and proposed solutions. What
the concluding discussion will bring we will now discover.&lt;/p&gt;
&lt;h3 id=&#34;concluding-discussion-session&#34;&gt;Concluding discussion session.&lt;/h3&gt;
&lt;p&gt;PLOS mention that they are going to automatically start to collect usage of data, and extend their ALM activity towards data use. They have an NSF grant to look at this. I understand that this program is called &amp;ldquo;making data count&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Good data management is good science!&lt;/p&gt;
&lt;p&gt;The carrot is a better approach than the stick. We need to listen to what scientists are telling us about how they see this situation, and we need to be responsive to that.&lt;/p&gt;
&lt;p&gt;When talking about raw costs for infrastructure, the purchasing power of an institution or a funder is much bigger than an individual researcher. This points towards an idea where funders possibly ought to do bulk negotiation, and distribute storage or compute credits to researchers, rather than raw funding. This is the approach the Phil Bourne is discussing with the NIH.&lt;/p&gt;
&lt;p&gt;There is a discussion on costs. Storage is mentioned as being perhaps not a significant factor, compute and electricity are also mentioned. (I&amp;rsquo;ve done an estimate that by 2050 it will cost 1$ to store an exobyte of data, however the truth here is that costs are highly domain specific, and there is a wide distribution of use cases and levels of expertise amongst researchers, raw storage costs are only one aspect of the issue.) I think that a general discussion on this topic is not as helpful as identifying specific issues, or specific solutions.&lt;/p&gt;
&lt;p&gt;The discussion on enforcement of policy is mentioned. The commission says that they want a bottom up solution, but it is mentioned that a data management plan represents a contractual obligation. (It&amp;rsquo;s fairly well known that funders are very shy of brandishing sticks, it&amp;rsquo;s unpopular, it could lead to unintended consequences, but when it comes to altering behaviour through financial incentive it&amp;rsquo;s hard to see options that could be as powerful as penalties for not sharing data as laid out in data management plans, though given the underlying complexity of different research areas I would not want to be the one to pull that trigger).&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s mentioned that making papers, data and software open will give a benefit to industry and innovation.&lt;/p&gt;
&lt;p&gt;We tip toe over to the topic of open peer review. I&amp;rsquo;ll just tip toe away from this topic right now, as it&amp;rsquo;s fairly off topic for this workshop.&lt;/p&gt;
&lt;h2 id=&#34;closing-remarks&#34;&gt;Closing remarks&lt;/h2&gt;
&lt;p&gt;This has been a harmonious workshop. There is general agreement that we should have open access to research data, and we have many interested parties. We have a long way to go, we also have agreement that we need to change the culture at every level, and that we are possibly not moving fast enough. Being able to hire and obtain technical support has resonated, and has been mentioned several times (I&amp;rsquo;ll put in another shout out to &lt;a href=&#34;http://software-carpentry.org&#34;&gt;http://software-carpentry.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Where does the data go? Who pays for it? Those are still big questions, and should be developed trans-nationally.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s mentioned that we need to identify specific repositories for specific disciplines, and I would refine that and say that we have very clear locations for specific kinds of data right now, what we need to identify are the fields that are struggling now, and in particular identify fields that are at early risk of walking into a data avalanche where there are no previous good examples of data care in those fields, and who have gotten into this situation due to new tools that have become available to them, for example microscopy.&lt;/p&gt;
&lt;h2 id=&#34;issues-and-questions-that-came-up-today&#34;&gt;Issues and questions that came up today.&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;- how do you deal with large unstructured data sets?
- legal and ethical issues affect the use of such data
- how does the distance to the commercial market affect acceptance of and practices in data sharing
- who sets the metadata structures in different communities?
- can we introduce licences that can be interoperable for data?
- who pays, who is responsible for paying?
- Issue: need coordination between different data repositories and related services
- can you trust the data in a repository?
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;suggested-solutions-to-issues-that-came-up-today&#34;&gt;Suggested solutions to issues that came up today.&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;- give DOIs, or similar, to data
- move towards an internationally level playing field on ethics for research
- create a profession of data curators
- appropriately label datasets to support fine-grained attribution
- develop a culture of acknowledgement
- provide funding for data sharing
- use embargoes as a mechanism to incentivise researcher to make timely use of their own data
- take a percent, say 15%, and set that aside in every grant for data sharing, curation and storage
- update the EU copyright directive
- give a prize for examples of good use of data (it&#39;s mention that there is a data prize in The Netherlands).
- convince people to copy good data management plans (and follow them)
- cite data in reference lists, use the [FORCE 11 data citation principles](https://www.force11.org/datacitation)
- create an open marketplace of good data management plans
- data managmeent plans should be a living document
- include the data scientist at the point of experimental design
    - (I&#39;m remineded of a story from Janelia Farm ...)
- cite the data
- enfore data policies
- reward data contributions
- create an EU-wide directive on data policy for scientific research
- provide certification for digital repositories
- improve advocacy around exising solutions
- Funders shuold mandate open data
- The EU shuoljd take care of infrastrucutre euope-wide to promote a level playhing field.
- create a code of conduct teaching young researchers about the ethical issues around data
- depost you data into existing strucutred DBs where they are available
- do bulk purchasing from providers, and distribute compute and storage credits to researchers
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>FuturePub - future of publishing event, hosted by NESTA and WriteLaTeX</title>
      <link>http://scholarly-comms-product-blog.com/2014/05/30/future-pub2/</link>
      <pubDate>Fri, 30 May 2014 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2014/05/30/future-pub2/</guid>
      <description>&lt;p&gt;This is the second #futurepub event that I&amp;rsquo;ve been to. I also attended &lt;a href=&#34;http://partiallyattended.com/2014/01/18/writelatex-overleaf-launch/&#34;&gt;the last one&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The event was hosted by &lt;a href=&#34;http://www.nesta.org.uk&#34;&gt;Nesta&lt;/a&gt;. Nesta have just launched the &lt;a href=&#34;http://www.nesta.org.uk/project/longitude-prize-2014&#34;&gt;&amp;ldquo;new longitude&amp;rdquo; prize&lt;/a&gt; - which looks pretty interesting. There were six rapid fire talks, and I found the presentation format to be excellent. As with the previous event, this one was organised by the &lt;a href=&#34;http://writelatex.com&#34;&gt;WriteLaTeX&lt;/a&gt; guys, and I&amp;rsquo;d just like to extend a big thanks to them for again putting on a great little event.&lt;/p&gt;
&lt;h1 id=&#34;shane-from-blikbook&#34;&gt;Shane from BlikBook&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.blikbook.com&#34;&gt;BlikBook&lt;/a&gt; have a service that helps students and teachers communicate. They aim to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;connect students and courses around the world&lt;/li&gt;
&lt;li&gt;maximise how knowledge is shared within a classroom&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Data coming out of behaviour is useful. (It looks a bit like a classroom version of a community hub, they also have some anonymous features). This started in London, they took their first round of investment in 2011. They are spread across a number of universities in the UK, with some penetration in the US. (would be interesting to understand how they compete/compare with course.com ??, and how big that opportunity is).
They have had no abuse of the use of the anonymous tool. It&amp;rsquo;s been useful where there are large cohorts of foreign students, who have a tendency to be shy in real life situation. Also in situations where there are large variations within the cohorts.&lt;/p&gt;
&lt;h1 id=&#34;jo-mcarthur---open-access-buttonhttpswwwopenaccessbuttonorg&#34;&gt;Jo Mcarthur - &lt;a href=&#34;https://www.openaccessbutton.org&#34;&gt;Open Access Button&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;The bottom line is that this is awesome. They are moving forward with the original idea. They now have about 20 people involved in helping to manage the project. They are working on
- integration into wikipedia
- monitoring compliance with OA policy
- linking published research with research in repositories&lt;/p&gt;
&lt;p&gt;One great idea that was hinted at via the Q&amp;amp;A was that if they see a specific paper being requested by a lot of people, they may try to encourage that author to deposit a version of their manuscript into an institutional repository, and then direct other users of the OA button to that version.&lt;/p&gt;
&lt;h1 id=&#34;lou-woodley---myscicareer-with-eva-amson&#34;&gt;Lou Woodley - MySciCareer (with Eva Amson)&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://myscicareer.com&#34;&gt;MySciCareer&lt;/a&gt; is a place where you can get stories about what people have ended up doing, who have come from a scientific background - including stories about people who have continued in the academic track.&lt;/li&gt;
&lt;li&gt;Scientists become preoccupied by the research cycle
&lt;ul&gt;
&lt;li&gt;reading papers&lt;/li&gt;
&lt;li&gt;writing grants&lt;/li&gt;
&lt;li&gt;getting data&lt;/li&gt;
&lt;li&gt;publishing&lt;/li&gt;
&lt;li&gt;communicating&lt;/li&gt;
&lt;li&gt;assessing/being assessed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;but there is one other big thing that troubles researchers:
&lt;ul&gt;
&lt;li&gt;carrer decisions
&lt;ul&gt;
&lt;li&gt;0.45% of people who start in research end up as a professor&lt;/li&gt;
&lt;li&gt;from NIH less than 8% end up on tenure track&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;MySciCareer
&lt;ul&gt;
&lt;li&gt;collates stories from across the web about the paths that people have taken in their careers&lt;/li&gt;
&lt;li&gt;you can also search the site by last scientific qualification or job type&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;they would like to:
&lt;ul&gt;
&lt;li&gt;expand the number of contributions on the site (if you have a story, do let them know).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;nowomicscomhttpnowomicscom---richard-smith&#34;&gt;&lt;a href=&#34;http://nowomics.com&#34;&gt;nowomics.com&lt;/a&gt; - Richard Smith&lt;/h1&gt;
&lt;p&gt;This allows you to follow the latest data and papers related to genes, and other entities. It&amp;rsquo;s a bit like &amp;ldquo;twitter&amp;rdquo; for genes.&lt;/p&gt;
&lt;p&gt;There are &amp;gt; 1500 biological databases.&lt;/p&gt;
&lt;p&gt;You follow the genes that you work on, or the processes that you are interested in. Does text mining on abstracts from pubmed. Sounds pretty interesting, and a bit of a low hanging fruit. (I later checked out the product, and it looks very nice).&lt;/p&gt;
&lt;p&gt;There are a lot of questions that one could ask of this service.
- are the feeds available as RSS&lt;br&gt;
- how many users to they have&lt;br&gt;
- what&amp;rsquo;s their traction &amp;amp; growth&lt;br&gt;
- how much of pubmed have they&lt;br&gt;
- do they have longitudinal information&lt;br&gt;
- do they distinguish OS vs non OA&lt;br&gt;
- do they look at citation based metrics&lt;br&gt;
- what is their DB, what&amp;rsquo;s their stack&lt;br&gt;
- can we feed a full text into this system?&lt;br&gt;
- no yet, but sometime in the future?&lt;/p&gt;
&lt;h1 id=&#34;greg---full-stack-developer-at-sparrhohttpwwwsparrhocom&#34;&gt;Greg - full stack developer at &lt;a href=&#34;http://www.sparrho.com&#34;&gt;sparrho&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Greg gave a nice little talk.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;greg mentions that the lab their CEO Vivian worked in had a personal curator - Steve - who would do a scan the journals that the group was interested in.&lt;/li&gt;
&lt;li&gt;sparrho is the attempt to build a digital Steve.&lt;/li&gt;
&lt;li&gt;sparrho creates a recommendations engine for people who want to read updated abstracts from the literature.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They have had researchers who have found those gems they would not have found via their normal search patterns. It looks like a good service, I&amp;rsquo;v been helping this team a little with advice on product development, and I wish them a lot of luck with what they are doing.&lt;/p&gt;
&lt;h1 id=&#34;altmetrics---cat---who-is-doing-the-marcomms-for-alt-metrics&#34;&gt;Altmetrics - Cat - who is doing the marcomms for alt-metrics&lt;/h1&gt;
&lt;p&gt;Cat is adamant that they are not saying that a paper is a good paper or a bad paper.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;300k doughnuts served a day&lt;/li&gt;
&lt;li&gt;3M API calls per day&lt;/li&gt;
&lt;li&gt;1 mention per second&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Publishers are starting to integrating this data into marketing. Nature are doing a social selection page - online and in print. They are showing trending articles on their site.&lt;/p&gt;
&lt;p&gt;Elsevier have created a virtual special issue around international archeology day.&lt;/p&gt;
&lt;h1 id=&#34;wrap-up&#34;&gt;wrap-up&lt;/h1&gt;
&lt;p&gt;So that was it, there were a few nice conversation after the talks, and it was well worth spending the evening attending.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to create threads between publications and clinical trial registraion numbers</title>
      <link>http://scholarly-comms-product-blog.com/2014/01/31/threaded-publications-discussion/</link>
      <pubDate>Fri, 31 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2014/01/31/threaded-publications-discussion/</guid>
      <description>&lt;p&gt;Yesterday I attended an interesting meeting to discuss how to improve the connection between clinical trial registration ids and publications. My raw notes from the meeting follow. This is being discussed as publication threads, but the idea discussed here stands apart from the kind of publication threads that the endcode project worked on.&lt;/p&gt;
&lt;h1 id=&#34;attendees&#34;&gt;attendees&lt;/h1&gt;
&lt;h2 id=&#34;attendees---organisations&#34;&gt;ATTENDEES - organisations:&lt;/h2&gt;
&lt;p&gt;eLife
f1000
PLOS
BMC
Springer
lancet
BMJ
crossref&lt;/p&gt;
&lt;h2 id=&#34;attendees---people&#34;&gt;attendees - people&lt;/h2&gt;
&lt;p&gt;Geoffrey Bilder, CrossRef, Director of Strategic Initiatives Rachael Lammey, CrossRef, Product Manager CrossMark Daniel Shanahan, BioMed Central, Associate Publisher Tim Stevenson, BioMed Central, Product Manager
Deborah Kahn, BioMed Central, EVP Publishing Caroline Black, BioMed Central, Senior Publisher Katherine Barton, BMJ, Operations Manager Josie Breen, BMJ, Head of Editorial Production Isaac Jones, BMJ, Production Manager
Ian Mulvany, eLife, Head of Technology
Iain Hrynaszkiewicz, F1000, Outreach Director
Karen Rowlett, F1000Research, Managing Editor
Helene Faure, ISRCTN Database Manager
Hannah Jones, The Lancet, Managing Editor
Dan Lewsley, The Lancet, Head of Production
Joseph Brown, PLoS, Senior Editorial Manager
Volker Boeing, Springer, Director, Process and Content Management Mirjam Kessler, Springer, Bibliographic Metadata Manager&lt;/p&gt;
&lt;h2 id=&#34;background-and-status&#34;&gt;Background and status&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;this is a followup for the idea that was originally &lt;a href=&#34;http://blogs.biomedcentral.com/bmcblog/2011/01/14/towards-threaded-publications-helping-to-set-the-scientific-record-straight/&#34;&gt;blogged by iainh in 2011&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;the initiative stalled in 2013&lt;/li&gt;
&lt;li&gt;about 30k trials are registered per year&lt;/li&gt;
&lt;li&gt;for trials that are registered, there is nothing to link all of this together&lt;/li&gt;
&lt;li&gt;currently we have 1 way links from the trial to the publications&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The vision is for every publication to forward and backward link to every other publication, but this is not practical. This is where crossmark comes in. The idea would be to implement a hub and spoke model, and get everyone to wire things together in crossmark. (My take from the technical side was that the trial registration id will be the parent in all of the relationships, but this needs a bit more fleshing out).&lt;/p&gt;
&lt;h2 id=&#34;crossmark-notes&#34;&gt;Crossmark notes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;lots of uptake.&lt;/li&gt;
&lt;li&gt;over 1M assertions so far.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;fundref&#34;&gt;Fundref&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;the key thing here is that it&amp;rsquo;s a typed namespace that sits under the crossmark metadata container. They call this a &amp;ldquo;program&amp;rdquo; within crossmark.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;threaded-publications-proposal&#34;&gt;Threaded publications proposal&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;they would like to mock the fundref approach for publication threads, and this meeting is to promote this idea, and to move towards an agreement for the namespace.&lt;/li&gt;
&lt;li&gt;Geoff does the &amp;ldquo;can you identify&amp;rdquo; the blacked out paper.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;threaded-publications-model&#34;&gt;Threaded publications model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;mentions that delays were related to missing infrastructure and competing with priorities&lt;/li&gt;
&lt;li&gt;feels that they have launched another project that can act as a good model for threaded publications&lt;/li&gt;
&lt;li&gt;this model is fundref&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;fundref-1&#34;&gt;Fundref&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;was done quickly
&lt;ul&gt;
&lt;li&gt;they tried to do the simplest thing that could possibly work&lt;/li&gt;
&lt;li&gt;they put together a quick governance group from funders and publishers&lt;/li&gt;
&lt;li&gt;knew that they didn&amp;rsquo;t want to overcomplicate things&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;they did a couple of things
&lt;ul&gt;
&lt;li&gt;they standardised on a vocabulary released under CC0&lt;/li&gt;
&lt;li&gt;have an update mechanism&lt;/li&gt;
&lt;li&gt;provided an example widget for filling out the info
&lt;ul&gt;
&lt;li&gt;main thing was this is an example for implementation purposes &lt;a href=&#34;labs.corssref.org/fundref/widget&#34;&gt;labs.corssref.org/fundref/widget&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;the data collected is also released under a CC0 licence&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;proposal&#34;&gt;Proposal&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;create a working group&lt;/li&gt;
&lt;li&gt;create a registry of registries
&lt;ul&gt;
&lt;li&gt;assign registry ids to the registries that we are interested in interoperating with&lt;/li&gt;
&lt;li&gt;registration template for URLs to get to the registration (might need a resolution service?)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;how do we collect it? would like to put up a similar widget to the fundref one, and get this implemented in the submission systems&lt;/li&gt;
&lt;li&gt;this all then gets passed into crossref in the same way that fundref information gets in.&lt;/li&gt;
&lt;li&gt;then using the crossref API you can answer the following question
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;show me all the articles that refer to the same clinical trial numbers mentioned in this document&amp;rdquo;&lt;/li&gt;
&lt;li&gt;could be extended to also include the following query&lt;/li&gt;
&lt;li&gt;AND &amp;ldquo;the publisher of that article is in a list of trusted participants&amp;rdquo;&lt;/li&gt;
&lt;li&gt;BOOM! done, thank you!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;issues&#34;&gt;Issues&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;how do you find out if something has been updated?
&lt;ul&gt;
&lt;li&gt;the &amp;ldquo;do you know what this is&amp;rdquo; game does not work with how we signify that there is an update to an article&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;open-discussion&#34;&gt;open discussion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;the proposal is to only include the trial ID&lt;/li&gt;
&lt;li&gt;original proposal from ian H proposed in addition
&lt;ul&gt;
&lt;li&gt;article type&lt;/li&gt;
&lt;li&gt;publication date&lt;/li&gt;
&lt;li&gt;some other attributes &amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;issues-1&#34;&gt;Issues&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;no article ontology&lt;/li&gt;
&lt;li&gt;systematic reviews could explode the threads&lt;/li&gt;
&lt;li&gt;many papers may not cite the trial&lt;/li&gt;
&lt;li&gt;how do you identify the trial ID in the content?
&lt;ul&gt;
&lt;li&gt;some publishers have collected this from some point in time, however not all publishers will have this data retroactively.&lt;/li&gt;
&lt;li&gt;BMC tags this in their XML, and example can be seen here:&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;how journals publish the id and name is not at all standardised
&lt;ul&gt;
&lt;li&gt;checking that the trial id is valid&lt;/li&gt;
&lt;li&gt;that the registration is valid
&lt;ul&gt;
&lt;li&gt;Geoff mentions that they had exactly the same issues at the start of fundref&lt;/li&gt;
&lt;li&gt;mentions that we might be able to identify the &amp;ldquo;shape of an id&amp;rdquo;&lt;/li&gt;
&lt;li&gt;issue is raised that journals might just screw this up&lt;/li&gt;
&lt;li&gt;AIP did a survey and ran fundref to see how often people got things wrong&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;discussions&#34;&gt;Discussions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;there is no standard article type ontology across publishers
&lt;ul&gt;
&lt;li&gt;Geoff mentions that his take is that the thing we are trying to capture is the relationship between the article and the protocol.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;the issue of systematic reviews is that they are important, can we afford not to include them
&lt;ul&gt;
&lt;li&gt;perhaps get the author to fill in the widget, ask them to be thoughtful in terms of which trials they link to.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;data is mentioned as being important&lt;/li&gt;
&lt;li&gt;tracking provenance of claims is mentioned as an issue
&lt;ul&gt;
&lt;li&gt;how do we allow other people to make assertions about DOIs after the fact, this leads to issues of provenance, and is related to trust&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;if we only go forward from this date, it will be years before we have any useful data, if we can go backwards as well as forwards (make assertions on existing entities), it would help
&lt;ul&gt;
&lt;li&gt;publishers could try and get their authors to go back and post-annotate their own publications - perhaps via ORCID&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;funders should be involved
&lt;ul&gt;
&lt;li&gt;wellcome should be involved&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Geoff mentions that more requirements on the initiative will increase the chances the initiative would fail
&lt;ul&gt;
&lt;li&gt;Iain H mentions that there is probably an editorial issue of what our tolerance to the data quality is. Are editors happy with the quality of the result?
&lt;ul&gt;
&lt;li&gt;is that an important enough of an issue to stop a pilot?&lt;/li&gt;
&lt;li&gt;thinks that a pilot is a great way to have that conversation, if people know we are tracking this data, then people will start playing attention to the data more quickly&lt;/li&gt;
&lt;li&gt;might help sort out the predatory publishers
&lt;ul&gt;
&lt;li&gt;mentioned that that might not be the case, we should worry about this, but on the other hand, the more contribuions you get from researchers, the easier it is to self regulate&lt;/li&gt;
&lt;li&gt;will get sock puppet problems, but these problems are there anyway, this initative won&amp;rsquo;t be a cause of that issue&lt;/li&gt;
&lt;li&gt;clearly there are people out there who are trying to game the system, best thing you can do is provide as much evidence as you can&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;would there be a role for an admin and curation role
&lt;ul&gt;
&lt;li&gt;these are the kinds of things that fundref are discussing&lt;/li&gt;
&lt;li&gt;in any of these cases you need to know who is making the assertion&lt;/li&gt;
&lt;li&gt;at the moment in the crossref system everything is being asserted by the publisher&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;interesting question, how is this better than just scraping pubmed?
&lt;ul&gt;
&lt;li&gt;would capture more articles that are connected to clinical ids, where at the moment those connections are not tagged&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Geoff mentions that there are many situations where the best answer is to force people to cite this information, but his instinct is that they don&amp;rsquo;t want to overload that section, there won&amp;rsquo;t be a way to enforce consistency. (we all know that won&amp;rsquo;t work :P)&lt;/li&gt;
&lt;li&gt;do we include article type?&lt;/li&gt;
&lt;li&gt;do we need any other metadata? can anyone think of any other kind of metadata&lt;/li&gt;
&lt;li&gt;how do we identify the trial id in the content?&lt;/li&gt;
&lt;li&gt;how feasible is this for production/operations to manage?
&lt;ul&gt;
&lt;li&gt;really needs to happen at submission, and automate everything downstream from there&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;is there any additional cost on top of crossref?
&lt;ul&gt;
&lt;li&gt;NO (at this point)
&lt;ul&gt;
&lt;li&gt;if however the steering group complicated the spec, the cost might need to be recouped, basically easier is cheaper&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;trials can be registered in many different places, often because trials can happen in different countries, each requiring registration
&lt;ul&gt;
&lt;li&gt;could the system under discussion be used to connect one trial to another trial&lt;/li&gt;
&lt;li&gt;how do you then traverse this graph&lt;/li&gt;
&lt;li&gt;could this be done in a self-policing kind of a way?&lt;/li&gt;
&lt;li&gt;how do you distinguish between multiple ids for the same trial and trials related to a clinical review
&lt;ul&gt;
&lt;li&gt;the parent is always the clinical trial, under one trial a clinical review will only be listed once&lt;/li&gt;
&lt;li&gt;if you want to traverse to publications that are tied a trial via another name for that trial, under the parent trial you would want to have the other trial listed as a child, and you could then traverse to the children of that node.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Do we need to worry about article types now?
&lt;ul&gt;
&lt;li&gt;Geoff says that we should come up with a proposed name for the relationships and if the debate goes on too long, then pass on and not worry about it.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;round-table-wrap-up&#34;&gt;round table wrap up&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;good idea&lt;/li&gt;
&lt;li&gt;registration of trials is a worry&lt;/li&gt;
&lt;li&gt;could become self policing&lt;/li&gt;
&lt;li&gt;doubts we could get back data
&lt;ul&gt;
&lt;li&gt;everything needs critical mass&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;prospect and licensing information is mentioned
&lt;ul&gt;
&lt;li&gt;Geoff mentions that this is all confused a bit&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;great project, need to talk to editors&lt;/li&gt;
&lt;li&gt;if you have a field in your submissions system then you want to make this as a condition of acceptance&lt;/li&gt;
&lt;li&gt;can we say that back-files are optional?
&lt;ul&gt;
&lt;li&gt;yes, we have to say that this is optional&lt;/li&gt;
&lt;li&gt;we would also say that back-files are not out of bounds&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;from the perspective of a registrar, there is a benefit
&lt;ul&gt;
&lt;li&gt;more often than not a trial, when quoted, it&amp;rsquo;s correct&lt;/li&gt;
&lt;li&gt;the key issue is that not all papers about trials quote the trial&lt;/li&gt;
&lt;li&gt;registers can find related papers via keyword search&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;thinks its worth doing from the point of the enduser
&lt;ul&gt;
&lt;li&gt;what&amp;rsquo;s the data policy? who owns the data? what&amp;rsquo;s the licence of the data?, crossref make no claims on the data, there are batch delivery methods, and there are charges for this delivery, with opt outs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;another element of this is that sometimes papers are recommended in F1000, and they will check if they include trial IDs there&lt;/li&gt;
&lt;li&gt;what next? is this system, or a version of this, a model for any funded piece of research that creates multiple outputs?
&lt;ul&gt;
&lt;li&gt;it has to be something that links them there has to be a starting point&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;f1000 research also thinks it&amp;rsquo;s a great idea, they have a posters type that might also be something that could be included. Is slightly worried about letting authors put the info in. Would be great to have a check to see if the registration is a valid registration.
&lt;ul&gt;
&lt;li&gt;anything we can do to help them not put in typos is a good idea, enforcing honesty is a much harder issue&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;voice of the researcher would be nice to have in the room&lt;/li&gt;
&lt;li&gt;BMC thinks its a strong idea, they think it&amp;rsquo;s overdue, it&amp;rsquo;s based on clinical trial registration, and we are not doing anything with this data at present. We are not currently exposing benefits of getting links between trials and publications. We need to possibly start simple, be aware that we might encounter some issues, but we need to move forward.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;actions&#34;&gt;Actions&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;- propose a namespace for relationships to be modelled  
- create an interest group now, that could potentially move towards being a crossref working group
    - get a funder involved  
    - get someone from the registry community  
    - get a chair who is not from crossref @done 
        - chair will come from BMC @done 
    - have a product manager who helps coordinate the meeting  
    - we can call the meeting today - meeting 0  
    - arrange the next meeting   
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>WriteLateX/Overleaf launch event at the British Library</title>
      <link>http://scholarly-comms-product-blog.com/2014/01/18/writelatex-overleaf-launch/</link>
      <pubDate>Sat, 18 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2014/01/18/writelatex-overleaf-launch/</guid>
      <description>&lt;p&gt;Last Thursday I attended the launch event for &lt;a href=&#34;https://www.writelatex.com/overleaf&#34;&gt;OverLeaf&lt;/a&gt;. The event was composed of a set of very short talks, followed by a good chance to chat to people. It was a pretty nice evening.&lt;/p&gt;
&lt;h1 id=&#34;dr-bibiana-campos-seijo---mrsc---magazines-publisher-and-editor-of-chemistry-world&#34;&gt;Dr Bibiana Campos Seijo - MRSC - magazines publisher and editor of chemistry world.&lt;/h1&gt;
&lt;p&gt;Science is changing, publising is changing, a lot of this is being driven by technology. There is information overlaod. Publishers need to try to provide solutions to these issues.&lt;/p&gt;
&lt;h2 id=&#34;what-was-interesting-in-2013&#34;&gt;What was interesting in 2013:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;OA mandate&lt;/li&gt;
&lt;li&gt;Predetary OA journals&lt;/li&gt;
&lt;li&gt;Luxury journals - (Go Randy!!!)&lt;/li&gt;
&lt;li&gt;Takedown notices from Elsevier&lt;/li&gt;
&lt;li&gt;Peer reiew developments&lt;/li&gt;
&lt;li&gt;Data management storage and sharing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(I think that&amp;rsquo;s a pretty good overview, I also think that the rise of tablets is the main broad technology trend. Another big story that might change how we think about interacting online are the NSA stories, but it&amp;rsquo;s not clear how that will break down in scientific publihsing. I also think that the increased focus on reproducability is of interest).&lt;/p&gt;
&lt;h2 id=&#34;new-publishing-models&#34;&gt;New publishing models&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;PeerJ&lt;/li&gt;
&lt;li&gt;Rubriq&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It&amp;rsquo;s noticed that chemists are a bit more conservative.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Figshare institutional data platform is mentioned.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The finishing line is in sight for libel reform (this is a great move). Is mentioned that this is indirectly related to publishing, but core to freedom of expression.&lt;/p&gt;
&lt;p&gt;The example of &lt;a href=&#34;http://retractionwatch.com/2013/08/08/insert-data-here-did-researcher-instruct-co-author-to-make-up-results-for-chemistry-paper/&#34;&gt;&amp;ldquo;Emma, please insert NMR data here!&amp;quot;&lt;/a&gt; was mentioned. It was noticed by bloggers, and they are now playing a role in what is happening &lt;a href=&#34;http://www.hyperorg.com/blogger/2014/01/08/what-blogging-was/&#34;&gt;see comments on blogging by David Weinberger&lt;/a&gt;, one of his quotes from this article is&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;We have changed where we turn for analysis, if not for news. We expect the Web to be easy to post to. We expect conversation. We are more comfortable with informal, personal writing. We get more pissed off when people write in corporate or safely political voices. We want everyone to be human and to be willing to talk with us in public.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;whats-in-sotre-for-2014&#34;&gt;What&amp;rsquo;s in sotre for 2014?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;OA, more and more OA&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;linus-schumacher---a-scientist---wolfson-centre-for-mathematical-biology-university-of-oxford-wcmbloghttpstwittercomwcmblog&#34;&gt;Linus Schumacher - a scientist - Wolfson Centre for Mathematical Biology, University of Oxford &lt;a href=&#34;https://twitter.com/WCMBlog&#34;&gt;@WCMBlog&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Language. Differences in understanding of jargon can hinder progress. He thinkgs more and more scientists in the future could act as connectors - jack of all trades, masters of none - but critical to the success of progress of science. They will translate across disciplines.&lt;/p&gt;
&lt;p&gt;Meeting in person is still more critical and productive that vitual meeting.&lt;/p&gt;
&lt;p&gt;In terms of sharing artefacts, and raw data, this can take a lot of space. Cloud based storage often does not cut it any more. BitTorrent Sync can be one way around this (I like that idea).&lt;/p&gt;
&lt;p&gt;Sharing annotations and literatire can be helpful, but the tools to do this are still not great, or are costly (should check out the &lt;a href=&#34;http://www.openannotation.org&#34;&gt;OpenAnnotation&lt;/a&gt; standard).&lt;/p&gt;
&lt;p&gt;Finally on writing, a lot is still done on emailing word documents back and forth. (Yeah, that sucks).&lt;/p&gt;
&lt;p&gt;Preprints opens and accelerates the scientific process. In collaborations this is still often hindered by reservations of some researchers (mentioned bioarxive, interestingly PeerJ&amp;rsquo;s preprints have more volume, genreally things are going in the right direction here).&lt;/p&gt;
&lt;p&gt;He mentions contributions. Now that the maths and biology are so intergrated that you no longer can see the maths anymore, it&amp;rsquo;s harder to see what contribtions a mathematcian might have made in a particualr paper. This integration is great, but how do you capture quite abstract contributions, and how do you acknowledge and mesure them beyond word of of mouth.&lt;/p&gt;
&lt;p&gt;Summary&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;more scientists as connectors&lt;/li&gt;
&lt;li&gt;better software needed&lt;/li&gt;
&lt;li&gt;praise for preprints&lt;/li&gt;
&lt;li&gt;acknowledge coneptual insights&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;karen-rowlett---managing-editor-f1000-research&#34;&gt;Karen Rowlett - Managing Editor, F1000 research.&lt;/h1&gt;
&lt;p&gt;They insist that authors include all the data (I wonder how they deal with very large data - ah, they use figshare, so there are very well understood data limits).&lt;/p&gt;
&lt;p&gt;Karen talks through the review process (of which I am a very big fan indeed).&lt;/p&gt;
&lt;p&gt;They have published 372 articles as of 2014-01-15.&lt;/p&gt;
&lt;p&gt;They are gonig to be working on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ORCIDs&lt;/li&gt;
&lt;li&gt;doi&amp;rsquo;s for reports&lt;/li&gt;
&lt;li&gt;article collections (planned for February - that&amp;rsquo;s almost certainly the &lt;a href=&#34;http://www.ebi.ac.uk/Tools/biojs/registry/&#34;&gt;BioJS&lt;/a&gt; collectoin of papers).&lt;/li&gt;
&lt;li&gt;integraion with WriteLaTeX into editorial workflow&lt;/li&gt;
&lt;li&gt;more repository integration&lt;/li&gt;
&lt;li&gt;data plotting tool to allow readers to play with datasets&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I do like that they have a template on their site that includes all of the elements that are required to produce a paper that will run through the F1000 process quickly.&lt;/p&gt;
&lt;p&gt;They are also very interested in focussing on reproducablity in science.&lt;/p&gt;
&lt;h1 id=&#34;john-hammersly---overleaf&#34;&gt;John Hammersly - OverLeaf&lt;/h1&gt;
&lt;p&gt;WriteLaTeX launched in 2011. They now have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;50k authors in 170 countries&lt;/li&gt;
&lt;li&gt;over 500k documents created&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;OverLeaf is that system in which the document is held centrally, and authors, reviewers, editors and publishers can come to the document. (I will be very interested to see how they manage the premissions process for the reviewing stage, it seems the idea is that where the document lives is held externally from the workflow process of managing the permissions of who gets to review the paper. In my opinion we need to improve that interface, and the place where the document is held can be abstraced, and become a plug and play kind of thing).&lt;/p&gt;
&lt;p&gt;Overleaf also has a RichText editor, this will be critical for gaining wider adoption.&lt;/p&gt;
&lt;h1 id=&#34;conclusion-from-the-evening&#34;&gt;Conclusion from the evening&lt;/h1&gt;
&lt;p&gt;It was fun evening. I got to chat to quite a few people that I&amp;rsquo;d not met before. The space is continuing to evolve, and this event was a good
example of the innovation that is happening here. There was nothing earth shattering mentioned, it was nice to see the Luxury Journal meme being taken up, and it was very nice to chat to Karen and to Bibiana. WriteLaTex have a very small, agile team, and they have created a nice tool, that is gaining market share amongst LaTeX users. I will watch with interest how it evolves.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>advice on publishing research online</title>
      <link>http://scholarly-comms-product-blog.com/2014/01/08/advice-on-publishing-online/</link>
      <pubDate>Wed, 08 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2014/01/08/advice-on-publishing-online/</guid>
      <description>&lt;p&gt;I have posted this post as a comment on the thread over at software carpentry in answer to the question &lt;a href=&#34;https://github.com/swcarpentry/bc/issues/199&#34;&gt;What do we teach about writing/publishing papers in a webby world?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I ended up writing a bit more than I expected, so here are the main peices of advice:&lt;/p&gt;
&lt;p&gt;tl;dr:&lt;br&gt;
- use a reference management tool&lt;br&gt;
- try to find the fastest venue to publish in&lt;br&gt;
- try to publish in an OA journal&lt;br&gt;
- have a look for the best preprint server for your discipline, and add your work there too (might be a university archive)&lt;br&gt;
- add as much supporting material as you can to the right locations, e.g. &lt;a href=&#34;&#34;&gt;github&lt;/a&gt; for code, &lt;a href=&#34;&#34;&gt;figshare&lt;/a&gt; for anything, &lt;a href=&#34;&#34;&gt;vimeo&lt;/a&gt; or &lt;a href=&#34;&#34;&gt;you tube&lt;/a&gt; for videos&lt;br&gt;
- do register for an &lt;a href=&#34;&#34;&gt;ORCID&lt;/a&gt; and add your newly minted publication to your ORCID profile&lt;br&gt;
- don&amp;rsquo;t be afraid to screw around with copyright transfer statements&lt;br&gt;
- use version control for your own sanity&lt;br&gt;
- remember that all the time you spend pretty formatting your paper will be ignored and thrown away by large publishing companies, especially the work you do on reference formatting, so don&amp;rsquo;t do it&lt;br&gt;
- if the collaborative environment of your choice is not working for the group, be pragmatic, drop it, get the damn paper finished already&lt;/p&gt;
&lt;p&gt;I would start by advising people to keep in mind the goals of publishing. You want to get your work out into a venue that will be respected by your peers, and noticed by them. In most cases - but not all cases - this will be a journal published by one of the large STM publishers. Elsevier, Springer, Wiley, Taylor &amp;amp; Francis, PLOS and Sage represent a very large part of that market.&lt;/p&gt;
&lt;p&gt;You want this process to happen as quickly as possible. Aside from the act of writing, and constructing your story, the act of publishing - getting it onto the web - is pure schlep. Every minute longer that you spend in this process is a minute wasted, as it&amp;rsquo;s not adding value to your research or your ability to put yourself in the position of being able to get the resources you need to do the research you are interested in.&lt;/p&gt;
&lt;p&gt;Your first priority is to understand the most appropriate venue and then understand the system that this venue uses to get the work online. Tailor your process to lower the friction between the artefact you create and the process that will be used to get it online.&lt;/p&gt;
&lt;p&gt;The great failure of my industry in the face of the web has been to make allow this process to remain orders of magnitude harder than publishing a post on blogger or wordpress.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll step through some advice covering these topics now.&lt;/p&gt;
&lt;h2 id=&#34;the-most-appropriate-venue&#34;&gt;The most appropriate venue&lt;/h2&gt;
&lt;p&gt;Ask your colleagues, confer with your coauthors, it&amp;rsquo;s usually not hard to determine. A tool like the &lt;a href=&#34;http://www.biosemantics.org/jane/&#34;&gt;Journal author name estimator&lt;/a&gt; has been around for years and it can suggest a journal based on the text of your abstract. In addition the following resources can also help &lt;a href=&#34;http://www.miketaylor.org.uk/tmp/journal-finder.html&#34;&gt;Journal Finder&lt;/a&gt;, &lt;a href=&#34;http://www.edanzediting.com/journal_selector&#34;&gt;http://www.edanzediting.com/journal_selector&lt;/a&gt;, &lt;a href=&#34;http://www.journalguide.com/&#34;&gt;http://www.journalguide.com/&lt;/a&gt; and &lt;a href=&#34;http://etest.vbi.vt.edu/etblast3&#34;&gt;http://etest.vbi.vt.edu/etblast3&lt;/a&gt;. Most of these are for the life sciecnes.&lt;/p&gt;
&lt;p&gt;If your publication is an OA publication the &lt;a href=&#34;http://www.eigenfactor.org/openaccess/index.php&#34;&gt;Eigenfactor Journal Rank tool&lt;/a&gt; will tell you if you are getting good value for money. This ranks cost of the article processing fee against a rank of the journal determined by their own algorithm.&lt;/p&gt;
&lt;p&gt;## Speed of publication&lt;/p&gt;
&lt;p&gt;It might be worth checking if there is an alternative venue that might be a lot faster than your first choice.&lt;/p&gt;
&lt;p&gt;A common approach is to submit to a high profile journal, and on rejection submit to PLOS one. This is done in order to reduce the thrashing around within the peer review system. Perhaps consider submitting to PLOS one first? You could also look for a journal that is smaller, and might be more responsive. In the life sciences the journal I work for - &lt;a href=&#34;http://elife.elifesciences.org&#34;&gt;eLife&lt;/a&gt; - is both prestigious and fast.&lt;/p&gt;
&lt;p&gt;For the life sciences &lt;a href=&#34;&#34;&gt;Anna Sharman&lt;/a&gt; has a great resrouce for a selection of journals giving information about &lt;a href=&#34;http://www.sharmanedit.co.uk/resources/decision-times&#34;&gt;decision times&lt;/a&gt;, &lt;a href=&#34;http://www.sharmanedit.co.uk/resources/open-access-charges&#34;&gt;OA charges&lt;/a&gt; and &lt;a href=&#34;http://www.sharmanedit.co.uk/resources/journal-metrics-spreadsheet&#34;&gt;journal metrics&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It might be interesting to encourage people attending your courses to contribute to these, or to create similar resources for their own disciplines.&lt;/p&gt;
&lt;h2 id=&#34;preprint-servers--archives&#34;&gt;Preprint servers / archives&lt;/h2&gt;
&lt;p&gt;Your discipline may have a discipline specific archive. Make sure a copy of your work is deposited there. If the full stext is deposited in one of these venues Google Scholar will be able to provide readers with a link to a full text version of your article - even if you have had to publish in a paywalled journal.&lt;/p&gt;
&lt;p&gt;Often you can get your work in draft up there before the peer review process is complete (if that&amp;rsquo;s considered Kosher in your field). This can give you priority on an idea, even before the idea has been formally reviewed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Physical sciences, astronomy, mathematics - &lt;a href=&#34;http://arxiv.org&#34;&gt;ArXiV&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Biomedicine - &lt;a href=&#34;http://www.ncbi.nlm.nih.gov/pmc/&#34;&gt;Pub Med Central&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Social Sciences - &lt;a href=&#34;http://www.ssrn.com&#34;&gt;Social Sciences Research Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Economics - &lt;a href=&#34;http://repec.org&#34;&gt;Repec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;High energy Physics - &lt;a href=&#34;http://inspirehep.net/?ln=en&#34;&gt;inspirehep&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also, check with your university library and find out what archives they run, deposit there for the same reasons as above.&lt;/p&gt;
&lt;h2 id=&#34;the-oa-advantage&#34;&gt;The OA advantage&lt;/h2&gt;
&lt;p&gt;Keeping control of your own content is a significant advantage that authors can derive from publishing in an OA journal. I&amp;rsquo;ll touch on that a bit later.&lt;/p&gt;
&lt;p&gt;There is another advantage, and that&amp;rsquo;s the advantage of discoverability.&lt;/p&gt;
&lt;p&gt;Currently - as of writing this post, the Google main search bot does not index content that is behind an academic paywall for users who do not have access. That means if you publish at an non paywalled venue more people have a chance to find your content.&lt;/p&gt;
&lt;p&gt;Now most of your immediate peers will probably be able to access your content by virtue of having it in either the appropriate venue or in an appropriate repository, but it can&amp;rsquo;t hurt to make it even easier to find.&lt;/p&gt;
&lt;p&gt;If your coauthors will not agree to publishing in an OA venue, you can always try to modify the copyright transfer agreement that the publishing company will ask you to sign.&lt;/p&gt;
&lt;p&gt;You can follow these examples to allow you to retain the right to distribute the paper in any way that you see fit. This is the one piece of advice that I&amp;rsquo;m giving that might slow down the process of publication, but go on, you know you want to do it, don&amp;rsquo;t you?&lt;/p&gt;
&lt;h2 id=&#34;what-happens-to-my-paper-in-a-big-publishing-company-and-why-should-i-care&#34;&gt;What happens to my paper in a big publishing company, and why should I care?&lt;/h2&gt;
&lt;p&gt;During the reviewing stage a very badly formatted version of your article will be created to be sent to the reviewers of your article. If you have a preprint of your article available, that might even be an easier artefact for the reviewers to use, and it might speed up the review process, though I don&amp;rsquo;t have any evidence to suggest that it will.&lt;/p&gt;
&lt;p&gt;If your manuscript is accepted for publication then it will be sent to a large typesetting company, where it will be digitally torn apart and converted to XML. All of the formatting that you do on figures, text and on the reference lists, will be thrown away. I&amp;rsquo;ll just say that again. All of the work and hours you spend carefully formatting your reference lists will be ignored as the content goes through an automated typesetting system. (That&amp;rsquo;s why at eLife we don&amp;rsquo;t have a proscriptive requirement on the format of the references that we get sent, we will take them in any format).&lt;/p&gt;
&lt;p&gt;All of your specially chosen fonts, and special text alignment will be mostly ignored.&lt;/p&gt;
&lt;p&gt;Depending on the state of the manuscript and the quality of the language in the manuscript it may be checked by a copy editor, either for internal journal style, or for the quality of the language. Much of this work is undertaken by highly educated graduates in developing countries, particularly India, the Philippines and increasingly China - globalisation in action.&lt;/p&gt;
&lt;p&gt;Why is this? For the most part the systems that run our global publication infrastructure are old, many of them have code bases that are older than 20 years. Back in the day XML was the only reliable transfer format, and it remains the industry standard today. A slow evolution has been happening with the XML that publishers are using, and under the gentle pressure to deposit into PubMed and PubMedCentral most publishers and typesetters are starting to target one of the many dialects of the NLM DTD. This has become a de-facto standard in the industry, however no writing tools export natively to this format, and the DTD supports, and is designed for, archiving print material. One of the very many consequences of this is that code that is typeset in this DTD is usually typeset as dumb text. On the other hand it does allow a resource like PMC to archive millions of articles, from thousands of publishers, and provide a very fine grained search interface on top of all of this content. I&amp;rsquo;ll mention writing tools a little later.&lt;/p&gt;
&lt;p&gt;In order to potentially reduce the time to review your manuscript, and in order to reduce your the time your manuscript takes in the copy editing / typesetting process the following things could help:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;As mentioned -having a preprint version of your article available that the reviewer may know about, e.g. on the ArXiVe.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If English is not your first language, have the manuscript proofed by a native speaking colleague, or pay to have the proofing done.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use a tool for managing your references, and don&amp;rsquo;t sweat the formatting details. Tools you might consider using any of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.bibtex.org&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.mendeley.com&#34;&gt;mendeley&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.zotero.org&#34;&gt;zotero&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.papersapp.com/mac/&#34;&gt;papers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Remember, this is probably a lifestyle choice, my main advice is pick a tool that does not have too much lock in. I used to work at Mendeley and believe it to be as good as any tool out there.&lt;/p&gt;
&lt;p&gt;## But wait! I want to do iPython, interactive, open data, virtual machines, 3D printed DNA dinosaur replication and what you have just told me sound like like I can&amp;rsquo;t do that - that sucks :(&lt;/p&gt;
&lt;p&gt;Yes, yes, it does suck, and I hear what you are saying, but remember, at the moment of publishing, your priority is to get the damn work published, and unfortunately that still means interacting with a system that has changed little since the late 17th century. There are moves in the right direction, oaises of sanity, but there is a long long way to go.&lt;/p&gt;
&lt;p&gt;If you feel really passionate about this then the best thing you can do is to keep the rights to your own work, get the paper out as a CC-BY paper in a boring old venue, and then do the kind of publication that you really want to on your own academic home page, and build your own audience around your work that way. In that case you want the boring route to take up as little time as possible.&lt;/p&gt;
&lt;p&gt;You should also deposit artefacts of your paper in the best possible place for them. Code to a location like &lt;a href=&#34;github&#34;&gt;github&lt;/a&gt;. Videos to &lt;a href=&#34;youtube&#34;&gt;youtube&lt;/a&gt; or &lt;a href=&#34;Vimeo&#34;&gt;Vimeo&lt;/a&gt;. Images to &lt;a href=&#34;http://www.flickr.com&#34;&gt;flickr&lt;/a&gt;. Data to &lt;a href=&#34;http://figshare.com&#34;&gt;Figshare&lt;/a&gt;, &lt;a href=&#34;http://datadryad.org&#34;&gt;DataDryad&lt;/a&gt;, &lt;a href=&#34;http://zenodo.org&#34;&gt;Zenodo&lt;/a&gt;, or one of the very many other subject specific data repositories that may be appropriate for your field.&lt;/p&gt;
&lt;p&gt;Try and keep your artefacts well organised, and backed up off of your machine. You can back a lot up to github as part of a git repo, but that&amp;rsquo;s not it&amp;rsquo;s main use case. You can use a service like &lt;a href=&#34;https://www.evernote.com&#34;&gt;EverNote&lt;/a&gt;, or get a licence for a research specific asset management tool like &lt;a href=&#34;http://www.digital-science.com/products/projects&#34;&gt;Projects&lt;/a&gt; or &lt;a href=&#34;http://www.labarchives.com&#34;&gt;LabArchives&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The aim here is to reduce the friction in getting instances of these resources into the hands of others - if you believe that this is a critical part of doing research.&lt;/p&gt;
&lt;p&gt;It can also to make it possible to recover this informaiton in the instance of losing your main machine. (I decomissioned my main machine last summer via cup of coffee).&lt;/p&gt;
&lt;p&gt;For the purposes of archiving your work you should also check with your institution and library to see if they can provide support or systems. Librarians in many institutions are mustard keen to help, as it provides a way for them to prove value to the academy in a world in which library subscriptions are under extreme pressure. You may find yourself with the problem of having too many options - which is not a bad problem at all.&lt;/p&gt;
&lt;h1 id=&#34;authoring-tools-and-why-does-this-all-suck-so-much&#34;&gt;Authoring tools, and why does this all suck so much?&lt;/h1&gt;
&lt;p&gt;I noticed that there was some discussion in the thread about collaborative tools for authoring. Again, I&amp;rsquo;ll just stress, get the work published as soon as possible. This might mean sending a PDF of the article to a publishing house, or having to just send in a Word file.&lt;/p&gt;
&lt;p&gt;On the other hand, there are a new generation of online tools emerging for writing, and also tools emerging for writing on the iPhone and iPad. I think we have more viable options now at our fingertips than at any time in the past. I don&amp;rsquo;t believe that there are any serious contenders yet ready to oust the Word/LaTeX duopoly, but it would not hurt to take some of the following for a test drive to help with the authoring experience. It&amp;rsquo;s too broad a topic to go into a detailed review of each one, I&amp;rsquo;ll leave an investigation of these tools as an exercise for the interested reader. The list below is just a smaple, there are a bunch of others out there.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.writelatex.com&#34;&gt;writelatex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sharelatex.com&#34;&gt;sharelatex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://fiduswriter.org&#34;&gt;fidus wrtiter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.google.com/drive/&#34;&gt;google docs/drive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://johnmacfarlane.net/pandoc/&#34;&gt;panodc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.martinfenner.org/2013/06/17/what-is-scholarly-markdown/&#34;&gt;scholarly markdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.authorea.com&#34;&gt;authorea&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.hogbaysoftware.com/products/writeroom&#34;&gt;WriteRoom&lt;/a&gt; (iOS/Mac)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://brettterpstra.com/projects/nvalt/&#34;&gt;nvALT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://agiletortoise.com/drafts/&#34;&gt;drafts&lt;/a&gt; (iOS)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://vesperapp.co&#34;&gt;Vesper&lt;/a&gt; (iOS)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The tool that I see emerging at some time on the horizon, and that I have a lot of excitement for, is the work on the &lt;a href=&#34;http://about.substance.io&#34;&gt;substance reader and composer&lt;/a&gt; and &lt;a href=&#34;http://lens.elifesciences.org&#34;&gt;eLife lens&lt;/a&gt;. What&amp;rsquo;s really nice about this is that to get started you can import NLM XML directly, or markdown via panodoc. It does a great job of separating the view, logic and control of the writing experience, and so it should also be possible to write directly in browser, and export to a publication ready format directly - but some work remains.&lt;/p&gt;
&lt;p&gt;In my own ideal world you can submit an idea to a journal as part of a pull request to the publication, peer review takes place in some system similar to how we do code review today. On acceptance the full digital artefact is published instantly. The writing and collaboration happens in almost any tool that the user likes, modifications are synced via something like dropbox. In this world writing tools support offline, as well as online modes, and content logic and views can be assembled independantly. In my ideal world the source is open. We are a little bit away from that at the moment, but there is no doubt in my mind that we are moving in that direction. [this great post by plos] has some great insights discussing what the native format for publihsing on the web should be.&lt;/p&gt;
&lt;h1 id=&#34;about-this-post&#34;&gt;About this post.&lt;/h1&gt;
&lt;p&gt;As we are discussing publishing on the web, I thought it might be useful to describe the tools I used to write this post. The body of the text is stored on my machine as a plain text file, and I store all of these in one directory using &lt;a href=&#34;http://brettterpstra.com/projects/nvalt/&#34;&gt;nvALT&lt;/a&gt; to manage them. This directory is also held under a Dropbox account, and I can access the content from my iPhone through a variety of editors, but in this case I didn&amp;rsquo;t use any of these.&lt;/p&gt;
&lt;p&gt;For writing this post I used WriteRoom for mac in distraction free mode. I often use &lt;a href=&#34;http://www.sublimetext.com&#34;&gt;SublimeText&lt;/a&gt; in distraction free mode too. For some shortcuts in formatting I used &lt;a href=&#34;http://smilesoftware.com/TextExpander/index.html&#34;&gt;TextExpander&lt;/a&gt;. To format the links I write the post in markdown, and did the formatting in SublimeText. I previewed the post using &lt;a href=&#34;http://marked2app.com&#34;&gt;Marked&lt;/a&gt;. I also used Marked to verify that all of the links were working, at the time of writing. In order to publish the post on my blog I posted it directly into a github repo using &lt;a href=&#34;http://pages.github.com&#34;&gt;github pages&lt;/a&gt; to render the content. You can see the result at &amp;hellip; . I used the &lt;a href=&#34;http://brettterpstra.com/2013/07/04/grablinks-bookmarklet-2-dot-0/&#34;&gt;GrabLinks&lt;/a&gt; bookmarklet to gather all of the links from this post to add in as a resources list at the end of this post.&lt;/p&gt;
&lt;h1 id=&#34;final-thoughts&#34;&gt;Final thoughts&lt;/h1&gt;
&lt;p&gt;I realise that I have mostly been answering the question about what shlould people know about the world as it is now, and not so much about what tools or approahces we should advocate to make the world a better place, but I hope that we can have a clear view on what is bad, so that this can help people make pragmatic decisions about how to change things for the better.&lt;/p&gt;
&lt;p&gt;# resources&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://partiallyattended.com/2014/01/08/advice-on-publishing-online/&#34;&gt;github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.biosemantics.org/jane/&#34;&gt;Journal author name estimator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.miketaylor.org.uk/tmp/journal-finder.html&#34;&gt;Journal Finder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.edanzediting.com/journal_selector&#34;&gt;http://www.edanzediting.com/journal_selector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.journalguide.com/&#34;&gt;http://www.journalguide.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://etest.vbi.vt.edu/etblast3&#34;&gt;http://etest.vbi.vt.edu/etblast3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.eigenfactor.org/openaccess/index.php&#34;&gt;Eigenfactor Journal Rank tool&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://elife.elifesciences.org/&#34;&gt;eLife&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.sharmanedit.co.uk/resources/decision-times&#34;&gt;decision times&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.sharmanedit.co.uk/resources/open-access-charges&#34;&gt;OA charges&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.sharmanedit.co.uk/resources/journal-metrics-spreadsheet&#34;&gt;journal metrics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://arxiv.org/&#34;&gt;ArXiV&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ncbi.nlm.nih.gov/pmc/&#34;&gt;Pub Med Central&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.ssrn.com/&#34;&gt;Social Sciences Research Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://repec.org/&#34;&gt;Repec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://inspirehep.net/?ln=en&#34;&gt;inspirehep&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.bibtex.org/&#34;&gt;bibtex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.mendeley.com/&#34;&gt;mendeley&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.zotero.org/&#34;&gt;zotero&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.papersapp.com/mac/&#34;&gt;papers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://partiallyattended.com/2014/01/08/advice-on-publishing-online/github&#34;&gt;github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://partiallyattended.com/2014/01/08/advice-on-publishing-online/youtube&#34;&gt;youtube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://partiallyattended.com/2014/01/08/advice-on-publishing-online/Vimeo&#34;&gt;Vimeo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.flickr.com/&#34;&gt;flickr&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://figshare.com/&#34;&gt;Figshare&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://datadryad.org/&#34;&gt;DataDryad&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://zenodo.org/&#34;&gt;Zenodo&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.evernote.com/&#34;&gt;EverNote&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.digital-science.com/products/projects&#34;&gt;Projects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.labarchives.com/&#34;&gt;LabArchives&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.writelatex.com/&#34;&gt;writelatex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.sharelatex.com/&#34;&gt;sharelatex&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://fiduswriter.org/&#34;&gt;fidus wrtiter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.google.com/drive/&#34;&gt;google docs/drive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://johnmacfarlane.net/pandoc/&#34;&gt;panodc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.martinfenner.org/2013/06/17/what-is-scholarly-markdown/&#34;&gt;scholarly markdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.authorea.com/&#34;&gt;authorea&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.hogbaysoftware.com/products/writeroom&#34;&gt;WriteRoom&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://brettterpstra.com/projects/nvalt/&#34;&gt;nvALT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://agiletortoise.com/drafts/&#34;&gt;drafts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://vesperapp.co/&#34;&gt;Vesper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://about.substance.io/&#34;&gt;substance reader and composer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://lens.elifesciences.org/&#34;&gt;eLife lens&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.sublimetext.com/&#34;&gt;SublimeText&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://smilesoftware.com/TextExpander/index.html&#34;&gt;TextExpander&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://marked2app.com/&#34;&gt;Marked&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://pages.github.com/&#34;&gt;github pages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://brettterpstra.com/2013/07/04/grablinks-bookmarklet-2-dot-0/&#34;&gt;GrabLinks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>STM innovations seminar, London, 2013</title>
      <link>http://scholarly-comms-product-blog.com/2013/12/04/stm-innovations-2013/</link>
      <pubDate>Wed, 04 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2013/12/04/stm-innovations-2013/</guid>
      <description>&lt;p&gt;Today I&amp;rsquo;m at the STM innovations seminar. The twitter tag for today is &lt;a href=&#34;https://twitter.com/search?q=%23ukinno&amp;amp;src=typd&#34;&gt;#ukinno&lt;/a&gt;. The program is &lt;a href=&#34;http://www.stm-assoc.org/events/innovations-seminar-2013/&#34;&gt;online&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m going to take a light approach to blogging today, I&amp;rsquo;ll probably hang out mostly on Twitter.&lt;/p&gt;
&lt;p&gt;## 9.35 The Research Data Revolution, Sayeed Choudhury, Associate Dean for Research Data Management,
Johns Hopkins University &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Data has become a major topic of interest from all sectors of society with headlines such as “Data is the new oil” to assertions from McKinsey that data is the fourth factor of production. Within higher education, new forms of data intensive scholarship have already begun to transform research and learning. The “Research Data Revolution” will examine the implications of these developments for libraries and publishers especially as they relate to new forms of competition.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Sayeed is talking about data in general, he is associated with &lt;a href=&#34;http://dataconservancy.org&#34;&gt;http://dataconservancy.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Some key points from his talk.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;data is the new Oil&lt;/li&gt;
&lt;li&gt;data has to be made open and actionable by machines&lt;/li&gt;
&lt;li&gt;from the libraries perspective, data are a new form of collections&lt;/li&gt;
&lt;li&gt;treat manuscripts like a dataset, even humanists are starting to see new possabilites and new methods, creating a completely new experience online.&lt;/li&gt;
&lt;li&gt;as librarians, think about the realtionship you can have with your communities, if they start to manage and curate the data, this leads to an opportunity for creating new relationships built around those special collections.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nice take on &lt;code&gt;Big Data&lt;/code&gt;. If a community is overwhelme by it&amp;rsquo;s data, then the data becomes big. For the Astronomers today the SDSS is no longer big - 150TB, as the astronomers now have tools to deal with this data. New proposed experiments are creating data that will overwhelm the field, lead to a situation in which most of the data will have to be thrown away on first run.&lt;/p&gt;
&lt;p&gt;He makes the point of needing to know the provenonce of data, how data moves from one fromat to another. I&amp;rsquo;m a big fan now of making this available in code, via automated tool chains such as &lt;a href=&#34;http://www.opscode.com/chef/&#34;&gt;chef&lt;/a&gt; and &lt;a href=&#34;http://www.vagrantup.com&#34;&gt;vagrant&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If data collections become open, no one is going to care about how much data you have, in contrast to the services that you provide. Some services may be hard to run across the network, but generally libraries and publishers might differentiate themselves via the servcies they offer, rather than as acting as gatekeepers to the content. We need to get serious about machine based services to the content. There will be some things that only humans can do.&lt;/p&gt;
&lt;p&gt;Mentions a seminal report on the dynamics of infrastrucure developement. (you can find a copy online &lt;a href=&#34;http://deepblue.lib.umich.edu/bitstream/handle/2027.42/49353/UnderstandingInfrastructure2007.pdf?sequence=3&#34;&gt;here&lt;/a&gt; - open access FTW!). The conclusion from the reoprt is that we will need new forms of infrastrucutre. Que story about stupid machines and reccomendation engines going wrong, but the machine corrected itself quickly.&lt;/p&gt;
&lt;p&gt;Google and Amazon have figured out that at scale even dumb machines can do well. That is why scale is important. It&amp;rsquo;s a means, but not an end. At Hopkins they have one large astronomy dataset, but there are many other astronomy data sets out there. They have to think of services across all of those data sets, not just on one data set. The scale will allow machines to appear to be clever, and this is where the role of the expert will remain important.&lt;/p&gt;
&lt;p&gt;The discussion moves on to curation of data, and who is doing a good job. He mentions &lt;a href=&#34;http://schema.org&#34;&gt;http://schema.org&lt;/a&gt;, mentions that top 5 places that do data management are Apple, Facebook, Twitter, Google and Amazon. Big question, are these pople doing the creation of these knowledge graphs with a view to preservation. The answer is proabaly not. The informaiton graphs that are coming out of the librarian community - OAE-ORE - are richer, smaller, are not incompatable with Schema.org, but at the moment they have no skin the the game, they have to offer something. (I didn&amp;rsquo;t really get the point here).&lt;/p&gt;
&lt;blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;Publising is about content and not format&lt;/p&gt;
&lt;p&gt;Question time, but I&amp;rsquo;m probably not going to capture the Q&amp;amp;A session.&lt;/p&gt;
&lt;p&gt;## 11.00 Morning plenary: ePublishing evolutions
Moderated by: Dave Martinsen, ACS
 
## Watson and the Journey to Cognitive Computing Frank Stein, IBM&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Today, many of our science, technical, and medical professionals can not keep up with the growing amount of new information being produced in the world.   The complication is that although we have ever faster computers, they haven’t been designed to process and understand this information which is mostly in human-consumable formats. This means we’re not getting the full value out of our scientific, medical, and technical endeavors.  This talk will provide a brief overview of Watson as a cognitive computing exemplar, and of Cognitive Computing more generally. We will discuss some of the implications of this new technology, providing a basis to envision changes that will start impacting our world and to understand some of the benefits and challenges that cognitive computers will bring to the scientists and professionals in the fields that you support.
 
He is making the Watson algorithm seem comprehensible. I&amp;rsquo;m sure there is a lot of work behind this system. I like that the Watson system uses inference, and they use probablistic reasoning.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;They have over 100 algorithms to do the matching between input and the underlying knowledge corpus.&lt;/p&gt;
&lt;p&gt;Semaintic technlogy was helpful, but not the be all, and end all, they used a whole bunch of other techniques in order to win at Jeapordy.&lt;/p&gt;
&lt;p&gt;How do they start appying the Watson technology to the needs of their customers? They have a new divsion of the company to just figure out how to commmercialise the tech. They started with appliations in Medicine, have moved to finance, and they are looking at networks of industries now that this could be applied in.&lt;/p&gt;
&lt;p&gt;(Of course a great question is how could we imagine brining this technology into the publishing domain).&lt;/p&gt;
&lt;p&gt;They are moving forward with productising Watson, however they have a problem. For them the next phase they are looking at cognitive computing. The Watson brain used about 85 - 100KW of power. If they are going to get bigger more sophisticated systems, they would need to spend more power.&lt;/p&gt;
&lt;p&gt;There are a bunch of techologies that need to be enabled, but they boil down to doming more networked computing on smaller faster less power hungry chips, with better learning algorithms sitting on top of this.&lt;/p&gt;
&lt;p&gt;They want natural language interactions to these systems, and they want to move away from the von Neumann architectures, these future architectures will break down the barrieres between memory and computaton, on the chip. They are prototyping chips - such as the SyNAPSE chip. Does not use a clock, it&amp;rsquo;s all event based on the chip. This needs an entirely new stack of programming tools, from compliers - all the way up. More information on these chips &lt;a href=&#34;http://www.research.ibm.com/cognitive-computing/neurosynaptic-chips.shtml#fbid=OH8oQM4QCP0&#34;&gt;here&lt;/a&gt;. The goal for SyNAPSE is to&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;IBM’s long-term goal is to build a neurosynaptic chip system with ten billion neurons and hundred trillion synapses, all while consuming only one kilowatt of power and occupying less than two liters of volume.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Basically, they want to build a brain on a chip - good luck to them!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;source-data-and-other-article-enrichments-thomas-lembergerembo-journals&#34;&gt;Source Data and other article enrichments Thomas Lemberger, EMBO Journals&lt;/h2&gt;
&lt;blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;The aim of the EMBO SourceData project is to build tools that will allow journals to integrate data and structured biological metadata in published papers and to develop data-oriented methods to search the literature. 
 
I&amp;rsquo;ve reported on what Thomas talks about elsewhere before. I&amp;rsquo;ll really focus my notes here on any specific, interesting and new thoughts that come up in the course of this talk.&lt;/p&gt;
&lt;p&gt;Makes the good point that the majority of journals in the life science are data journals as they all have data in them. Small scale data, but data nonetheless.&lt;/p&gt;
&lt;p&gt;They look for Actionable data, this does not have to be the underlying raw data. An example is the graph derviced from high throughput screening. The underlying data is several TB in size. They allow the community to decide what the most actionable, and processed version of the data are, that should be presented with the paper.&lt;/p&gt;
&lt;p&gt;For tagging the metadtaa in their papers they have three levels of metadata:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;level 1 - lists of chemcial and biological components&lt;/li&gt;
&lt;li&gt;level 2- representation of the causality of the experiment, e.g. &amp;ldquo;measurement of Y as a function of A using method C&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;level 3 - machine readable representation with standard identifiers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They are generating tools to help with this curation. &amp;lsquo;Data copy editors&amp;rsquo;. This is very interesting, and it would be great to understand their toolchain. Hopefully I&amp;rsquo;ll get a chance to see it more closely at the workshop that we are organising next week on data and literature integration.&lt;/p&gt;
&lt;p&gt;Now using Filemaker to demo how a structured query based on tagged entities could do for simplifying discovery. I suspect that the next speaker will have something to say about this topic too.&lt;/p&gt;
&lt;h2 id=&#34;actionable-data---the-wolfram-approach-matthew-daywolfram-research&#34;&gt;Actionable Data - the Wolfram Approach Matthew Day, Wolfram Research&lt;/h2&gt;
&lt;p&gt; &amp;gt;
Wolfram Research has created several actionable data technologies, particularly WolframAlpha.com and the Computable Document Format. The new focus is on enhancements that will greatly extend our ability to make all sorts data files readily actionable with minimal work from data producers or curators.&lt;/p&gt;
&lt;p&gt;(In terms of the context of Matt&amp;rsquo;s talk, it might be interesting to point to the recent blog post by Stephen Wolfram on &lt;a href=&#34;http://blog.stephenwolfram.com/2013/11/something-very-big-is-coming-our-most-important-technology-project-yet/&#34;&gt;the promise of the Wolfram ecosystem&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;They want to be able to create an engine that can provide answers, where those answers don&amp;rsquo;t exist yet, so an engine like google can&amp;rsquo;t currently help.&lt;/p&gt;
&lt;p&gt;Not taking so many notes, getting a little nervous about my impending flash presentation.&lt;/p&gt;
&lt;h2 id=&#34;14-00-special-introduction-on-science-20-and-current-developments-in-the-eu-prof-jean-claude-burgelman-eu-commission-dg-research-and-innovation-head-of-unit&#34;&gt;14 00 Special Introduction on Science 2.0 and current developments in the EU, Prof Jean-Claude Burgelman, EU Commission, DG Research and Innovation, Head of Unit&lt;/h2&gt;
&lt;p&gt;Bottom line, most stakeholders support open science. Different aspects of open science, and science2.0, are moving at different paces. The directorate is producing a green paper on the topic. This will focus on what actions, or not, the EC will take.&lt;/p&gt;
&lt;p&gt;## 14 15 Afternoon Keynote: e-Science and we-Science: Making citizen science work Moderated by: Dave Smith (IET)  ## Lessons from the Zooniverse : Science with (nearly) a million collaborators Chris Lintott, Researcher, Oxford Astrophysics and Principal Investigator, Zooniverse&lt;/p&gt;
&lt;blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;The Zooniverse is the world’s most successful platform for citizen science - the involvement of non-professionals in the scientific enterprise. Zooniverse founder Chris Lintott will cover the highlights of six years of collaboration, including mysterious galaxy-sized gas clouds, unusual planets and a journey across the Serengeti, and explain the lessons for researchers and publishers alike in bringing science to such a large audience.&lt;/p&gt;
&lt;p&gt;This is an amazing talk. I&amp;rsquo;m not even going to try to take notes, I&amp;rsquo;m enjoying the talk waaaaay too much.&lt;/p&gt;
&lt;p&gt;There is an interesting issue raised at the end of the talk. Zooniverse still cares about scientific publications, but they see little interaction between the community who do the contribution, and the research publications. There is a strong desire from Chris to do something about this, but what to do is not yet aswered.&lt;/p&gt;
&lt;p&gt;15 00 Afternoon Plenary: e-Science, e-Research, e-Publishing
 
## e-Research and the demise of the scholarly article. David De Roure, Director of e-Research Centre, Oxford&lt;/p&gt;
&lt;p&gt;Dave is going to focus on the intersection between large numbers of people and large computaional capacity. (A great example is that facebook is genering 100s of terabytes of data per year).&lt;/p&gt;
&lt;p&gt;A nice story on how Dave came up with the 8 things that will have led to the demise of the paper.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;A pdf exploded today when a scientist tried to paste in the twitter firehose&amp;rdquo;&lt;/p&gt;
&lt;p&gt;## 16 00 Afternoon Plenary: e-Science, e-Research, e-Publishing Moderated by: Howard Ratner, CHORUS   ## Text and Data Mining, discovering new patterns Nicko Goncharoff, Digital Science&lt;/p&gt;
&lt;p&gt;Very interesting. SureChem is going into the public domain. Will find out more during the course of this talk. Bottom line, EMBL is taking over.&lt;/p&gt;
&lt;p&gt;(This is obviously the first entity that Digital Science is divesting. It&amp;rsquo;s important that Digital Science do this if their model is to succeed in the long term, it&amp;rsquo;s intersting to see this starting to happen. It will be interesting to see if this trend continues).&lt;/p&gt;
&lt;p&gt;  
## Wearable Computers: Future Fix-All or Fashion Faux Pas? Heather Ruland Staines, SIPX    &lt;/p&gt;
&lt;blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;From Google Glass to the Samsung Gear, technology blogs and expos are all over wearable computing. Whether as extensions to our phones or as sensors to monitor our health or purchasing behavior, wearables are coming. Learn about how wearable computers are being tested in education and teaching contexts, what the latest initiatives are, and hear from a Glass Explorer about the potential and the perils of a computer on your face.&lt;/p&gt;
&lt;p&gt;She is wearing a google glass. I really like the idea of diminished reality, in which a google glass like device could block out aspects of reality that you don&amp;rsquo;t want to interact with - like advertising (pverty and misery perhaps?).&lt;/p&gt;
&lt;p&gt;(Getting one of these things into a contact lens would really really annoy my wife, so I applaud the rest of humanities march towards the cyborg singularity, and I shall be watching quietly from the sidelines.)&lt;/p&gt;
&lt;p&gt;Final word on ethics of using the google glass&lt;/p&gt;
&lt;blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you are going to be a jerk, you&amp;rsquo;re going to be a jerk.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>STM brainstorming session - 2013</title>
      <link>http://scholarly-comms-product-blog.com/2013/12/03/stm-brainstorming-meeting/</link>
      <pubDate>Tue, 03 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2013/12/03/stm-brainstorming-meeting/</guid>
      <description>&lt;p&gt;Just attended the STM brainstroming session. I&amp;rsquo;ll update these notes in due course, and fix spelling issues, but I wanted to get the post live first.&lt;/p&gt;
&lt;h1 id=&#34;notes-ill-just-mention-the-things-that-i-found-interesting&#34;&gt;Notes I&amp;rsquo;ll just mention the things that I found interesting.&lt;/h1&gt;
&lt;p&gt;## Round1&lt;/p&gt;
&lt;p&gt;Science Gists get a mention, yay!!&lt;/p&gt;
&lt;p&gt;Google scholar library gets a mention.&lt;/p&gt;
&lt;p&gt;Visualising data as maps is mentioned, mentions that there are no standards&lt;/p&gt;
&lt;p&gt;Howard mentions much richer tagging in the article, and upfront semantic tagging. Someone mentions PeerJ as a submissions system that can do this.&lt;/p&gt;
&lt;p&gt;New payment methods.&lt;/p&gt;
&lt;p&gt;A growing trend in peer review experiments. John Sack mentions prescore, a service that evaluates the people participating in the peer review process, and gives those people an impact rating.&lt;/p&gt;
&lt;p&gt;Of course research data is getting a bunch of mentions.&lt;/p&gt;
&lt;p&gt;Detection of scientific fraud needs to be dealt with. This is related to reproducability but not always. Fraud happens.&lt;/p&gt;
&lt;p&gt;The importance of communicating science through images and graphics, not visualisation, rather explaining the contents of an article through an image, sometimes called graphical abstracts. Readers start reading articles through the images, and writers start writing through the images, the role of a publisher could be to create image creation and curation services.&lt;/p&gt;
&lt;p&gt;The notion of virtual laboratories is meniotned, sucah a chemconnective out of carnegie mellon. This is going to require different types of content than we are used to.&lt;/p&gt;
&lt;p&gt;A machine processable format to enable textmining, perhaps a format that all publishers can make avilalbel, normalise all the formats. Pharma have been asking for this too.&lt;/p&gt;
&lt;p&gt;Smart articles are mentioned, the article that finds you, and not you finding the articles. (this matches my point #2).&lt;/p&gt;
&lt;h1 id=&#34;round-2&#34;&gt;Round 2&lt;/h1&gt;
&lt;p&gt;Following on from standards for text mining - standard formats for research data (interestingly I&amp;rsquo;ll be at a meeting about this next week). This also touches on reproducabilty. Crossref guy mentions virtual machine enviornment issue.&lt;/p&gt;
&lt;p&gt;The cloud is mentioned, for running all of the computers, the hosting, will run on several clouds.&lt;/p&gt;
&lt;p&gt;I mention the eclipse of the authoring tool.&lt;/p&gt;
&lt;p&gt;eBooks is mentioned. The development of the good display of ebooks using ePub3, it might not happen, but it might be good to have happen.&lt;/p&gt;
&lt;p&gt;The importance of google and google scholar, and the control that they drive over the sites. We just jump and do whatever they say. ACS is not alone in that upwards of 40-50% of usage can originate from google. (I do mention that OA can help).&lt;/p&gt;
&lt;p&gt;Unoticable search, like google now.&lt;/p&gt;
&lt;p&gt;The notiion of text mining and digital humanities. In connection with this the issue of data privacy is mentioned.&lt;/p&gt;
&lt;p&gt;Metadata, this is the future. Tim O&amp;rsquo;Reilly has mentioned the notion of digital smog. Project Mesur from Johan Bollen is mentioned. Watching the behaviour of scholars via DOI resolvers, and finding where users are at the intersection of new and emerging fields of study. There is someting very interesing there.&lt;/p&gt;
&lt;p&gt;Licencing at the component level, below the traditional level of the article, this is realted to the metadata issue.&lt;/p&gt;
&lt;p&gt;Standards for supplimentary data for articles (this is sounds like standard formats for research data), though one might say is that&lt;/p&gt;
&lt;p&gt;Crossref mentions hosted journals as a platform. I mention scholastica, based out of chicago.&lt;/p&gt;
&lt;p&gt;Jeremey mentions services for individuals. (Eefke mentions that it could be related to predictive analytics, Jeremey mentions that the important thing is thinking about turing this into services for individuals).&lt;/p&gt;
&lt;p&gt;The work around altmetrics is mentioned as something that has really exploded. Beckham medial library have mentioned that they are collecting 350 measures of impact. Mentioned that last year . Crossref is taking on the PLOS ALM tool, and providing that as a service for publishers. They have started to look at metrics that PLOS have. They have log files from the dx.doi resolution log files. They are considering whether they can make that data avilalbe.&lt;/p&gt;
&lt;p&gt;Machine readable article reuse rights is mentioned by Howard. Says that CC is fine, but there are a lot of publishers who don&amp;rsquo;t want to use those license. Eefke mentions the NISO groups.&lt;/p&gt;
&lt;p&gt;John Sack mentions the effects of dealing with industrial espionage. He says that their servers are getting hit hard by usage patterns that look like those it&amp;rsquo;s coming from china. This is counter complient usage, you can&amp;rsquo;t just filter it out, it&amp;rsquo;s the.&lt;/p&gt;
&lt;p&gt;Someone mentioned that someone tried to log in to their peer review system 10k times within the same hour from the same IP address.&lt;/p&gt;
&lt;p&gt;The infrastrucutre and business model coming from funder mandates.&lt;/p&gt;
&lt;p&gt;Sharing sutes, how can publishers work with the usage on sharing sites. (From Elsevier). Want to make it a win win situation, do not want to obstruct the scientific collaboration. (really intersging).&lt;/p&gt;
&lt;p&gt;Richard from OUP - an increasing value in privacy and secrecy in a world that is open.&lt;/p&gt;
&lt;p&gt;Wearable Computers, content creation and content comsumption on these devices, example geo tagging. Touches on privacy issue.&lt;/p&gt;
&lt;p&gt;The last word, Socail networkding for scientists, and whether their needs are filled by linkedin or google plus, or whether they should build those sites.&lt;/p&gt;
&lt;p&gt;IBM guy comments. Many of these ideas impact on cognitive computing. Mentions that the doctors don&amp;rsquo;t have the time to read all of the articles. The machines are going to read these articles for the doctor, gets into the question of computers that can also interpret images, like X-Rays. Now they are starting to get into rich media. Now everythin is being recoreded on a you tube challen (each executive seems to have their own you tube channell). Can Watson interpret the grahps, the spreadsheets. There is the whole questions. Then there is the other aspect that these computers can theoretically come up with new knowledge. How does that get disseminated. Can the computer publihs to the world that it is creating. (what would it look like for the computer to apply for grant funding).&lt;/p&gt;
&lt;p&gt;John mentioned the research that happened at Stanford that showed that AI can generate hypothesis. The collossus experiment is mentioned.&lt;/p&gt;
&lt;p&gt;# Some general discussion&lt;/p&gt;
&lt;p&gt;A question come up on what are the metrics that will come out from the text and data mining world, what will be success? (I suggest that we indeed don&amp;rsquo;t have the answer to that question).&lt;/p&gt;
&lt;p&gt;# Now we are going to group these themes into common themes.&lt;/p&gt;
&lt;p&gt;We are looking for people to standup and talk about what they think the main themses are. Here are some:&lt;/p&gt;
&lt;h3 id=&#34;machine-to-machine-and-machine-enabled&#34;&gt;Machine to Machine, and machine enabled.&lt;/h3&gt;
&lt;p&gt;Many items came up in this, from machine readable data, to the general issue of standards. Computational lingusitsics as an enabling technology.&lt;/p&gt;
&lt;h3 id=&#34;new-authoring-mechanisims&#34;&gt;New authoring mechanisims&lt;/h3&gt;
&lt;p&gt;From new authoring tools, to experiential data, and this leads in to big data. There is some discussion that ALMs and enrichment might fall under this category, but enrichement and metadtaa might be &amp;hellip;&lt;/p&gt;
&lt;h3 id=&#34;enrichment-and-metadta-is-its-own-topic&#34;&gt;Enrichment and metadta is it&amp;rsquo;s own topic.&lt;/h3&gt;
&lt;p&gt;A better way to describe this is artefact enrichment.&lt;/p&gt;
&lt;h3 id=&#34;helping-the-human&#34;&gt;Helping the human&lt;/h3&gt;
&lt;p&gt;Helping the human at both ends, submissions systems that coulld help get the content in, the peer review process, the publication process, these are all pain points for authors and readers. Can this publisher to human interaction be made more efficient.&lt;/p&gt;
&lt;h3 id=&#34;b3b-vs-b2c-is-an-interesting-tension-the-author-as-the-new-customer-and-where-is-the-librarian&#34;&gt;B3B vs B2C is an interesting tension, the author as the new customer, and where is the librarian.&lt;/h3&gt;
&lt;p&gt;The individual is becoming more imporant (I mention APC and the rise of the social media as something that can make these complaints louder). Looking after the author in the way we have to gear up to is a different way to thinking about how to manage the business. Someone mentions the cost of geraing up to this is a hidden cost.&lt;/p&gt;
&lt;p&gt;## Howard asks about media&lt;/p&gt;
&lt;p&gt;Is this an area that we just do? Has it become rote? I suggest that it has.&lt;/p&gt;
&lt;p&gt;## Headaches&lt;/p&gt;
&lt;p&gt;I mention that 150 GB is a headache for me. Someone mentions conversions of small chunks of traditional written english or spanish content, and translating it to a local audio or video file that can be disseminated via smarphones, epscially in Afriac, across langages, in BRIC countries&lt;/p&gt;
&lt;p&gt;Someone mentions that the google translate widget was put on their platform. The usage is phenomenal - that&amp;rsquo;s really interesing to me. John Sack mentioned that the editor;s of the journals complained a few years ago as the quality was so bad. It was put on places like the aims and scope, and it has helped drive submissions. It is also used on the abstracts. (could one do a tranlstion of the gist of the aritlce, like the digest). It was not marketed as pure translation. IBM for the first time passed the point where they have more employees in India than in the US, and soon this will also be true for China. Nairobi is the latest place where IBM have a research lab - you go where the customers are, you go where the world is. If you did that 5 years ago, you might have thought that South Africa would have been the place to be.&lt;/p&gt;
&lt;p&gt;Comment on where is the growth in submissions are coming from - obviously Asia,&lt;/p&gt;
&lt;p&gt;## Google&lt;/p&gt;
&lt;p&gt;Eefke mentioned that this is the first time for a while that Google has been mentioned. Howard mentions that google is changing. Knowledge graph is mentioned. The death of the UI is mentioned. Is google the Publisher UI now? It&amp;rsquo;s mentione dthat the UI can now speak to you, or drop a&lt;/p&gt;
&lt;p&gt;## The Steam gamings platforms is mentioned.&lt;/p&gt;
&lt;p&gt;This allows games to be allowed to compete on a common platform.&lt;/p&gt;
&lt;p&gt;## Google Scholar&lt;/p&gt;
&lt;p&gt;What will happen to google schoalr. The sentiment is taht this product is going to be dead in the water.&lt;/p&gt;
&lt;p&gt;Hmmm, my notes become a bit distracted at this point. There is a conversation about the death of the UI, being replaed by google, hooking into siri, into google now, selling that as a service.&lt;/p&gt;
&lt;p&gt;IBM person mentions the development of an ecosystem, they have just opned a Watson developers platform that helps move towards an ecosystem.&lt;/p&gt;
&lt;p&gt;THere is a discussion of APIs. I&amp;rsquo;m going to stop talking now, I&amp;rsquo;ve talked a little bit too much today. There is the mention of STM being a place where these kinds of standards can emerge.&lt;/p&gt;
&lt;p&gt;Last year the hybrid reading experience was a trend last year. John mentions eLife lens as an experiment in the reading experience. The Elsevier person mentions that the reading experience is changing because of the rise of the PDF. He thinks that the tablet might kill the PDF. Howard things that the PDF has had a resurgance because the the PDF.&lt;/p&gt;
&lt;p&gt;Another trend from last year was the movement from the institution to the individual, and this year we are talking about the author. Someone raises the question of what do you get for your money if you are an author. If you were to sign up to Kudos, for example, who is going to pay for that - it&amp;rsquo;s said that publishers are currently paying for that, would funders pay for that in the future. The transactional stuff of how you watch money flow is difficult. For a hybird journal this is really complicated. There is also a thing about the various pots of money.&lt;/p&gt;
&lt;p&gt;Another trend from last year OA is still a hot topic. We are beginning to understand it better. Is there a place in the world for a common payment system for paying for OA. There was a hope that OAK could have done something, but it didn&amp;rsquo;t happen. The Ebsco&amp;rsquo;s and SWETS are possibly looking to move into that market. It might be the university that decides what system is going to be used. One mechanisim would be institutional payment accounts. This also comes back to the comment about authoring tools.&lt;/p&gt;
&lt;p&gt;I was unable to not talk any more, and I mentioned my idea around vendors havine a bigger capacity to do digital product development. The rise of more startups is also mentioned.&lt;/p&gt;
&lt;p&gt;(As a comment, looking over the discussion to date, it&amp;rsquo;s hard to view a few key actionables that will actually effect the industry. What do we d?).&lt;/p&gt;
&lt;p&gt;John Sack mentions that a lot of what we have up here is evolution from last year. One trend that John sees is that there are more startups. They are driving the conversation, they might not be making money, but they are driving the convesation. Open access is also driving this.&lt;/p&gt;
&lt;p&gt;There is a lot of change that has to come in education, and that is drving money into the system too. Talking about education, have there been discussions around the differnece in the role of scholarly material in education, rather than in the research context, for example K12, Professor who is teaching. There is huge growth in the budgets in education in Inida and China.&lt;/p&gt;
&lt;p&gt;Research data is mentioned again. We might think of the new ego-system, rather than an eco-system, in terms of our relationships with our authors. There is also the data citation component.&lt;/p&gt;
&lt;p&gt;We saw a bunch of startups around citation managmend, now about the authoring proces, and increasingly about the managment of the research data component. There is also the open data pressure from funding agencies. There is a growing need to handle data. Libraries are morphing into the experts on how to do grants, how to manage data, how to support the research process, and how to support researchers.&lt;/p&gt;
&lt;p&gt;# We are moving to the final summary.&lt;/p&gt;
&lt;p&gt;We are that STM ecosystem. As an industry trend can see us collaborating more for the common good. The issue is raised, why can&amp;rsquo;t we standardise around submission systems?&lt;/p&gt;
&lt;p&gt;My one thought is that some of these issues are easier to deal with if you are an OA publisher (As Hpward says, we still have not solved the Machine to Macine MD issue - and I agree with that)&lt;/p&gt;
&lt;p&gt;Someone mentions that most of the revenue that comes from institutional subscriptions, the &amp;ldquo;transition&amp;rdquo; is going to be a multiyear tool.&lt;/p&gt;
&lt;p&gt;What are the trends and standards that are either dead now, or what are our predctions that have not matured. It would make for an intersting article or white paper. Eefke says that many of the startups that were mentioned in previous years have now been aqcuired, or are dead.&lt;/p&gt;
&lt;p&gt;How do you do innovation within the establishment. Startups have it easy. For larger publhsers, you always have to make some kind of jsutification to a set of people to whom your idea can never succeed.&lt;/p&gt;
&lt;p&gt;The issue of where data sharing used to be unknown in schoalry research. Holding on to your data is becoming no longer a culturally viable option (I think we are still behind on this, in that data sharing remains the exeption rather than the rule).&lt;/p&gt;
&lt;p&gt;Archives and the grey literaure, and content being mopped up from these archives, is an interesting trends. The feeling is that for publishers to stuff this content into dark archives will no longer cut it. We run the risk of getting excited by exiting stuff, but we have to think about this. There is some disagreement on this point.&lt;/p&gt;
&lt;p&gt;The increase in the amount of semantic information that is becoming avialabe is intersing. The growth of DOIs, it&amp;rsquo;s ending up in organisation that is not siloed, like crossref and ORCID. It is gong to be interesting to see waht else comes out from this. John mentions that it&amp;rsquo;s interesting that corssref becomming a hub is interesting.&lt;/p&gt;
&lt;p&gt;Jeremey says that we have a lot to learn, it&amp;rsquo;s early days, where these trends will take us, we don&amp;rsquo;t know. For publishers who are in transition, it is going to be a very interesing project.&lt;/p&gt;
&lt;p&gt;Standards are mentioned. 70 million articles having been managed at Portico shows that there are wildly different levels of quality, even within a single publisher. It&amp;rsquo;s exiting, but there is a lot of work.&lt;/p&gt;
&lt;p&gt;The transition to multiple devices is exiting, some preparatory work has happned, but there is a lot of work left. We can never more as fast as we would like.&lt;/p&gt;
&lt;p&gt;Howard says that the validation of all of this metadata is really important. There is a difference between good enought for a human and good enough for a machine.&lt;/p&gt;
&lt;p&gt;John mentions that social media has not been mentioned that much. Sharing tools were mentioned from the Elsevier guy. He says that it&amp;rsquo;s not a part of most platforms, but it&amp;rsquo;s a part of the life of most users. Someone mentions that the ussage they get from traffic come from social media has been really good. It&amp;rsquo;s driving full text downloads. You can do marketing campegins on top of this.&lt;/p&gt;
&lt;p&gt;The biggest oppportunities for catastopic disintermediation are right at the beginning and right at the end. At one end google, at the other end authoring tools.&lt;/p&gt;
&lt;p&gt;Collective responsability from us as publishers to do fraud detection is really important. We have to go after the abuse of science. He does not know how to handle that. As an industry wide group we need to go after that. For example when malicously incorrect articles are being sent. Pulling out missing clinical trials has been mentiond. Can you do analysis on anamolous looking data. There could be a question that could be takeld as an industry standard, like doing automatic checking of figures. It is mentioned that this is difficult, that COPE is a good way to tackle this after the fact, but it is reiterated that we want to capture these instances after the fact.&lt;/p&gt;
&lt;p&gt;What happens with societeis, OA is going to affect societies more in the next few years.&lt;/p&gt;
&lt;p&gt;The opportunity for linked data is mentinoed. Could this be used as a fraud detection tool, such as &amp;ldquo;this particular author does not seem to have enough connections to have done this reseracher&amp;rdquo;. An example is mentioned that a faculty discovered that their math faculty&amp;rsquo;s strengths were not what they thoguht they were.&lt;/p&gt;
&lt;p&gt;We get back to the chair, and his final comments. The key phrases he sees are &amp;ldquo;moving from B2B to B2C&amp;rdquo; and the label of moving to an &amp;ldquo;ego-driven system&amp;rdquo;. Many of the things we discussed last year we discussed today. One thing we didn&amp;rsquo;t really talk about this were Mendeley Readcube. We didn&amp;rsquo;t have a conversation about these sites this year - why is that. Do we see them as becoming part of our own platforms, are they still a threat? Perhaps the authoring mechanisims is an areas. It&amp;rsquo;s mentioned that these are more people to collaborate wiht, and to work with. If they can build technology that a publisher can use, then that&amp;rsquo;s great. Are they community driven? Does one of them appeal to specific communities. It&amp;rsquo;s mentioned that endnote also belongs in that space. And that concludes this afternoon.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PLOS ALM 13 day 2</title>
      <link>http://scholarly-comms-product-blog.com/2013/10/17/plos-alm-13-day2/</link>
      <pubDate>Thu, 17 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2013/10/17/plos-alm-13-day2/</guid>
      <description>&lt;h1 id=&#34;connecting-alm-and-literature&#34;&gt;Connecting ALM and Literature&lt;/h1&gt;
&lt;p&gt;As I took part in the first session I don&amp;rsquo;t have many notes from it. I&amp;rsquo;ve posted the &lt;a href=&#34;https://speakerdeck.com/ianmulvany/connecting-data-and-literature&#34;&gt;slides from my talk&lt;/a&gt;, and I&amp;rsquo;ll write up some more on those in due course. For me the standout talk of the session, if not the entire meeting, was from &lt;a href=&#34;%5Bssrn%5D:&#34;&gt;Jevin West&lt;/a&gt; who talked about using networked ranked data to provide recommendations. The algorithms his group are working on are being tested on [SSRN][ssrn], and will be rolled out to PLOS. The first set of tests indicated that recommendations driven by this algorithm are out-preforming a random recommendation, and out-preforming a collaborative-filter based approach. From discussion the previous day it looks like the emergence of reader driven metrics might also provide opportunities to explore how to make not only recommendations on the existing literature, but also how to predict future success. In particular that Post-Docs and PhD students were found to read more highly cited literature than other demographics in Mendeley begs the question, could we identify a cohort amongst these groups that reads papers before they become highly cited. Aside from those speculations, the work that Jevin presented was both visually appealing, and possibly useful. The platform he is working on is open, and is ready to be tried by you, dear interested publisher.&lt;/p&gt;
&lt;p&gt;Lisa Schiff and Carly Strasser talked about the intricacies of introducing ALMs into the context of an organization that has a high bureaucratic overhead. It was nice to be reminded that the sacred DOI only covers a set of the scholarly record. ARCs are in widespread use, and as a community we should get on top of that. I was very impressed by Carly&amp;rsquo;s passion to bring change to this system, and share her hopes that we can make things better.&lt;/p&gt;
&lt;h1 id=&#34;putting-alm-in-context&#34;&gt;Putting ALM in Context&lt;/h1&gt;
&lt;h2 id=&#34;amy-brandfaculty-appointments-and-the-record-of&#34;&gt;Amy Brand	Faculty appointments and the record of&lt;/h2&gt;
&lt;p&gt;scholarship&lt;/p&gt;
&lt;p&gt;Amy wrote on this topic in eLife in &lt;a href=&#34;http://elife.elifesciences.org/content/2/e00452&#34;&gt;Point of view: Faculty appointments and the record of scholarship&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Harvard only introduced tenure track about 7 years ago. They now have a real commitment to tenuring junior faculty. They do a lot of faculty coaching and mentoring.&lt;/p&gt;
&lt;p&gt;She only heard about the San Francisco &lt;a href=&#34;http://am.ascb.org/dora/&#34;&gt;Declaration on Research Assessment&lt;/a&gt; a few days ago - but this mirrors a lot of what Harvard does in terms of faculty appointment.&lt;/p&gt;
&lt;p&gt;Harvard only gives tenure to people who have a Harvard degree. If you come from outside of Harvard and they want to give out tenure, then they give you an honorary degree.&lt;/p&gt;
&lt;p&gt;The main thing that flows through the system is the &amp;ldquo;Case statement&amp;rdquo; - a dossier.&lt;/p&gt;
&lt;p&gt;The works of scholarship that are getting featured in the CV is getting diverse - including software and public communications, which also includes artifacts such as blog posts.&lt;/p&gt;
&lt;p&gt;The most important part is the peer review, and the peer evaluations - that come from 10 - 15 independent experts, and confidential letters from every member of the faculty to the Dean.&lt;/p&gt;
&lt;p&gt;They do this analysis in a comparison set, where they look at a set of scholars together. One of the key things is field definition. They look at citations to papers - journal name is not listed. This citation report is not extremely important, they just want to know if the person is in range, it&amp;rsquo;s only interesting if it indicates that there are any outliers. I&amp;rsquo;ll say that again. For the citation record they don&amp;rsquo;t look at the journal names. They only use citations to help understand whether the candidate is operation within the normal expected range within their discipline, and in comparison to their peers. It&amp;rsquo;s used as a sanity check, and not insanely used as the key piece of evidence.&lt;/p&gt;
&lt;p&gt;ORCID is really important. They want to extend the &amp;ldquo;on-grid&amp;rdquo; record. They want service info to be included in the ORCID record.&lt;/p&gt;
&lt;p&gt;Change will happen in an institution like Harvard if it is driven from the faculty.&lt;/p&gt;
&lt;p&gt;Peer review of people and papers tend to be closed systems. There is high trust, known expertise, but closed systems. In open systems there is less trust, but the actors can be identified, and that&amp;rsquo;s an opportunity.&lt;/p&gt;
&lt;h2 id=&#34;mike-taylorthe-many-faces-of-altmetrics-mapping-the-social-reach-of-research&#34;&gt;Mike Taylor	The many faces of altmetrics: mapping the social reach of research&lt;/h2&gt;
&lt;p&gt;Mike is with &lt;a href=&#34;labs.elsevier.com&#34;&gt;labs.elsevier.com&lt;/a&gt;. He has been working on Altmetrics for the last two years. He is looking at the social reach of research.&lt;/p&gt;
&lt;p&gt;He finds it helpful to explain ALMs simply but putting them into low-judgment buckets of data classes. This helps to communicate internally. His buckets are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Social activity&lt;/li&gt;
&lt;li&gt;Component re-use&lt;/li&gt;
&lt;li&gt;Scholarly commentary&lt;/li&gt;
&lt;li&gt;Scholarly activity&lt;/li&gt;
&lt;li&gt;Mass Media mentions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;He is trying to see if these buckets can be tied against the different areas in which ALMs can be applied. It turns out that there are lot&amp;rsquo;s of areas of research needed. Mike shows a great graph looking at different type of activity.&lt;/p&gt;
&lt;p&gt;A few years ago we were were in a black hole. A paper would be published and we would wait a few years before citations coming in. ALM data is now starting to sprinkle some light into that darkness, but we need a lot more data, the volume of papers with ALM data remains low.&lt;/p&gt;
&lt;p&gt;We need more data points, more buckets.&lt;/p&gt;
&lt;p&gt;He had a look at PR notices going out. Many don&amp;rsquo;t have links to the primary research. Government documents are worse.&lt;/p&gt;
&lt;p&gt;Mike mentions Geo Location to research - that&amp;rsquo;s the first mention of that idea across the three days so far.&lt;/p&gt;
&lt;p&gt;Not all buckets are equal.&lt;/p&gt;
&lt;p&gt;If research gets picked up in social networks it often drops it&amp;rsquo;s formal name, gets known by a nickname, happens in real time. As it progresses through these networks, the more the name changes and evolves, the less likely that it will continue to be linked back to the original paper.&lt;/p&gt;
&lt;h2 id=&#34;juan-alperinare-almsaltmetrics-propagating-global-inequality&#34;&gt;Juan Alperin	Are ALMs/altmetrics propagating global inequality?&lt;/h2&gt;
&lt;p&gt;Juan is talking about the potential dangers of ALMs. This is a barnstorming talk. One of the other stand out highlights of the conference. I&amp;rsquo;m embarrassed that I didn&amp;rsquo;t take notes, I must have been too engaged. TL;DR we have to be careful not to fuck up how we structure these systems to not leave the global south behind. We are building these tools through the lens and web of the affluent North. Just go look at his &lt;a href=&#34;https://speakerdeck.com/jalperin/altmetrics-propagating-global-inequality&#34;&gt;slide deck&lt;/a&gt; now.&lt;/p&gt;
&lt;h1 id=&#34;expanding-the-breadth-of-alms&#34;&gt;Expanding the breadth of ALMs&lt;/h1&gt;
&lt;h2 id=&#34;eva-amsen-the-metrics-of-public-referee-reports&#34;&gt;Eva Amsen The metrics of public referee reports&lt;/h2&gt;
&lt;p&gt;She is going to talk about a product that doesn&amp;rsquo;t exist yet - ALMs for referee reports. F1000 research uses Cross Mark to link versions of the article together. Great slide on the history of open peer review in the literature. If you are accused of being a predatory journal, having open reviews really helps.&lt;/p&gt;
&lt;p&gt;They are providing a 50% reduction in article processing fees on any article that is submitted in the next year, for researchers who do a review for them - smart!.&lt;/p&gt;
&lt;p&gt;When they asked their community what feedback they had - they were asked for badges, metrics and making the reports &amp;ldquo;citable&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Is there anything that we can standardize about how we display referee reports? That would allow for aggregation of report data across different journals.&lt;/p&gt;
&lt;p&gt;Would a referee report metric affect the ALM metric?&lt;/p&gt;
&lt;p&gt;If you are a referee of an article that is very popular, does that credit get to rub off on you? - we could totally do this in eLifee.&lt;/p&gt;
&lt;h2 id=&#34;martin-fenner--jennifer-songbuilding-for-the-future-plos-alm-application&#34;&gt;Martin Fenner &amp;amp; Jennifer Song	Building for the future: PLOS ALM application&lt;/h2&gt;
&lt;p&gt;They use vagrant to get the PLOS ALM app running. They have a feature that tracks health of the calls to external API calls.&lt;/p&gt;
&lt;p&gt;They have integrated ALM scores into search results page of PLOS. I got to play with the ALM app over the weekend, and was able - with Martin&amp;rsquo;s help - to setup an instance with eLife DOIs. We will probably roll that out permanently over the next few weeks.&lt;/p&gt;
&lt;h2 id=&#34;gregg-gordonthe-real-impact-of-alms&#34;&gt;Gregg Gordon	The real impact of ALMs&lt;/h2&gt;
&lt;p&gt;They have about 240k authors in the library. They have about 500k papers, and are getting about 70k papers per year. They are getting to the point where they were getting more revisions happening per day, than new submissions - shows that this is a corpus of living documents, they go into SSRN and then they evolve there.&lt;/p&gt;
&lt;p&gt;There are a lot of different metrics out there, their first metric was Download. They spend between 50 - 100k per year verifying downloads.&lt;/p&gt;
&lt;p&gt;There is trust in the systems, and authors are now using these downloads as a proxy to look at other factors of impact.&lt;/p&gt;
&lt;p&gt;You see gaming everywhere, you just have to deal with it. They have an internal fraud index report.&lt;/p&gt;
&lt;p&gt;They created CiteReader within SSRN. They have introduced eigenmetrics into their system.&lt;/p&gt;
&lt;p&gt;Get the book - picture cook see make eat - a different interface for recipes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PLOS ALM 13, day1</title>
      <link>http://scholarly-comms-product-blog.com/2013/10/16/plos-aml-13-day1/</link>
      <pubDate>Wed, 16 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2013/10/16/plos-aml-13-day1/</guid>
      <description>&lt;p&gt;## Cameron Neylon - Introduction &amp;amp; Welcome&lt;/p&gt;
&lt;p&gt;Interesting - this is the first PLOS ALM meeting that is a &amp;ldquo;normal&amp;rdquo; scheduled presentation. Time is going to be tight.&lt;/p&gt;
&lt;h2 id=&#34;pete-binfield-alm-looking-back-moving-forward&#34;&gt;Pete Binfield ALM: Looking back, moving forward&lt;/h2&gt;
&lt;p&gt;A large chunk of OA does not select for impact - this is why ALMs are key for this space. PLOS didn&amp;rsquo;t invent ALMs - Frontiers were doing it a little ahead of PLOS&amp;rsquo;s launch. Web of science didn&amp;rsquo;t tell PLOS until 2010 that PLOS one was being tracked for an impact factor.&lt;/p&gt;
&lt;p&gt;People are quoting ALMs in their resumes for applying to be on the editorial board on PeerJ!&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://aig.cs.man.ac.uk/people/srp/&#34;&gt;Steve Pettifer&lt;/a&gt; claims ALMs have helped him to get tenure.&lt;/p&gt;
&lt;p&gt;The big thing that is missing is that we have not provided mechanisms to provide readers with recommendations. Filtering mechanisms for ALMs have not been well developed yet.&lt;/p&gt;
&lt;p&gt;There has been a period of talking to ourselves, but there seems to a broader interest now.&lt;/p&gt;
&lt;p&gt;Why have publishers not yet improved on the display of the PLOS ALM data?&lt;/p&gt;
&lt;p&gt;ALMs were built on the assumption that there is a pool of data available via APIs, but already in the last few years a number of sources have already hit the deadpool. (We need to own local copies of our own ALM data, as a publisher - really really important).&lt;/p&gt;
&lt;p&gt;Filtering tools were on the backlog from day one - but not yet built.&lt;/p&gt;
&lt;h1 id=&#34;best-practices-and-standards&#34;&gt;Best Practices and Standards&lt;/h1&gt;
&lt;p&gt;## Todd Carpenter - NISO altmetrics project&lt;/p&gt;
&lt;p&gt;Probably best to see my notes that I &lt;a href=&#34;http://partiallyattended.com/2013/10/16/niso-alm-standardisation/&#34;&gt;posted&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;## Scott Chamberlain - Programmatic access for Altmetrics&lt;/p&gt;
&lt;p&gt;Works with &lt;a href=&#34;http://ropensci.org/&#34;&gt;rOpenSci&lt;/a&gt; - creates open software to make science better.&lt;/p&gt;
&lt;p&gt;Mentions &lt;a href=&#34;http://raml.org/&#34;&gt;raml.org&lt;/a&gt; - a markdown file for creating an API. I&amp;rsquo;ve not heard about this before, sounds very interesting.&lt;/p&gt;
&lt;h2 id=&#34;kaitlin-thaney-metrics-for-web-native-science&#34;&gt;Kaitlin Thaney Metrics for web-native science&lt;/h2&gt;
&lt;p&gt;This was a great talk, but I as I&amp;rsquo;m quite familiar with a lot of the topics that Kaitlin was discussion, I ended up working on my own slides during this talk.&lt;/p&gt;
&lt;p&gt;## Geoffrey Bilder - CrossRef metadata and services in the service of ALMs&lt;/p&gt;
&lt;p&gt;Geoff has got to the point where he has started to caution people about the use of DOIs, there is a bit of a cargo cult growing around this.&lt;/p&gt;
&lt;p&gt;Brand is important - Geoff does the thing with showing a blacked out paper, we all recognize it.&lt;/p&gt;
&lt;p&gt;He says that DOIs are beginning to gain that type of branding. By and large, when we encountering DOIs we tend to think of them as scholarly entities - but every DVD in the world now includes a DOI - these are just being used as a registry, DOIs may escape into the wild.&lt;/p&gt;
&lt;p&gt;DOIs are not an arbiter of quality - some articles have two DOI&amp;rsquo;s, one is the author copy and one is the publisher copy.&lt;/p&gt;
&lt;p&gt;search.crossref.org has had a nice facelift - it also has a REST api.&lt;/p&gt;
&lt;p&gt;Crossref has citations in their metadata, and an increasingly large number of publishers are starting to do this. They are collecting funder information via fundref, the funder taxonomy is available under a CC0 license. They are increasingly including license information in the form of URIs. They are gathering ORCID information too. Abstracts are starting to be distributed via JATS compatible abstracts (we should do this from within eLife).&lt;/p&gt;
&lt;p&gt;They are starting to gather patent info for any patent that cites a paper.&lt;/p&gt;
&lt;p&gt;They are starting to look at reverse lookup service for referrals via DOI. (some people played with some of this data at the data challenge hackday on the following Saturday, and it looks very promising and interesting).&lt;/p&gt;
&lt;p&gt;I really like the distinction between persist-able and persistent identifier. There is nothing intrinsically persistent about a DOI, it&amp;rsquo;s intrinsically redirect-able. Persistent doesn&amp;rsquo;t mean that something will be around forever, but rather that it&amp;rsquo;s stubborn.&lt;/p&gt;
&lt;h1 id=&#34;scholarly-research-on-alm&#34;&gt;Scholarly Research on ALM&lt;/h1&gt;
&lt;h2 id=&#34;william-gunn---research-assessment-using-mendeley-readership-data&#34;&gt;William Gunn - Research assessment using Mendeley readership data&lt;/h2&gt;
&lt;p&gt;William is going to talk about the Mendeley data set. William talks about wanting to measure units of innovation and impact (I think this is one use cases for ALMs, but I think other use cases, such as domain mapping, are also pretty interesting).&lt;/p&gt;
&lt;p&gt;One of the more interesting discussions I&amp;rsquo;ve had so far at the meeting was last night with William talking about the reproducibility initiative that he is working on with Elizabeth Iorns from &lt;a href=&#34;https://www.scienceexchange.com/&#34;&gt;ScienceExchange&lt;/a&gt;.  You can read more about that &lt;a href=&#34;https://www.scienceexchange.com/reproducibility&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;zohreh-zahedi---what-is-the-impact-of-the-publications-read-by-the-different-mendeley-users-could-they-be-considered-as-signals-of-future-impact&#34;&gt;Zohreh Zahedi - What is the impact of the publications read by the different Mendeley users? Could they be considered as signals of future impact?&lt;/h2&gt;
&lt;p&gt;It is fantastic to see the ALM community starting to bring in researchers from the bibliometrics community. Anything that we can have that can help with us to understand the robustness of ALMs can only help. They ran this study against 20k randomly selected documents, and then with 200k publications.&lt;/p&gt;
&lt;p&gt;This talk This is research in progress, but the answer seems to be that there are advantages to using readers over citations, and the trend is that post-docs and phd students read more highly cited papers than professors. Further testing needs to be done.&lt;/p&gt;
&lt;p&gt;## Ehsan Mohammadi - Mendeley readership altmetrics for clinical medicine and engineering&lt;/p&gt;
&lt;p&gt;He was unable to get a visa to come to the US. Stefanie will give Ehasn&amp;rsquo;s presentation instead. There are a number of bibliometritians looking at altmetrics, at the recent bibliometrics conference there were two full sessions on this. This is another talk that compares mendeley reader data to citations in clinical medicine.&lt;/p&gt;
&lt;p&gt;There is a significant correlation between Mendeley readers and citations. Cancer his the highest correlation of 0.6. Some fields have a lower correlation, but at some point over the conference it was mentioned that a lower correlation in clinical fields might be expected, where doctors do read, but publish less, and so in a field like that Mendeley may be capturing impact that is not visible through citation counts.&lt;/p&gt;
&lt;h1 id=&#34;stefanie-haustein---empirical-analyses-of-scientific-papers-and-researchers-on-twitterresults-of-two-studies&#34;&gt;Stefanie Haustein - Empirical analyses of scientific papers and researchers on Twitter: Results of two studies&lt;/h1&gt;
&lt;p&gt;I think this is similar to the talk that Stefaine gave yesterday. They looked at a large scale analysis of tweets on 1.4M articles from PMC, and they also looked at a small scale analysis of 32 astronomers who are active on twitter.&lt;/p&gt;
&lt;p&gt;They divided papers into four classes, based on low vs high coverage of documents from within a disciple against high vs low correlation with citations.&lt;/p&gt;
&lt;p&gt;The overall correlation of twitter was low - 0.1, with a 9% coverage of the literature.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Penis&amp;rdquo; papers get highly tweeted. (on the topic of tweets, the comment from Euan yesterday is interesting to keep in mind - that authors &lt;em&gt;love&lt;/em&gt; to read the actual tweets about their papers).&lt;/p&gt;
&lt;p&gt;The in-depth study was also interesting. Those who do not publish, tweet regularly. Those who publish frequently don&amp;rsquo;t tweet frequently. There is a negative correlation between the number of tweets and the number of publications.&lt;/p&gt;
&lt;p&gt;What this tells us is that tweets and followers are definitely a different measure to citations and standard citation metrics.&lt;/p&gt;
&lt;p&gt;Overlap between phrases in abstracts and twitter is pretty low. This means that 4.1% of tweets are tweeting bits of abstracts - this seems low, but of the 100 most frequently used terms in abstracts, most of these terms do make it to twitter. (So either tweets are having parallel conversations to the abstracts, or a using a very different shorthand to refer to abstracts.)&lt;/p&gt;
&lt;p&gt;# Reporting on ALM&lt;/p&gt;
&lt;p&gt;## Andrea Michalek - Alternative metrics in practice&lt;/p&gt;
&lt;p&gt;I really like the direction that this product is heading in.&lt;/p&gt;
&lt;p&gt;## Adam Dinsmore - An exploratory review of PLOS ALM Reports: a funder’s perspective&lt;/p&gt;
&lt;p&gt;There is a great example of Wellcome looking at ALM data for evaluating one of their researchers. They used the PLOS ALM reports tool. Great example about looking at the people who tweeted about a paper on alcohol abuse.&lt;/p&gt;
&lt;p&gt;Institutional identifiers would help the Trust.&lt;/p&gt;
&lt;p&gt;## Juan Alperin - Visualizing ALMs in d3.js&lt;/p&gt;
&lt;p&gt;This talk could be very interesting for how we chose to visualize ALMs in the future on eLife. eLife is also using the D3 javascript library.&lt;/p&gt;
&lt;p&gt;Does not like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;monotonically increasing numbers. For a reader it&amp;rsquo;s harder to see what the actual usage is over time.&lt;/li&gt;
&lt;li&gt;a table of a lot of numbers is hard to use.&lt;/li&gt;
&lt;li&gt;lot of prominence to the sources is not so useful.&lt;/li&gt;
&lt;li&gt;the doughnut is not helpful for the reader - you are only guided by the number.&lt;/li&gt;
&lt;li&gt;the line in the total-impact metric only tells you one number.&lt;/li&gt;
&lt;li&gt;badges from impact story are giving the you the same percentile info.&lt;/li&gt;
&lt;li&gt;does not like squeezing the three types of data into one view - you can&amp;rsquo;t follow the pdf trend-line.&lt;/li&gt;
&lt;li&gt;XML data does not add much value.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Does like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;having a reference set.&lt;/li&gt;
&lt;li&gt;likes being able to get more details from altmetric - being able to see the individual tweets.&lt;/li&gt;
&lt;li&gt;does like the non-cumulative views.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;His visualization tries to get to the following principles:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://jalperin.github.io/almviz/&#34;&gt;example&lt;/a&gt;, &lt;a href=&#34;https://github.com/jalperin/almviz&#34;&gt;repo&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;has a number for the source.&lt;/li&gt;
&lt;li&gt;has a sense of history and not giving you a cumulative view, I like the weekly/yearly switch.&lt;/li&gt;
&lt;li&gt;on the axis the only thing that is showing on the y-axis is the maximum value to drive a sense of history.&lt;/li&gt;
&lt;li&gt;being able to rollup into different time levels.&lt;/li&gt;
&lt;li&gt;few labels - no grid lines.&lt;/li&gt;
&lt;li&gt;having a tool-tip with hover over the data point.&lt;/li&gt;
&lt;li&gt;graphs are the same size, but stacked on top of each other, and scaled accordingly.
&lt;ul&gt;
&lt;li&gt;different data sources have different granularity, so it would not be simple to make the graphs switch to different   granularities simultaneously&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;when there is a small number of events you need to be able to turn off the graphs.&lt;/li&gt;
&lt;li&gt;being able to distinguish between a 0 count, and not having any data for that time.&lt;/li&gt;
&lt;li&gt;be like to be able to turn on and turn off sources, rearrange the stacking.&lt;/li&gt;
&lt;li&gt;should be able to go to a page that contains a more detailed article profile.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They are going to roll this out to all of the journals that implement OJS and that sign up for the ALM service. They are going to run the PLOS ALM tool.&lt;/p&gt;
&lt;p&gt;# Heather Piwowar - The altmetrics CV: opportunities and challenges (also featuring Jason Priem)&lt;/p&gt;
&lt;p&gt;They think that if you give researchers ALMs then you are putting science back in the hands of scientists (I guess rather than having it in the hands of ISI).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cambridge usability group - UX and agile</title>
      <link>http://scholarly-comms-product-blog.com/2013/09/30/cug-agile-ux/</link>
      <pubDate>Mon, 30 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2013/09/30/cug-agile-ux/</guid>
      <description>&lt;p&gt;Sophie Freiermuth(&lt;a href=&#34;https://twitter.com/wickedgeekie&#34;&gt;@wickedgeekie&lt;/a&gt;)) from &lt;a href=&#34;https://twitter.com/BaguetteUX&#34;&gt;Baguette UX&lt;/a&gt; gave tonight&amp;rsquo;s talk on how lessons learnt from operating as a UXer within an agile environment. I thought it was a great talk. When at Mendeley we always faced the problem of trying to square the challenge to fitting an inherently creative process into the systems that emerged while we attempted to adopt agile processes, in my time there we learnt a lot, we had some success and some areas where we obviously needed to learn more, so I was delighted to hear a very thoughtful presentation from the perspective of someone who it seems has worked with quite a few teams implementing agile in a variety of flavours.&lt;/p&gt;
&lt;p&gt;My notes below are pretty much a transcript typed out while listening, but as usual, any inaccuracies and typos are mine.&lt;/p&gt;
&lt;h1 id=&#34;agile&#34;&gt;Agile&lt;/h1&gt;
&lt;p&gt;Remember that Agile remains a manifesto - it was created by developers and is  aimed at solving problems that developers have in attempting to build products.&lt;/p&gt;
&lt;p&gt;The agile manifesto is now over 11 years old, over that time it&amp;rsquo;s been adopted by teams of all sizes worldwide.&lt;/p&gt;
&lt;p&gt;At Agile 2013 - the recent agile conference - there were lots of stories about agile at companies of all sizes.&lt;/p&gt;
&lt;p&gt;Nokia and Spotify are two of the bigger companies that have adopted it, and Sophie talked quite a bit about Spotify&amp;rsquo;s experience. Spotify say that for them agile is still a work on progress.&lt;/p&gt;
&lt;p&gt;One of their big secrets is that the spaces need to be managed to enable collaboration. They have built the 3-wall rooms. The 3 walls allow enough space to pin up work, but the 3rd wall makes the work visible to people outside of those rooms, and also the work in action visible. Each space has enough room for people to co-share, to have meetings, and to collaborate.&lt;/p&gt;
&lt;p&gt;They need a large space - they needed more room to allow people to collaborate, and move around. (need more space than is usually assigned by facilities). Line management bought in, but the big blocker was getting facilities management to agree to the space requirements.&lt;/p&gt;
&lt;p&gt;Points that worked in this setup  - needs to be flexible - self sufficient - 3 walls only - mix of desks and meeting spaces - self organised, a team of up to 12 people.&lt;/p&gt;
&lt;p&gt;Spotify have been experimenting with team size, there is a magic number of about 12 people. 12 includes all the people that are needed to get the work done - design - front end - ux - qa - Product Owner. When a team gets to larger than 12 they break the team up, and split the work streams. The team needs to have all of the skills required. Every team is responsible for how they work, there is no company-wide process. They are allowing teams to self-organise around how they manage their work.&lt;/p&gt;
&lt;p&gt;They are trying to allow functions across the teams to get together in structures they call guilds, to share function specific know-how.&lt;/p&gt;
&lt;p&gt;The PO&amp;rsquo;s job is to ensure that the right jobs get into the room (Many problems just should never enter the dev room, and should be answered  in other locations). The PO has this responsibility. Some POs don&amp;rsquo;t want to know how the team tackles problems, some POs are interested - each team is finding it&amp;rsquo;s own way.&lt;/p&gt;
&lt;p&gt;Another interesting thing is the role of the agile coach.  In one instance mentioned the agile coach helped the business feel that there was oversight, however some people in the agile community think that coaches can disrupt the natural learning cycle of a team can be disrupted. In the story presented here having an agile coach helped the presenter. We used a couple of agile coaches at Mendeley, and my experience was generally positive, but I was standing on the pointy hair&amp;rsquo;d side of the desk, so I had some sense that I was bringing control to a situation. Some of the people who did some of the coaching liked it, and some didn&amp;rsquo;t.&lt;/p&gt;
&lt;h1 id=&#34;how-has-agile-worked-for-this-ux-person&#34;&gt;How has agile worked for this UX person?&lt;/h1&gt;
&lt;p&gt;Sitting with a dev team can be invaluable, seeing the thing being built as it goes along is invaluable. It also helps to ensure that the quality stays in place, conversations can happen about shortcuts before they get backed into the product, and optimal decisions can be taken, rather than being laboured with decisions that have taken place on the fly.&lt;/p&gt;
&lt;p&gt;The walls are an amazing resource. Placing the user journey on the walls as a print out, next to the stories can really help the developers. Developers can also comment on the user flows when they see that there might be a technical issue. By doing the work on the board, and making the process apparent, this leads to collaboration, and to reaching a solution faster, it also brings confidence to the team that UX can bring a lot of value. (Sophie mentioned in the Q&amp;amp;A that where she had drawn a user journey on the whiteboard, when explaining it to other people she would erase it and re-draw it. By making them a participant in the bringing out of the journey onto the whiteboard they got it a lot faster, and could often ask critical questions that she might have overlooked, she described this as something like &amp;ldquo;sharing the deliverable&amp;rdquo; rather than &amp;ldquo;arriving with the deliverable&amp;rdquo;).&lt;/p&gt;
&lt;p&gt;When a UX person is placed with a dev team, often the UX person will have invested in the idea that the developers know what they are doing, however the developers often don&amp;rsquo;t have trust yet in the UX person. That person has to prove value.&lt;/p&gt;
&lt;p&gt;The UX/Design/Copy team - how this fits into the sprint cycle has not been cracked. Smaller tasks can be easier to fit in within a sprint cycle, however you need to be clear on the work that needs to be done, what the request means - e.g. we need to improve conversions - is not often clear - is it page speed, does the flow need to be improved - is there a code issue, does some user research need to be done?&lt;/p&gt;
&lt;h1 id=&#34;tools&#34;&gt;Tools&lt;/h1&gt;
&lt;p&gt;Taking tools from agile can be very helpful, for example using Kanban to display where a bottleneck might occur, even when the work is flowing through one person. (At Mendeley we discovered that our bottleneck was not in UX, where we had thought it was, but rather downstream from UX in the front end development side of the product).&lt;/p&gt;
&lt;h1 id=&#34;another-big-tip-is-lower-the-definition&#34;&gt;Another big tip is lower the definition.&lt;/h1&gt;
&lt;p&gt;You can say that you need to release &amp;gt; test &amp;gt; iterate within the UX cycle. Give a definition of done. Done is easy to say, but impossible to define within UX. What is the definition of done within UX - a sketch - a full spec? Try and find the lowest definition of done as possible, it&amp;rsquo;s a win for everyone, as long as the developer can work with it, and you don&amp;rsquo;t lose information. This will serve the team better, to make this work you need to be sure you are collaborating, you need to have trust with the team, you can&amp;rsquo;t start with scribbles from day one - but you can get there.&lt;/p&gt;
&lt;h1 id=&#34;lean&#34;&gt;Lean&lt;/h1&gt;
&lt;p&gt;The lean manufacturing process defined by Taiichi Ohno has a lot of value. You eliminate waste, you empower people, and you introduce Kanban. The more you adopt the thinking that one thing can only be in one place at one time. Agile is not faster, but it does show pieces of the solution earlier. The overall time spent may not be less than the waterfall process, but you will see parts of the solution earlier - (In my opinion you should in theory be able to reduce waste, and make invisible work more visible). Lean is human first, it&amp;rsquo;s about people.&lt;/p&gt;
&lt;h1 id=&#34;firefighting&#34;&gt;Firefighting&lt;/h1&gt;
&lt;p&gt;Often the dev team is going full speed ahead - surfacing problems that were not identified earlier. If you drop into a team, you won&amp;rsquo;t be able to just shout &amp;ldquo;STOP&amp;rdquo;. The best way to firefight is to know the technology as much as possible - have accurate technical knowledge. Agile is all about &lt;code&gt;people&lt;/code&gt; &lt;code&gt;people&lt;/code&gt; &lt;code&gt;people&lt;/code&gt;. Empower people, make them feel successful, make them feel valued. UX people naturally tend towards being empathetic, so people relations should work well for them. Define what success looks like. Often the question of what success looks like surprises the stakeholders. If you can get this question out, you can get the steak-holders to commit to what the success should look like. Be Michael Schumacher. This is not about being the fastest, but about cutting the right corners in the right way - e.g. can you get away with a low fidelity sketch, can you get away with shorter stand-ups in the morning? It&amp;rsquo;s not about making time to squeeze more work in, but rather making the time you have count, and making sure you have a life.&lt;/p&gt;
&lt;h1 id=&#34;side-competencies-and-agile&#34;&gt;Side competencies and Agile&lt;/h1&gt;
&lt;p&gt;Refuse the absurd (or at least point it out) - when you see these issues crop up in user stories. Make sure the stories have a target, a requirement for success. Every team is unique. There is no generalisation, everyone is trying to make agile work. No one has fully cracked it, but it seems to work. Every case is different. Agile is for people who can deal with constant change, with people who like experimenting. This is a method of working that suites outgoing people. Apply top skills first, and refine later - reach wide, try and solve problems where you can, then refine later. You need to learn the value of getting things done, rather than getting things perfect. In agile teams there is the rise of the unicorns. A UX who is a developer, a great copy writer - these people &lt;em&gt;do not exist&lt;/em&gt;. It&amp;rsquo;s OK not to have unicorns on the team. Fight the battle with the troops you have, not the troops you wish you had.&lt;/p&gt;
&lt;h1 id=&#34;what-is-the-future-of-agile&#34;&gt;What is the future of agile?&lt;/h1&gt;
&lt;p&gt;Agile has guardians! The people who designed the agile manifesto are still out there, still communicating. There are lots of flavours, we have had scrum, lean, XP &amp;hellip; there will be something coming along next. One big problem with agile is when will something be done, what will the stakeholders get. In agencies it is really difficult. When you go agile you need to work with a client who understands this. Each team has it&amp;rsquo;s own process. Are we all in harmony towards a common goal? Is agile the promised land? This still needs to be cracked. Could there be a riot of fed-up UXers brandishing sharpies and stickies? Yes! UX and design cannot sit perfectly in harmony, but agile is going to stick around, and the future will be interesting.&lt;/p&gt;
&lt;p&gt;Remember that agile was developed by developers, take the spirit, but don&amp;rsquo;t take it as a cookie cutter approach.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Great talk!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>EC consultation on Open Data - a report.</title>
      <link>http://scholarly-comms-product-blog.com/2013/07/02/ec-open-data-consultation-report/</link>
      <pubDate>Tue, 02 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2013/07/02/ec-open-data-consultation-report/</guid>
      <description>&lt;p&gt;This is a report on todays consultation on open data that was help by the EC. The notes are long, so I have put my conclusions and general comments at the start.&lt;/p&gt;
&lt;h1 id=&#34;general-comments&#34;&gt;General comments&lt;/h1&gt;
&lt;p&gt;There was not much disagreement throughout the day. There were repeated calls  for the need to incentivise  researchers to engage in data sharing, but not too many concrete proposals on how to do this. It does seem from my perspective that libraries could do an amazing job here, but that will depend on to which extent these libraries have deep technical expertise. One problem libraries seem to have is bridging the gap between their expertise and the scientist at the bench who just doesn&amp;rsquo;t know about what services they can call on.&lt;/p&gt;
&lt;p&gt;I spoke late in the day, but I was the first to mention CC0 explicitly, and the first to call for the explicit adoption of CC0/CC-BY, I was surprised by this.&lt;/p&gt;
&lt;p&gt;There was an overwhelming reiteration that primary research data is a public good, and as such the default position is that this data should be &amp;ldquo;open by default&amp;rdquo;. This was hugely encouraging. There was plenty of nuanced discussion that there are indeed areas where one would need to have restrictions in place around certain kinds of data, but the majority of people who made this point wanted to start from a default open position, and look for explicit reasons on a case by case basis for why one might not adopt this principle. I think this is a healthy way to proceed.&lt;/p&gt;
&lt;p&gt;There were some skirmishes over IP, patents and an explicit call from representatives from Phillips and from the German Defence industry that data should not be made open. One even saying that they liked public funding, but didn&amp;rsquo;t like the idea of opening that data (hello, can someone please let this person know what a &amp;ldquo;public good means?&amp;quot;). Anyway, both representatives were amenable to the idea of embargoes for data that is generated in public-private partnerships, so I think that was healthy. One aside is that this thread of conversation popped up throughout that day, but I feel that it is largely a distraction from the core question, one of the status of OpenData for primary publicly funded research. What it does show is that in this debate we need to get the lines really clear, so as not to waste cycles discussing edge cases, and so that we don&amp;rsquo;t end up imposing artificial restrictions for fears that should not really be applicable.&lt;/p&gt;
&lt;p&gt;No one mentioned linked data. No one.&lt;/p&gt;
&lt;p&gt;The really key issue, in my mind, is how do you build a system that captures data in a way that is more robust than the life time of the researcher who created it. If we could say with confidence that the data that a researcher used is as accessible to future generations, in the way that their publications are available, then we will have succeeded. We can still get out hands on the finches that Darwin worked on, which is amazing. That we can&amp;rsquo;t get the excel file that Joe Postdoc created six years ago is a shame on us.&lt;/p&gt;
&lt;h2 id=&#34;notes-from-the-day&#34;&gt;Notes from the day.&lt;/h2&gt;
&lt;p&gt;My notes on the actual discussions through the day are pretty much sketch like. If anything is unclear, I&amp;rsquo;m happy to respond in the comments.&lt;/p&gt;
&lt;h2 id=&#34;opening-notes&#34;&gt;Opening notes&lt;/h2&gt;
&lt;p&gt;Opening remarks, sets out three reasons that the commission believes the opening access to research data is a must.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;good for science&lt;/li&gt;
&lt;li&gt;good for SME&amp;rsquo;s - they have evidence for this already&lt;/li&gt;
&lt;li&gt;good for the citizen&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There will be room until the 15th of July to send written contributions to the consultation. Today is not a workshop, it is a hearing, and the main purpose for the day is to hear opinions from the stakeholders.&lt;/p&gt;
&lt;p&gt;The day starts with:&lt;/p&gt;
&lt;h2 id=&#34;the-research-perspective&#34;&gt;The research perspective.&lt;/h2&gt;
&lt;h4 id=&#34;jildau-bouwman-tno-department-of-microbiology-and-systems-biology&#34;&gt;Jildau Bouwman, TNO Department of Microbiology and Systems Biology&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;negative data.&lt;/li&gt;
&lt;li&gt;small data from home experiments.&lt;/li&gt;
&lt;li&gt;information in the methods section, including the meta-data data, the paper should be reusable.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Limits should be on sensitive data, commercial data. Need a specific budget in projects to help data being put into the open.&lt;/p&gt;
&lt;h4 id=&#34;paola-de-castro-istituto-superiore-di-sanità&#34;&gt;Paola De Castro, Istituto Superiore di Sanità&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;data management plans should be stressed&lt;/li&gt;
&lt;li&gt;mentions the g8 policy&lt;/li&gt;
&lt;li&gt;need a way to provide incentives for researchers&lt;/li&gt;
&lt;li&gt;they stress the importance of creating a global infrastructure.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;menno-kok-erasmus-universiteit-rotterdam&#34;&gt;Menno Kok, Erasmus Universiteit Rotterdam&lt;/h4&gt;
&lt;p&gt;Discusses the why of this.&lt;/p&gt;
&lt;p&gt;An important factor in the why, is how can we get more value from data. This means data enrichment. Mentions the danger of making some data available, specifically genome sequences. Sequences and phenotypes are the things that are dangerous.&lt;/p&gt;
&lt;p&gt;How do we stimulate the process? This is to do with fairness. This relates to when data should be made available.&lt;/p&gt;
&lt;p&gt;The patent question is going to be an important one. We may have to come to a tailor made solution that fits to all types of research.&lt;/p&gt;
&lt;p&gt;How can you get to this kind of solution? Through trial and error.&lt;/p&gt;
&lt;p&gt;They would like to propose that EU incorporates OA under Horizon 2020 as a carefully monitored limited trial.&lt;/p&gt;
&lt;h4 id=&#34;salvatore-mele-cern&#34;&gt;Salvatore Mele, CERN&lt;/h4&gt;
&lt;p&gt;How can you get 1000&amp;rsquo;s of people to share and to cooperate? If you build a community, where every contributor is known, and every contribution is acknowledged. He advocates that we can build a global community of sharing.&lt;/p&gt;
&lt;p&gt;He mentions the ODIN project. They found a very clear answer, they need to augment the existing infrastructure, need one that is technical, social and something else.&lt;/p&gt;
&lt;p&gt;Mentions key pieces of the infrastructures.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ORCID&lt;/li&gt;
&lt;li&gt;DataCite&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We need to accelerate the adoption of these approaches.&lt;/p&gt;
&lt;p&gt;No researcher should be left behind. Researchers without access to specific infrastructure should be able to make use of tools such as Zenodo.&lt;/p&gt;
&lt;h4 id=&#34;corrette-ploem-academic-medical-center-university-of-amsterdam&#34;&gt;Corrette Ploem, Academic Medical Center, University of Amsterdam&lt;/h4&gt;
&lt;p&gt;Patients, which are the providers of data, may expect that their data is shared as much as possible, from their perspective, It is not in their interest that researchers sit on their data (the patients want to be cured, right?).&lt;/p&gt;
&lt;p&gt;Standardisation of data, and encoding techniques, and legislation, on an EU level, is required.&lt;/p&gt;
&lt;h4 id=&#34;heading&#34;&gt;?&lt;/h4&gt;
&lt;p&gt;Incentives are crucial to build up a culture of data sharing. Research data needs to be considered as a research output on the level of journal articles.&lt;/p&gt;
&lt;p&gt;Costs should be included in project funding. Data management and data sharing plans should be required. Such an approach was proposed by the US government.&lt;/p&gt;
&lt;p&gt;Initiatives such as the Research Data alliance are helpful.&lt;/p&gt;
&lt;h4 id=&#34;andrew-smith-embl-ebi--elixir&#34;&gt;Andrew Smith EMBL-EBI / ELIXIR&lt;/h4&gt;
&lt;p&gt;We need to look at the term open data. That is a bit misleading, in life science, clearly not all data will be made open. When we talk about open data, we really are talking about accessible data.&lt;/p&gt;
&lt;p&gt;Mentions EU-PMC and EBI infrastructure. Where we can we should look to build on existing data bases.&lt;/p&gt;
&lt;p&gt;When we use the term data storage, we need to be careful. The costs in running these repositories often sits on the curation, running courses, developing standards, the cost is not just on the storage side.&lt;/p&gt;
&lt;p&gt;Feels that we should use Horizon 2020 for driving change.&lt;/p&gt;
&lt;h4 id=&#34;rolf-vermeij-european-consortium-of-innovative-universities&#34;&gt;Rolf Vermeij, European Consortium of Innovative Universities&lt;/h4&gt;
&lt;p&gt;Need to be able to find the data through a search engine. Need ways to enable searching that goes beyond Google.&lt;/p&gt;
&lt;p&gt;Some areas of science have a long standing history of data sharing. Chemists do not share data.&lt;/p&gt;
&lt;p&gt;People will need to be educated.&lt;/p&gt;
&lt;p&gt;Need stronger peer review on the data.&lt;/p&gt;
&lt;h4 id=&#34;debate&#34;&gt;Debate&lt;/h4&gt;
&lt;p&gt;A lot of data that is used is often coming from public administration. There is not much of a culture of data sharing from public sector information. These sources of information should be considered.&lt;/p&gt;
&lt;p&gt;Education is important. We need to convince researchers that their data is worth sharing. We need to educate researchers about what the basic elements of sharing are. Feels that naturally libraries have a role to play in that.&lt;/p&gt;
&lt;p&gt;There is a study that says that peer review of data is just too difficult, there is too much of it, and it would just break the system, we need to think of some other way of doing this.&lt;/p&gt;
&lt;p&gt;One approach is to just allow users to make comments on the data that they are using. The second approach is to do this through journals, journals should ensure that data is reviewed.&lt;/p&gt;
&lt;p&gt;If there is a clear understanding of whose job it is to do what when it comes to review, that&amp;rsquo;s helpful.&lt;/p&gt;
&lt;p&gt;My comments on peer review of data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;aside from reuse, making data available helps to prove that the experiment happened&lt;/li&gt;
&lt;li&gt;not the only solution, but tracking reuse is a good indicator that the data is useful&lt;/li&gt;
&lt;li&gt;must support negative publication results, to overcome publication bias for successful events in the lab.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A comment is made about what is data, archaeology provides great examples of how heterogeneous data is.&lt;/p&gt;
&lt;p&gt;If you can really associate who has provided which part of the data, then you are pinning reputation on the quality of the data. If you identify who is putting data out, you do not need a peer review system.&lt;/p&gt;
&lt;p&gt;In medical science the citation index is more important for your reputation than the quality of the data that you produce. Therefore some fields need education, and a change to their incentive structures.&lt;/p&gt;
&lt;h2 id=&#34;industry--industrial-research-perspective&#34;&gt;Industry / industrial research perspective&lt;/h2&gt;
&lt;h4 id=&#34;jan-van-den-biesen-philips-research-2&#34;&gt;Jan van den Biesen, Philips Research 2&lt;/h4&gt;
&lt;p&gt;OA to Scientific publications is really not an issue. No interference with the ability to protect IPR.&lt;/p&gt;
&lt;p&gt;Open access to research data is another matter. Open access to research data could affect the ability to protect innovations and IPR. Fully OA might destroy more value than it creates.&lt;/p&gt;
&lt;p&gt;They think it should be decided case by case. For example unsuccessful clinical trials, sharing these results can help reduce redoing unnecessary experiments, however making data from Enabling technologies open could scare away partners.&lt;/p&gt;
&lt;p&gt;They support the OA approach &lt;a href=&#34;http://www.whitehouse.gov/blog/2013/02/22/expanding-public-access-results-federally-funded-research&#34;&gt;proposed by the Obama administration&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Making raw data available to citizens doesn&amp;rsquo;t really help, this data should be refined into products by industry for citizens.&lt;/p&gt;
&lt;h4 id=&#34;helge-pfeiffer-advisory-council-for-aeronautics-research-in-europe-&#34;&gt;Helge Pfeiffer, Advisory Council for Aeronautics Research in Europe /&lt;/h4&gt;
&lt;p&gt;Needs to avoid inflation of papers - though salami slicing and fraud.&lt;/p&gt;
&lt;h4 id=&#34;thomas-weise-federation-of-german-security-and-defence-industries&#34;&gt;Thomas Weise, Federation of German Security and Defence Industries&lt;/h4&gt;
&lt;p&gt;Funding is highly appreciated, however OA cannot be in the interest of industry. 100% ownership of background information has to be guaranteed to industry. Release to 3rd parties has to be agreed by industry.&lt;/p&gt;
&lt;h4 id=&#34;debate-1&#34;&gt;Debate&lt;/h4&gt;
&lt;p&gt;Strong debate on the topic of standard position for openness.&lt;/p&gt;
&lt;p&gt;Someone makes the case that there is a strong difference between pure research and applied research. They believe that this is one of the things that the Horizon 2020 pilot should investigate.&lt;/p&gt;
&lt;p&gt;Mentions that there are parallels in an amendment that has been seen in public sector information directive, and the research cycle within industry. The directive has said that for public private partnerships, the default will be open, but that openness will happen under embargo. In addition those embargoes can be challenged. The PSI directive might provide a good framework.&lt;/p&gt;
&lt;p&gt;Thomas Weise could agree to this idea for embargo. In the US in defence, secret programs are suddenly published, but this means that they might not be interesting any more.&lt;/p&gt;
&lt;p&gt;q: does it make sense for Europe to have a policy, in spite of policies in other parts of the world, or do these policies need to be global. Thomas Weise says that there should be an EU policy in order to retain EU competitiveness. Need and EU publication strategy and research data strategy.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s important that Europe is showing leadership in open policies. On the other hand you cannot limit open access to within specific regions. So - yes European policies make sense, especially to join forces with other regions that are interested, but you cannot limit access to this research.&lt;/p&gt;
&lt;p&gt;Do not think that we should wait to harmonize our policies.&lt;/p&gt;
&lt;h2 id=&#34;research-funder-perspective&#34;&gt;Research funder perspective&lt;/h2&gt;
&lt;h4 id=&#34;juan-bicarregui-uk-research-councils&#34;&gt;Juan Bicarregui, UK Research Councils&lt;/h4&gt;
&lt;p&gt;Thinks most countries are already producing polices that are already harmonized. G8 agenda is mentioned again.&lt;/p&gt;
&lt;p&gt;STFC holds 40 PB of data, doubles every 15 months, soon they will hold 80PB. They also support a bunch of other tools, including the Square Kilometre meter Array.&lt;/p&gt;
&lt;h4 id=&#34;david-carr-wellcome-trust&#34;&gt;David Carr, Wellcome Trust&lt;/h4&gt;
&lt;p&gt;Value of data vs resources required.
There can be limits to data sharing via IP.
Need to balance the needs between data generators and users.&lt;/p&gt;
&lt;p&gt;There are challenges:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;enhancing implementation and enforcement of policies&lt;/li&gt;
&lt;li&gt;guiding a sustainable culture of data sharing&lt;/li&gt;
&lt;li&gt;recognise that different disciplines are at different stages&lt;/li&gt;
&lt;li&gt;need to forge partnerships between funders the research community and other stakeholders&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;anne-wetterbom-swedish-research-council--science-europe&#34;&gt;Anne Wetterbom, Swedish Research Council / Science Europe&lt;/h4&gt;
&lt;p&gt;Funders role is to provide the framework for their research environments.&lt;/p&gt;
&lt;p&gt;Swedish government in October 2012 published a bill on research and innovation. There is already a Swedish bill for making public information open, and research conducted through universities is considered to be public information.
The universities are responsible for archiving data from their scientists, but this puts a burden on universities, with the current data deluge.&lt;/p&gt;
&lt;p&gt;They want infrastructures to be cost efficient, and heterogeneous.&lt;/p&gt;
&lt;p&gt;They will work with different stakeholders over the coming year.&lt;/p&gt;
&lt;p&gt;During 2014 they are going to go to the government with a draft policy.&lt;/p&gt;
&lt;p&gt;They would like to have a discussion on funding models.&lt;/p&gt;
&lt;h4 id=&#34;debate-2&#34;&gt;Debate&lt;/h4&gt;
&lt;p&gt;A comment to focus on success stories, which can be used to show the value of access to open data.&lt;/p&gt;
&lt;p&gt;Should the way that IP is currently working be discussed, particularly around patents?&lt;/p&gt;
&lt;p&gt;Perhaps we should look at the property issue in a new way? In medical research contracts are very one sided. This is really a problem, we should think about the cash that is being generated through these partnerships.&lt;/p&gt;
&lt;p&gt;Public directive indicates that from 2015, any information that ends up in the university library will be considered as public information, including public sector information that has been generated from within the university.&lt;/p&gt;
&lt;p&gt;The question of licensing is being raised. A proposal is made to clarify copyright and licence, and suggests that there should be a limited set of patterns, like as in what has happened with creative commons.&lt;/p&gt;
&lt;p&gt;The decision about licensing should happen at the proposal stage, so that funders will know whether to fund up front.&lt;/p&gt;
&lt;p&gt;There is a defence of the patent system, a description of &amp;ldquo;the deal&amp;rdquo; for patents. (There is a strong [argument that the patent system is broken are broken][brpat], as &lt;a href=&#34;http://www.thisamericanlife.org/radio-archives/episode/441/when-patents-attack&#34;&gt;this American life episode&lt;/a&gt; The White House &lt;a href=&#34;http://www.whitehouse.gov/the-press-office/2013/06/04/fact-sheet-white-house-task-force-high-tech-patent-issues&#34;&gt;seems to agree&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;For open data, public funding is involved, and this does not preclude the protections that companies have. When we are talking about open access to data for public funding, we should not add more protections or additional layers of protection, as these layers already exist - via the patent system.&lt;/p&gt;
&lt;h2 id=&#34;information-systems--e-infrastructure-perspective&#34;&gt;Information systems / e-infrastructure perspective&lt;/h2&gt;
&lt;h4 id=&#34;nikos-askitas-institute-for-the-study-of-labor-in-bonn-germany&#34;&gt;Nikos Askitas, Institute for the Study of Labor in Bonn Germany&lt;/h4&gt;
&lt;p&gt;(I think this person is an economist)&lt;/p&gt;
&lt;p&gt;Sharing is a good thing, but not all researchers should share, it&amp;rsquo;s like donating, it&amp;rsquo;s a good thing, but not everyone will do it.&lt;/p&gt;
&lt;p&gt;Data is insurance against fact pollution. Data is not cheap.&lt;/p&gt;
&lt;p&gt;You have to make sure that the data stays meaningful over time. Research data is potentially any data.&lt;/p&gt;
&lt;p&gt;Research data could be defined as data that has been used at least once to answer a research question.&lt;/p&gt;
&lt;p&gt;What does making it available mean? Making it available separates research from hearsay. That must be what defines openness in this context.&lt;/p&gt;
&lt;p&gt;On limiting, two remarks - open does not mean free, at the same time proprietary does not mean closed. In the context of data, perhaps we could introduce a data tax.&lt;/p&gt;
&lt;p&gt;Perhaps an idea of corporations paying for proprietary data, could be opened in the form of data taxes (definitely an economist).&lt;/p&gt;
&lt;p&gt;In terms of storage, there could be journals, libraries, individuals. So centrally or distributed? The library of Alexandria no longer existed. Monks stored the data in a distributed fashion.&lt;/p&gt;
&lt;h4 id=&#34;donatella-castelli-italian-national-research-council-and-openaire--yannis-ioannidis&#34;&gt;Donatella Castelli, Italian National Research Council and OpenAIRE / Yannis Ioannidis&lt;/h4&gt;
&lt;p&gt;Requirements on data preservation, and management should be light at the project submission stage, and become much more rigorous before awarding of grants.&lt;/p&gt;
&lt;p&gt;Openness should be limited to quality data.&lt;/p&gt;
&lt;h4 id=&#34;peter-doorn-data-archiving-and-networked-services&#34;&gt;Peter Doorn, Data Archiving and Networked Services&lt;/h4&gt;
&lt;p&gt;Important to address small data, in addition to BIG data. We should not make open data a religion or a dogma, it&amp;rsquo;s important to be pragmatic.&lt;/p&gt;
&lt;p&gt;Researchers should not own the data that they collect with public funding. On limiting openness, protection of privacy is a factor, but it should not be a dogma. Certain public interests should be protected.&lt;/p&gt;
&lt;p&gt;It is good to allow an embargo for up to two years, for researchers who want to publish on data.&lt;/p&gt;
&lt;p&gt;On reuse, we need certain citation rules for data. These should include at least a persistent identifier. Make data available for peer review.&lt;/p&gt;
&lt;p&gt;It should be stored in trustworthy archives, should be certified by the EU framework - there are German and  ISO standards, for data archiving standards (could be very helpful).&lt;/p&gt;
&lt;p&gt;Make data management eligible for funding.&lt;/p&gt;
&lt;h4 id=&#34;matthew-dovey-joint-information-systems-committee--knowledge-exchange&#34;&gt;Matthew Dovey, Joint Information Systems Committee / Knowledge Exchange&lt;/h4&gt;
&lt;p&gt;Half of funding agencies in north Europe had data management plans, but only half of them had plans to implement these plans.&lt;/p&gt;
&lt;p&gt;Makes the point that sometimes it&amp;rsquo;s cheaper to recreate data, rather than storing.&lt;/p&gt;
&lt;p&gt;Data is often generated from an array of funding, and researchers are often not aware of the funder requirements.&lt;/p&gt;
&lt;p&gt;Funders need to fund ongoing support of data.&lt;/p&gt;
&lt;p&gt;Again, training is important. Do we concentrate on new researchers, or the exiting researchers?&lt;/p&gt;
&lt;p&gt;Infrastructure is easy, technology is easy, getting people to use the infrastructure is harder. (often often, the social context is harder than the technology).&lt;/p&gt;
&lt;p&gt;Any technology must fit with existing workflows and not impose new workflows.&lt;/p&gt;
&lt;h4 id=&#34;adam-farquhar-datacite&#34;&gt;Adam Farquhar, DataCite&lt;/h4&gt;
&lt;p&gt;DataCite now have 1.7M DOIs. +3M resolutions in 2013, 200 data centres. 275k DOIs in 2013.&lt;/p&gt;
&lt;p&gt;Founded in 2009.&lt;/p&gt;
&lt;p&gt;Data identification has now matured up away from local country standards. The point is there is no need to re-invent the wheel. Identification and citation level meta data are critical for incentives systems.&lt;/p&gt;
&lt;p&gt;Data citation require interoperable APIs and meta data (e.g. content negotiation with crossref).&lt;/p&gt;
&lt;p&gt;Data identification is more than just assigning a number. You need essential services to support this.&lt;/p&gt;
&lt;h4 id=&#34;david-giaretta-alliance-for-permanent-access&#34;&gt;David Giaretta, Alliance for Permanent Access&lt;/h4&gt;
&lt;p&gt;The common thread is how can we add value? Not just adding value to the creator, but also in other disciplines - commerce, government, the general public.&lt;/p&gt;
&lt;p&gt;Most data is unfamiliar to most people. Most people don&amp;rsquo;t think anything of clicking through 100 different web pages, most people would never do this for data sets - life is just too short.&lt;/p&gt;
&lt;p&gt;The key question is who pays, how much, and why? No one makes indefinite commitments.&lt;/p&gt;
&lt;p&gt;The solution seems to be to make data usable, by as many people as possible, for as long as possible. CIBER-DS is a project that is trying to do this.&lt;/p&gt;
&lt;p&gt;Need to investigate data marketplaces.&lt;/p&gt;
&lt;h4 id=&#34;bram-luyten-mire&#34;&gt;Bram Luyten, @mire&lt;/h4&gt;
&lt;p&gt;Data should be stored in the research institution. Researcher&amp;rsquo;s are able to forge large volumes of data. The reputation of the institution is at stake. The academic institution has a horizon that is longer than the span of an individual career, or a single project.&lt;/p&gt;
&lt;h4 id=&#34;discussion&#34;&gt;Discussion&lt;/h4&gt;
&lt;p&gt;Two specific questions - what should be the embargo period? When do you start counting the embargo period. Would it be reasonable for a funding agency, that has a rejection rate of 90%, should they ask for this up front, or as a first deliverable?&lt;/p&gt;
&lt;p&gt;Someone is missing the researcher in terms of how we are arguing how things should be like. Many people are arguing that their projects should be the one that holds the data. This person (the economist again), does not thing we should over burden the researcher. We may end up with shiny open mediocre stuff. Putting all of your data into one big trough makes it easy to put in, but hard to get out, thinks it is better to have small projects, with community driven curation, the solution needs to be distributed. (of course this perspective does not address the actual problem that our current data policies address).&lt;/p&gt;
&lt;p&gt;Salvatore says that adding more things to do when writing a proposal is not great, but some thinking about data management can be hugely useful. We could think of a process which encourages people to make sure that the data curation and opening can happen, for example, allowing time at the end of a project with funding, for doing the curation. Set an example, and let people know that it&amp;rsquo;s OK to take time out from core research to make the data open.&lt;/p&gt;
&lt;p&gt;If you want to convince researchers to publish data, you need to make researchers understand what is happening with their data - simple legal templates could help with this.&lt;/p&gt;
&lt;p&gt;Someone says something, but I couldn&amp;rsquo;t follow what they were talking about.&lt;/p&gt;
&lt;p&gt;On the question of embargoes it&amp;rsquo;s not possible to say that this should start at the end of data collection, as this is not a well defined point in time. In the data management plan if there is a request for embargo, this should be laid out in the data management plan. A way that could work, is to tie it to the end of the funding period, and tie this to the need to have embargo requests as part of the data management plan negotiation.&lt;/p&gt;
&lt;p&gt;In the UK, we are seeing that data management plans are being required up front in the grant application. In terms of support for creating data management plans, there is a role here for libraries to help in this domain.&lt;/p&gt;
&lt;p&gt;What you need at proposal time are data management intentions, and what you need during implementation is data management practice.&lt;/p&gt;
&lt;p&gt;At proposal time getting a feeling for how much it costs would also be good.&lt;/p&gt;
&lt;h2 id=&#34;publisher-perspective&#34;&gt;Publisher perspective&lt;/h2&gt;
&lt;h4 id=&#34;ian-mulvany-elife--plos--peerj--ubiquity-press&#34;&gt;Ian Mulvany, eLife / PLOS / PeerJ / Ubiquity Press&lt;/h4&gt;
&lt;p&gt;See my statement, and slides at my &lt;a href=&#34;http://partiallyattended.com/2013/07/02/ec-open-data-consultation-our-view/&#34;&gt;previous blog post&lt;/a&gt;.&lt;/p&gt;
&lt;h4 id=&#34;fiona-murphy-wiley&#34;&gt;Fiona Murphy, Wiley&lt;/h4&gt;
&lt;p&gt;Mentions PREPARDE - this is an ongoing project and set of activities.&lt;/p&gt;
&lt;p&gt;Adapting the publication model for publication is about adapting the existing model.&lt;/p&gt;
&lt;h4 id=&#34;jarosław-perzyński-polish-chamber-of-books&#34;&gt;Jarosław Perzyński, Polish Chamber of Books&lt;/h4&gt;
&lt;p&gt;Polish publishers do not think about growing, rather about surviving. There are two examples of alarming ideas from Poland. At the end of 2012 the polish ministry proposed that publishers mush transfer electronic rights, in this situation the government wanted to pay 50% of the cost, and have 100% control of the work - this would have been a disaster for the polish book industry.&lt;/p&gt;
&lt;p&gt;An other example is graphene research in 2012. The question is who is able to apply for the commercial use of this research. OA would mean that only rich states would be able to benefit from this work.&lt;/p&gt;
&lt;p&gt;He mentions a question about spying and connects this to open access, but I don&amp;rsquo;t understand what he said in this regard.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s mentioned by the chair that opening things is a way to prevent spying.&lt;/p&gt;
&lt;h4 id=&#34;eefke-smit-international-association-of-scientific-technical-and-medical-publishers-stm&#34;&gt;Eefke Smit, International Association of Scientific, Technical and Medical Publishers (STM)&lt;/h4&gt;
&lt;p&gt;Publishers welcome this imitative. It is no secret to us how much of a hot topic research data is.&lt;/p&gt;
&lt;p&gt;If you want to reuse data, they must be understandable. There needs to be a connection between research data and publication. This addresses some of the fears that researchers would have in terms of researchers being afraid of others misusing their data.&lt;/p&gt;
&lt;p&gt;On the culture of sharing - it is again very important that data is integrated with publications.&lt;/p&gt;
&lt;h4 id=&#34;anita-de-waard-elsevier-data-collaborations&#34;&gt;Anita de Waard, Elsevier, Data Collaborations&lt;/h4&gt;
&lt;p&gt;Elsevier∫ - we are a large publisher (one of the best lines of the day).&lt;/p&gt;
&lt;p&gt;Storing/annotation/curation is not the same thing as sharing.&lt;/p&gt;
&lt;p&gt;They do advocate the creation of data catalogues, so that if even data is not shared, there could be a role for data catalogues, so you can at least discover what there is, who has it, and what the rights are around it.&lt;/p&gt;
&lt;p&gt;where do you store the data? there are three types of repositories&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;generic repositories&lt;/li&gt;
&lt;li&gt;domain specific repositories&lt;/li&gt;
&lt;li&gt;institutional repositories&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They are interested in developing machines accessible formats for interrogating the data&lt;/p&gt;
&lt;p&gt;How do we enhance data awareness?&lt;/p&gt;
&lt;p&gt;Need to look at how the researchers are working now, need to develop tools that can store data at the point of capture - the older self wants to reuse the data created by the younger self.&lt;/p&gt;
&lt;p&gt;Allow the researcher insight into why, and the extent to which data was reused.&lt;/p&gt;
&lt;p&gt;They would like to suggest the creation of a shared network of best practice in data sharing.&lt;/p&gt;
&lt;h2 id=&#34;library-perspective&#34;&gt;Library perspective&lt;/h2&gt;
&lt;h4 id=&#34;paul-ayris---liber&#34;&gt;Paul Ayris - LIBER&lt;/h4&gt;
&lt;p&gt;Libraries should retool to be able to support data management. LERU is compiling a roadmap for the impact of research data, this will be available end of 2013. It will include looking at costs - the first question that any vice-chancellor will ask.&lt;/p&gt;
&lt;p&gt;LERU believes that there are boundaries, not all data can be bade open on day one, but they believe that the default position should be open and not closed.&lt;/p&gt;
&lt;h4 id=&#34;thomas-bourke-european-university-institute-florence&#34;&gt;Thomas Bourke, European University Institute (Florence)&lt;/h4&gt;
&lt;p&gt;Mirrors what Paul says. There are huge differences between financial economic data and development economic data. The question of scope is a key question. Libraries have established quality control mechanisms around publication, they might be able to provide something like this on the data side.&lt;/p&gt;
&lt;p&gt;The source of the data should be captured, is it original data, derived data, modified data?&lt;/p&gt;
&lt;p&gt;Would be good if the commission could hear from publishers of primary data - Bloomberg, Thompson Reuters.&lt;/p&gt;
&lt;h4 id=&#34;michael-franke-max-planck-digital-library-3&#34;&gt;Michael Franke, Max Planck Digital Library 3&lt;/h4&gt;
&lt;p&gt;What types of data should be open?
It is important to find intelligent ways to assess how valuable a data set is, or how appropriate it is to archive, rather than recreating.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Continuous monitoring of data reuse could tell you whether a data set should be kept any longer. It should find out how well a data set preforms in terms of reuse
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;On data awareness and the culture of sharing
This contains a motivation problem for the researchers. There is hardly any
individual incentive to share this. One way to overcome this dilemma is a reward system for sharing data. Such a system could go hand in hand with the San Francisco Declaration on Research Assessment (&lt;a href=&#34;http://am.ascb.org/dora/&#34;&gt;DORA&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;(at this point in the evening we are starting to heavily retread over points discussed earlier, so I am cutting back on note taking).&lt;/p&gt;
&lt;h4 id=&#34;discussion-1&#34;&gt;Discussion&lt;/h4&gt;
&lt;p&gt;On quality - libraries don&amp;rsquo;t do peer review. They do do some selection, and they ensure that the stuff you get in is the stuff that you will get out later on. Increasingly we are seeing the use of digitised material as research data collections. The libraries is increasingly becoming the data provider as well as the archiver of the data.&lt;/p&gt;
&lt;p&gt;The issue of text and data mining conversation around licensing within the EU and the breakdown of those discussions is raised as a point of discussion.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>EC consultation on Open Data - my presentation.</title>
      <link>http://scholarly-comms-product-blog.com/2013/07/02/ec-open-data-consultation-our-view/</link>
      <pubDate>Tue, 02 Jul 2013 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2013/07/02/ec-open-data-consultation-our-view/</guid>
      <description>&lt;p&gt;The following is the written representation that I made to the EC hearing on Open Data on behalf of Co-Action publishers, Copernicus Publications, eLife, F1000 Research, FigShare, Frontiers, Open Books Publishers, PeerJ, the Public Library of Science, Ubiquity Press and Bloomsbury Qatar Foundation Journals (QScience). I had a five minute slot to present, and the key recommendations at the end of this written response formed the basis of that presentation. I added one slide at the end with a personal view on some of the challenges of getting researchers to share data.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#slides&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#written-representation&#34;&gt;Written representation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-can-we-define-research-data-and-what-types-of-research-data&#34;&gt;How can we define research data and what types of research data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#when-and-how-does-openness-need-to-be-limited?&#34;&gt;When and how does openness need to be limited?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-should-the-issue-of-data-re-use-be-addressed?&#34;&gt;How should the issue of data re-use be addressed?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#where-should-research-data-be-stored-and-made-accessible?&#34;&gt;Where should research data be stored and made accessible?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#how-can-we-enhance-data-awareness-and-a-culture-of-sharing?&#34;&gt;How can we enhance data awareness and a culture of sharing?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#key-recommendations&#34;&gt;Key Recommendations:&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;a-idslidesa-slides&#34;&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt; Slides&lt;/h1&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;h1 id=&#34;a-idwritten-representationa-written-representation&#34;&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt; Written representation&lt;/h1&gt;
&lt;h3 id=&#34;a-idhow-can-we-define-research-data-and-what-types-of-research-dataa-how-can-we-define-research-data-and-what-types-of-research-data&#34;&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt; How can we define research data and what types of research data&lt;/h3&gt;
&lt;p&gt;should be open?&lt;/p&gt;
&lt;p&gt;Research data are primary outputs of a research process that are intended to be incorporated into research communications as support for the claims of that research. As with other research outputs, research data can be valuable to others for the purpose of validation, confirmation, and critique of research, as well as for entirely new applications that were not considered by the original researchers. Research data can be in any format.&lt;/p&gt;
&lt;p&gt;Research data generated with the support of public funds is a public good and should be open by default. This means making it available in a format and with contextual information that makes it technically usable, with the legal rights that enable re-use in any field.  We endorse the concept of “Intelligent Openness” described in the report of the UK Royal Society “Science as an open enterprise”. Data needs to be accessible, intelligible, assessable and usable.  Given a default position that data should be open the more appropriate question to ask is where and how should that openness be limited.&lt;/p&gt;
&lt;h3 id=&#34;a-idwhen-and-how-does-openness-need-to-be-limiteda-when-and-how-does-openness-need-to-be-limited&#34;&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt; When and how does openness need to be limited?&lt;/h3&gt;
&lt;p&gt;Research data created with support from public funds should be open by default. Release of, and access to, research data should be limited in cases where making the data available will do more harm to the public good than restrictions of access. Such cases might include clinical data where personally identifiable information is included, data that reveal the location of critically endangered species, data that has potential to create a public health or security risk, poses a danger to the researchers themselves, or where the release of data would damage the conduct of the research itself.&lt;/p&gt;
&lt;p&gt;The appropriate approach to limiting openness will depend on the case in hand and should be subject to a risk assessment by appropriate experts. Methods for restricting access with a proven track record include, delaying publication for a defined period, providing access under specific conditions, or only to approved persons, or after review of a specific access request. Such systems need to be carefully considered and designed so that they hamper access as much as is appropriate but no more. We emphasise again that the default should be open, and such systems should be used only where they can be justified.&lt;/p&gt;
&lt;p&gt;The question of access to research data where there is a commercial contribution to their creation is a separate issue. Research data created by private interests is the property of the creator and can be shared in a way that advances their interests. Where public and private interests contribute to the creation, collection, or analysis of data it may be appropriate for the release of data (and other communications) to be delayed for some defined period. Such periods should be negotiated and defined in collaboration and grant agreements. We would recommend that the maximum such period should be one year after the conclusion of the project or two years after the creation of the data, whichever occurs first.&lt;/p&gt;
&lt;h3 id=&#34;a-idhow-should-the-issue-of-data-re-use-be-addresseda-how-should-the-issue-of-data-re-use-be-addressed&#34;&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt; How should the issue of data re-use be addressed?&lt;/h3&gt;
&lt;p&gt;If data is to be made available with the intention of maximising the economic impact and the public good created it is critical that re-use be enabled both technically and legally. Researchers should adopt best practice in the formatting and description of data and the Commission and other funders and community groups should support the creation, documentation, and communication of such community best practice. Data should be placed in the repositories and archives that best support the availability, discoverability and usability of the specific data. Data should be released under a license which maximises the potential for re-use and recombination of that data. The appropriate licenses are the Creative Commons CC0 waiver and CC BY copyright licenses. This approach is similar to that of “Intelligent Openess” described in the Royal Society Report.&lt;/p&gt;
&lt;p&gt;To create incentives for publicly funded researchers to maximise the re-usability of their research outputs it is important that the Commission and other funders adopt and develop approaches that measure re-use and require researchers to report on the re-use of their data. The Commission and other funders should engage with the emerging tools for tracking re-use and engagement of web resources, including data. These include initiatives and tools to support Data Citation, measures of data downloads, and online conversations around this data.&lt;/p&gt;
&lt;h3 id=&#34;a-idwhere-should-research-data-be-stored-and-made-accessiblea-where-should-research-data-be-stored-and-made-accessible&#34;&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt; Where should research data be stored and made accessible?&lt;/h3&gt;
&lt;p&gt;Research data should be made available in the place which best supports its use in a sustainable and reliable fashion. The question needs to be addressed on a domain by domain basis, as well as for specific data types. Funders play a critical role in supporting the infrastructure that makes data available, both its creation, and long term sustainability and shared systems and community infrastructures need to be put in place to support these services in the long term.&lt;/p&gt;
&lt;p&gt;It is generally not ideal for data to stored as supplementary data to published research papers on a publishers website. While we recognize that this is the current default for many domains of research we recommend a shift towards the housing of data in dedicated repositories, ideally specialised for specific data types and domains, but in any case focussed on the preservation, discoverability, and re-use of data as opposed to research papers. To ensure the connection between data and research communications that may be spread between a number of different repositories it is critical that effective and persistent citation systems are in place to link research to its supporting data.&lt;/p&gt;
&lt;h3 id=&#34;a-idhow-can-we-enhance-data-awareness-and-a-culture-of-sharinga-how-can-we-enhance-data-awareness-and-a-culture-of-sharing&#34;&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt; How can we enhance data awareness and a culture of sharing?&lt;/h3&gt;
&lt;p&gt;As noted above a critical aspect of enhancing data awareness and building a culture of data sharing is for funders to explicitly show that they value data sharing as a primary output of funded research. Specifically funders should engage with the emerging field of usage measurement and require evidence of re-use from researchers. An infrastructure that supports data citation and usage tracking is required to provide the underlying data to support this and we recommend and support the principles of the Amsterdam Manifesto on Data Citation.&lt;/p&gt;
&lt;p&gt;Funders should act as exemplars of data sharing by sharing their own data effectively and efficiently. In addition the creation of specific funding initiatives, such as Marie Curie Fellowships  that support the creation of data sharing platforms, or those providing data resources that improve reproducibility and re-usability, send a powerful message on the importance of this work to the Commission. In addition to acting as exemplars funders will also need to place achievable and appropriately scoped requirements on grant conditions to ensure effective data sharing. Such conditions should be clear, agreed, and most importantly auditable. These need to be combined with advocacy for data sharing, collection and promotion of success stories, and clear rewards for those leading the development of best practice in specific communites.&lt;/p&gt;
&lt;h2 id=&#34;a-idkey-recommendationsa-key-recommendations&#34;&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt; Key Recommendations:&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Publicly funded research data is a public good and should be shared effectively to maximise the benefits that arise from the public funding of research. To achieve this the default position must be that data is open.&lt;/li&gt;
&lt;li&gt;We endorse the concept of “Intelligent Openness” from the Royal Society report. Data must be accessible, legally usable, and technically usable to maximise the benefits from sharing.&lt;/li&gt;
&lt;li&gt;In specific and limited cases access to, or release of, research data should be restricted. There is existing and appropriate best practice in this space that can be adopted.&lt;/li&gt;
&lt;li&gt;To support and maximise the re-use of publicly funded research data funders should promote and require best practice in data sharing and explicitly monitor and reward those who can demonstrate the re-use of data generated.&lt;/li&gt;
&lt;li&gt;Research data should be made available from the place or places that best support its discovery and re-use, preferably in subject specific repositories. This will differ from domain to domain and between types of data.&lt;/li&gt;
&lt;li&gt;Support for the development of infrastructure that tracks the usage and discussion of data is crucial.&lt;/li&gt;
&lt;li&gt;Systems that support data citation and the tracking of usage are developing, require support, and should be retained in the public domain. We recommend and support the principles of the Amsterdam Manifesto on Data Citation&lt;/li&gt;
&lt;li&gt;Funders need to act explicitly to demonstrate that they value data sharing. This can be achieved through a) acting as exemplars of best practice in sharing their own data b) supporting those that demonstrate and embody best practice in datasharing and the development of new tools that support data sharing c) requiring data sharing as a condition of funding.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;http://www.force11.org/AmsterdamManifesto&#34;&gt;Amsterdam Manifesto&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://royalsociety.org/uploadedFiles/Royal_Society_Content/policy/projects/sape/2012-06-20-SAOE.pdf&#34;&gt;Science as an Open Enterprise&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Some thoughts on the Taylor and Francis Survey.</title>
      <link>http://scholarly-comms-product-blog.com/2013/04/27/tf-survey/</link>
      <pubDate>Sat, 27 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2013/04/27/tf-survey/</guid>
      <description>&lt;p&gt;I took a look at the &lt;a href=&#34;http://www.tandf.co.uk/journals/pdf/open-access-survey-march2013.pdf&#34;&gt;T&amp;amp;F survey&lt;/a&gt; with interest. I&amp;rsquo;m also very aware of the concerns and confusions that exist around licensing. I&amp;rsquo;m also aware of the &amp;ldquo;one size doesn&amp;rsquo;t fit all&amp;rdquo; argument.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll address the T&amp;amp;F survey first, and then I&amp;rsquo;ll briefly discuss CC-BY pros and cons, purely from the point of view of my own understanding of these issues - I might be very wrong on this.&lt;/p&gt;
&lt;p&gt;I believe there are two core flaws with the T&amp;amp;F survey.&lt;/p&gt;
&lt;p&gt;1.When asking about reuse and licensing, ownership of the work was presented as if the author owned the work. In some cases this may be true, but for the most part the work is owned either by the university or the grant funder. Most academics don&amp;rsquo;t thing that way, naturally, but this is the reason why funders can take a role in dictating licensing.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;The opportunity costs of the current system were not presented to authors in the survey, especially the aspect of financial gain that publishers make from the content that authors sign over. In effect the questions around licensing were biased in favor of the status quo.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Notwithstanding these issues, the survey had a large number of respondents, and you can&amp;rsquo;t look at those results and argue in any way that there is no concern amongst authors over the issue of licensing. For those of us who believe in the value of CC-BY it clearly sends a message that we have to up our game in explaining that value, and showing that value.&lt;/p&gt;
&lt;p&gt;On CC-BY, it really is the case that there is a potential lose of revenue to both publishers and authors, in that commercial value can be created outside of the control of either the publisher or the author. But that&amp;rsquo;s kind of the point! In reality the commercial value that will be created is very unlikely to every be actually created by publishers or authors. Saying that you don&amp;rsquo;t want this to happen is a &amp;ldquo;dog in a manger&amp;rdquo; kind of approach. It&amp;rsquo;s selfish, and short sighted. It&amp;rsquo;s the tragedy of the commons. To promote an argument like that authors might miss an opportunity to generate revenue, is to promote fear uncertainty and doubt - FUD. There will be a few cases where some authors will miss out, but the counter balance of making this content open to the commons far outweighs this downside.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ENCODE - an example of open publication and data integration.</title>
      <link>http://scholarly-comms-product-blog.com/2013/01/30/euan-birney-data-publishing-talk-plos-elife/</link>
      <pubDate>Wed, 30 Jan 2013 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2013/01/30/euan-birney-data-publishing-talk-plos-elife/</guid>
      <description>&lt;p&gt;On Monday the 14th of January we met at the PLOS offices in Cambridge to hear a talk from &lt;a href=&#34;http://genomeinformatician.blogspot.de/&#34;&gt;Euan Birney&lt;/a&gt; on lessons learned from publishing data rich publications though the &lt;a href=&#34;http://encodeproject.org/ENCODE/&#34;&gt;encode project&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This was the first time that Euan was far less worried about the print, and far more worried about how well the online version was going to work.&lt;/p&gt;
&lt;h2 id=&#34;dimensions-of-the-project&#34;&gt;Dimensions of the project&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;5 TeraBases&lt;/li&gt;
&lt;li&gt;1715 times the size of the Human Genome&lt;/li&gt;
&lt;li&gt;3k experiments&lt;/li&gt;
&lt;li&gt;410 authors on the main paper&lt;/li&gt;
&lt;li&gt;6 high profile papers&lt;/li&gt;
&lt;li&gt;~35 companion papers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The output should not be thought of as papers, but as the raw data. The organisation and display of that data required non-paper publication methods.&lt;/p&gt;
&lt;h2 id=&#34;publishing-innovations&#34;&gt;Publishing innovations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Threads&lt;/li&gt;
&lt;li&gt;iPad app&lt;/li&gt;
&lt;li&gt;Interactive Figures&lt;/li&gt;
&lt;li&gt;Virtual Machine for making the data available&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;threads&#34;&gt;Threads&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;http://www.nature.com/encode/&#34;&gt;Threads&lt;/a&gt; are basically paragraph level tagging. It was built by creating a HTML5 app, and the core interactivity model is the same for the website and the iPad app. A big question in the mind of the authors was would this be useful. Would reading a paragraph and a figure from different papers be too incoherent without the overall narrative of a single paper? Euan was really excited when he talked to someone who had printed an entire thread, and she asked questions about the thread. He has had more people talking to him about their use of threads, over the number of people talking about interactive figures.&lt;/p&gt;
&lt;p&gt;One of the main points behind doing threads was to bring the companion papers together with the main papers. To make it work you needed to make all of the papers open access. This could just not be done without the papers being open access. The tagging was done by the authors, they created 13 word documents, and they copy and pasted and curated the threads into these word documents. (it sounds like the worst process in the world, but the reader doesn&amp;rsquo;t care, to Euan this is just an implementation detail). They were never going to be in a position to get the different publishers to align their internal XML tagging processes so they went with the dumbest method that was guaranteed to work.&lt;/p&gt;
&lt;p&gt;For Science, Nature etcetera, they needed to make a &amp;ldquo;Special Case&amp;rdquo; to make these papers open access to allow this to work. There is an opportunity for OA publishers to be able to say &amp;ldquo;this is just what we do&amp;rdquo;. CC-BY makes the rights aspect of this trivial.&lt;/p&gt;
&lt;h5 id=&#34;some-lessons&#34;&gt;Some lessons&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Could be supported by an open tagging and mash-up site.&lt;/li&gt;
&lt;li&gt;The tagging must be paragraph level, and not papers&lt;/li&gt;
&lt;li&gt;You need ownership of tags, at individual and group levels. You might possibly need to include hierarchies.&lt;/li&gt;
&lt;li&gt;The two way nature of this is interesting, threads point to papers, papers to threads.&lt;/li&gt;
&lt;li&gt;The ordering of the tags is interesting.&lt;/li&gt;
&lt;li&gt;A nice well executed site that scales in terms of UI is critical - what to do with thousands of threads?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This sounds like what &lt;a href=&#34;http://storify.com/&#34;&gt;storify&lt;/a&gt; does for Tweets and Blogs.&lt;/p&gt;
&lt;p&gt;In this scenario, should one just be publishing at the level of paragraphs and tags? Euan feels that the paper provide an important narrative structure.&lt;/p&gt;
&lt;p&gt;It was a decision that the threads did not require independent review, as each atom within them had already been reviewed.&lt;/p&gt;
&lt;p&gt;People have started to cite the threads via the URL of the thread, however Nature did not ask for DOI&amp;rsquo;s for the threads. (this would cause a problem for ISI and traditional citation counting practices).&lt;/p&gt;
&lt;h3 id=&#34;interactive-figures&#34;&gt;Interactive figures&lt;/h3&gt;
&lt;p&gt;This was not the most successful part of the project. There needs to be more investment into the widget creation process. There needs to be more standardisation.&lt;/p&gt;
&lt;p&gt;There was a lot of back and forth between the coders and the academics and the publishers on the creation of the interactive figures, but what does this actually give you? &amp;ldquo;It sort of doesn&amp;rsquo;t really change the world&amp;rdquo;. There could be a lot more if you could click on a data point, and drill down on that point. The authors spent a large amount of time, and created some JavaScript prototypes, and these were then refined, but they didn&amp;rsquo;t really get to a thing that they thought was useful.&lt;/p&gt;
&lt;p&gt;They tried to make every figure interactive, but they ran out of time, and perhaps lost a bit of focus around this.&lt;/p&gt;
&lt;p&gt;There needs to be an open library of JS with standard formats to go into the backed.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.biojs&#34;&gt;Biojs&lt;/a&gt; is a ground-up EBI widget group, something like this which is domain specific, targeted at specific standard data sets, could be more useful than doing custom visualisations.&lt;/p&gt;
&lt;h3 id=&#34;virtual-machine&#34;&gt;Virtual machine&lt;/h3&gt;
&lt;p&gt;This is the ultimate materials and methods for computational papers. This is not a sophisticated thing to publish, it&amp;rsquo;s just a big file. Euan is very proud of this as a scientist. It should be trivial to do from a publishing point of view.&lt;/p&gt;
&lt;p&gt;The only thing you have to worry about is the size of your supplementary file limits. Ideally one should deposit it before review, and allow reviewers to spin up for review.&lt;/p&gt;
&lt;p&gt;The thing that has been most used have been the tarballs for each individual figures, rather than the VM of the entire machine.&lt;/p&gt;
&lt;p&gt;The biggest piece of computation can&amp;rsquo;t be placed into a single VM, but they did put in one example pipeline (a bit like blue peter - here is one we did earlier).&lt;/p&gt;
&lt;h3 id=&#34;data-integration&#34;&gt;Data integration.&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Majority of structured databases have deposit during review&lt;/li&gt;
&lt;li&gt;A few systems allow reviewer accounts, but they are not much used. The VM concept might be a better approach.&lt;/li&gt;
&lt;li&gt;BioStudy database is piece of re-factoring for all data types - a new project coming out of the EBI in 2013.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;biostudy&#34;&gt;BioStudy&lt;/h3&gt;
&lt;p&gt;This is a new initiative from EBI as a way to allow the weaving of multiple data intensive studies together. Each will have it&amp;rsquo;s own access number. These could be tied to individual papers, but not always.&lt;/p&gt;
&lt;h3 id=&#34;lessons&#34;&gt;Lessons&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;This kind of approach only makes sense if it&amp;rsquo;s Open Access.&lt;/li&gt;
&lt;li&gt;The interactive figures will be harder to run if you have to make the data access hack proof.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At the outset, most people were interested in the interactive figures, the threads were known about, but not the main focus, however in the end the threads have been more successful. When they started curating threads they started out with about 26, these were collapsed down to 13 final threads. There was a balance between thin threads and thick threads, where you could think of a thin thread of being a hyper-focussed thread.&lt;/p&gt;
&lt;h3 id=&#34;message&#34;&gt;Message&lt;/h3&gt;
&lt;p&gt;He now thinks that digital publishing is truly the future, and for open access publishers this kind of thing should be easy, but it is soul searching for closed access publishers.&lt;/p&gt;
&lt;p&gt;This really highlights the difference between free to read and free to use. You can only do this when you are in the free to use domain.&lt;/p&gt;
&lt;p&gt;It has to be done generically, it can&amp;rsquo;t be done in a publishing house kind of way, as the science is not published though just one publishing house.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Going for gold, open access debate.</title>
      <link>http://scholarly-comms-product-blog.com/2012/10/02/icoa-going-for-gold/</link>
      <pubDate>Tue, 02 Oct 2012 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2012/10/02/icoa-going-for-gold/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;, audio of the meeting is now &lt;a href=&#34;http://figshare.com/articles/Open_Access:_Going_for_Gold_/96158?utm_term=%23oa&amp;amp;utm_source=twitterfeed&amp;amp;utm_medium=twitter&#34;&gt;available on figshare&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Last Thursday I attended the &lt;a href=&#34;http://www3.imperial.ac.uk/humanities/sciencecommunicationgroup/scicomm%20forum&#34;&gt;SciCommForum&lt;/a&gt; debate &amp;ldquo;&lt;a href=&#34;http://www3.imperial.ac.uk/newsandeventspggrp/imperialcollege/eventssummary/event_5-9-2012-16-32-1&#34;&gt;Open access: going for gold?&lt;/a&gt;&amp;rdquo; held at Imperial College. Below are my notes from the event. The notes are fairly raw, and not comprehensive.&lt;/p&gt;
&lt;p&gt;The debate is going to be looking at open access in the context of the &lt;a href=&#34;http://www.rcuk.ac.uk/Pages/Home.aspx&#34;&gt;RCUK&lt;/a&gt; &lt;a href=&#34;http://www.rcuk.ac.uk/media/news/2012news/Pages/120716.aspx&#34;&gt;policy&lt;/a&gt;, it is being hosted by &lt;a href=&#34;https://twitter.com/Richvn&#34;&gt;Richard Van Noorden&lt;/a&gt; (RVN), &lt;a href=&#34;https://twitter.com/MarkRThorley&#34;&gt;Mark Thorley&lt;/a&gt; (MT) from RCUK, and &lt;a href=&#34;http://occamstypewriter.org/scurry/&#34;&gt;Stephen Curry&lt;/a&gt; (SC).&lt;/p&gt;
&lt;p&gt;MT opens by making the point that RCUK want to make research open to the largest number of people possible, including SMEs who might want to exploit the literature in order to drive innovation and growth. It&amp;rsquo;s clearly not just about being able to only read the literature. He mentions that the policy is being misinterpreted, in that some people say the policy says researchers must publish in Gold. They have a strong preference for Gold, but they don&amp;rsquo;t restrict publishing via the green route.&lt;/p&gt;
&lt;p&gt;MT says they are going to announce the amount of money that they will be making available in the Autumn (probably not tonight then). MT says RCUK will make publishing the funding information of the published paper a requirement. (I recommend using the &lt;a href=&#34;http://dtd.nlm.nih.gov/archiving/tag-library/3.0/n-mjv0.html&#34;&gt;funding-group&lt;/a&gt; tag within the NLM tag suite).&lt;/p&gt;
&lt;p&gt;SC steps up to discuss some of the potential issues about the policy. SC points out that most movements (EU, US), are mainly driving towards green (I didn&amp;rsquo;t know this). I think SC raises the reasonable question about wanting to know about the time-scale and costs for the transition. He does a good job of laying out the general contra argument.&lt;/p&gt;
&lt;p&gt;Time for questions.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;How will the money be paid?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The answer is pretty vague. The question of what happens when the money runs out also gets a pass. I suspect that the answers to these questions are just not known yet, but I think it is better to try this, then to not try it for lack of answers to some of these questions. MT stresses that the money will go to institutions, and those institutions will have the freedom to arrange matters as they see fit. It is noted that HEFCE is pro OA. OA is a very important thing that has to happen, and that there will be transition costs. They expect institutions to find the funding form within their research budgets.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;SC asks if people have thought about how to manage the transition costs?&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;He points out that in the long run an APC model will be cheaper because the costs will be transparent. We will get better value for money. I think he is implying that the moral imperative of getting to a system that is more efficient in terms of costs, means we need to have a plan to get over the transition hump asap.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Jan Velterop says if Uk negotiates on a national scale on subscriptions, that you might be able to negotiate&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;He says that if a librarian walks away from a negotiation from a publisher, the librarian is in trouble. If a country walks away form a negotiation with a publisher, the publisher might be in trouble. (on the point of people who know how to negotiate, I always find the Dutch very good at this). JV makes the point that if you could bulk negotiate, the savings from this could pay for the transition costs. MT says that all of the data will be made open and available on the costs for APCs and which publishers will be getting which funds.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;David Prosser asks &lt;strong&gt;If a researcher wants to publish in Journal X, and X is very expensive, and the researcher decides to go via the Green route, but the green option of that journal is not within the RCUK policy, then what happens?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;MT says its clear, the researcher must use a compliant route. MT says they want to create a market in the APC market, up to now it has been a free route, it seems to researchers like there is a money fairy. By exposing these fees you expose the market.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;There is a question about &lt;strong&gt;why they don&amp;rsquo;t tweak the policy to require Green after 6 months?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;MT says that there will be unintended consequences of any policy, but obviously they don&amp;rsquo;t know what those would be, otherwise they wound not be unintended. The current version of the policy is a start of a journey, they will review the policy, but now they are not in a position to start tweaking their policy.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;RVN says that in the UK there is no underlying green OA mandate.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;This differs from outside of the UK, and that is seems to be downplaying the importance of repositories. The NIH has the policy that everyone must have their research in a repository.(I would say that PMC is not really a good representative of a typical &amp;ldquo;repository&amp;rdquo;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;MT says that he a Robert Kiley have written to the top 60 publishers, by volume of output from RCUK funding, to ask whether and how they will become compliant to the policy. About 50% have responded to date, and many have explained what they will do. Of those that have replied, 82% of the respondents will offer a CC-BY. One US publisher has said no to CC-BY, but will allow green after 6 months.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;At this point in the evening I started to respond to some of the questions, so my notes do not give good coverage of the debate past this point.&lt;/em&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An astrophysics researcher mentions the ArXiV. IMO a key issue of the ArXiV is that it raises the question of what value do the &lt;a href=&#34;http://scoap3.org/&#34;&gt;scoap3&lt;/a&gt; charges actually provide. It seems naively obvious that they are only providing the stamp of peer reviewed publication, and that they are providing hardly any value on top of that.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;The issue of success or otherwise of the institutional repository movement is mentioned, and it is proposed that the repository movement has not succeeded due to it not being around for a long time, but the comment was made in the context of preprints and preprint servers in the life sciences. IMO that preprints in life sciences have not worked is due to social and not technical reasons. .&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Will RCUK monitor the quality of the journals that research will be published in?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;No. It is not the role of the funder to check on this. It is up to researchers to publish in the most appropriate venues for their research.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;What of repositories?&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;MT: We are not downplaying the role of repositories. They have a crucial and long term role to play.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There were some more interesting points raised, but at this point I was unable to keep up with taking notes, I believe that the audio of the debate will be published online.&lt;/p&gt;
&lt;h2 id=&#34;my-conclusions&#34;&gt;My conclusions.&lt;/h2&gt;
&lt;p&gt;It seemed to me that two big questions came up throughout the debate. 1. what happens when the money runs out for supporting the policy? 2. What about repositories and green OA.&lt;/p&gt;
&lt;p&gt;I think 1 is not answerable, and I think that any answer given now would need to be changed according to changing circumstances, and according to how the policy works out. We might like to have a nice roadmap and answers laid out, but the world is a messy place. I applaud RCUK for making this push. They will learn a lot more by trying this, than by almost any other action that they could take.&lt;/p&gt;
&lt;p&gt;For 2, I have a hunch that what is going on here is that the decision comes out of a desire to create a transparent and more innovative market, ahead of almost any other concern. If one were to mandate green OA, out of the box, one would be putting an extreme distortion onto an existing marketplace, from outside. Now, I believe that this marketplace is ripe for disruption, and that STM publishers often fail to articulate exactly what value they bring to the table, but if you are a government that subscribes strongly to the view of market forces being the correct mechanism for driving innovation and efficiency, then you might want to look for a policy that expands the potential number of market players, and unleashes frees market competition to work effectively between the existing players. Mandating green OA does not place STM publishers in true competition with each other, in a way it meddles with an existing industry without offering robust alternatives, and it is possibly more interventionist that a conservative philosophy may stand to support.&lt;/p&gt;
&lt;p&gt;One of the problems that the new policy begins to get to the heart of is the non-open nature of the STM market. This market is not an open and fair one due to confidentiality clauses between publishers and libraries over subscription fees. Driving towards an APC model resolves this, and starts to put publishers in direct competition with each other on the APC fees. Now they have to explain exactly why publishing in their journal costs twice or three times that of publishing in a competitors journal. The scientists may never actually pay these fees themselves, but perhaps they will start thinking about them.&lt;/p&gt;
&lt;p&gt;Requiring content to be made CC-BY expands the potential numbers of players in the market who can offer services. I know that STM publishers have been working on bringing innovations to their content, but perhaps by having more players active, they will have to work a bit harder. The government has been making noises about the digital economy for a long time, and it might be no coincidence that this policy has opted for CC-BY licence while the UK is home to some of the best research groups on data mining, the semantic web, and has been the home of many innovative products in the STM space.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The slow web, more thoughtful experiences.</title>
      <link>http://scholarly-comms-product-blog.com/2012/08/22/the-slow-web/</link>
      <pubDate>Wed, 22 Aug 2012 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2012/08/22/the-slow-web/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been concerned for a few years about the flow of data that we are producing, and how to handle the angst of not being able to keep up with everything, ever. I think it started when I became a very heavy user of google reader back in 2006 or so.&lt;/p&gt;
&lt;p&gt;There is little doubt that the web is moving more in this direction, Anil Dash recently &lt;a href=&#34;http://dashes.com/anil/2012/08/stop-publishing-web-pages.html&#34;&gt;called for people to stop creating web pages&lt;/a&gt;, and to start creating only streams.&lt;/p&gt;
&lt;p&gt;The angst of being awash in a stream of data is that you sometimes feel a little like you are drowning in it. For the last few years, I&amp;rsquo;ve been using twitter and more recently hacker news, as my main filters. If a link bubbles up enough times on either of those places it&amp;rsquo;s probably worth checking out, but it&amp;rsquo;s an inelegant solution.&lt;/p&gt;
&lt;p&gt;On the flip side, I&amp;rsquo;ve been concerned about the creation of artificial scarcity as a method of increasing impact factors and prices within the STM industry. By setting a high bar, for acceptance, you make it harder to get in, and you hope that exclusivity extends to getting more attention to the papers that make it in.&lt;/p&gt;
&lt;p&gt;So here I am worrying about both ends of the spectrum. It seems perhaps wrong to have an artificial bar on publishing science, as science is so important, and posterity is usually the best judge, but on the other hand throwing away the bar pitches you into the stream, and perhaps allows you to drown a little.&lt;/p&gt;
&lt;p&gt;We eLife I we are removing artificial scarcity in terms of having no limits on page numbers, or numbers of articles published per unit time, on the other hand, we are setting a very high bar in terms of quality. It&amp;rsquo;s fascinating to be at the heart of an initiative that is tackling these questions directly. I&amp;rsquo;m learning a lot.&lt;/p&gt;
&lt;p&gt;I don&amp;rsquo;t know what the answer is yet. I know I want to formulate the question in my own words a bit more, but in the last week a thread of a thought has intrigued me. I read a post about a new music service that has just launched. The service is called &lt;a href=&#34;http://www.thisismyjam.com/&#34;&gt;this is my jam&lt;/a&gt;, and the philosophy behind it is &lt;a href=&#34;http://www.alistapart.com/articles/everything-in-its-right-pace/&#34;&gt;beautifully described&lt;/a&gt; by one of the co-founders.&lt;/p&gt;
&lt;p&gt;The big idea is that they ask members to nominate one new song a week, no more. If you follow a dozen people, you will get the thoughtful recommendations of about an album&amp;rsquo;s worth of music a week, just the right amount to consume in that time. By thinking about the value of the content, and how long an ideal amount of time is to interact with that content, they have created a service which tries to give you the time to experience the content, they are trying to create a more thoughtful experience.&lt;/p&gt;
&lt;p&gt;Perhaps there are some lessons here on how we can create a curatorial experience for the research literature, perhaps not. One of the key differences here is that what I listen to from a musical point of view is highly optional. What I read in terms of the literature is deeply connected to the eventual outcomes of success as an academic. There are far fewer songs in the world than there are scientific publications (about an order of magnitude less). The rate of song creation is much lower than the rate of scientific article creation, so a slower approach to presenting the literature could end up damaging the researcher. On the other hand, the vast majority of research that is published is either wrong or irrelevant, or both.&lt;/p&gt;
&lt;p&gt;I want a system that supports researchers, that does not make them fearful of the vast quantity of information that is out there, that can deliver timely quality recommendations, and humane experiences for interacting with that information.  Interwebs, show me your ideas, throw me your suggestions, diffuse the solution towards me through tweets, the mailing lists, and news sites, craft the answer and allow it to emerge, partially formed, and ready to be moulded into a thing that we can make to help, to try to help.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The cost of production</title>
      <link>http://scholarly-comms-product-blog.com/2012/07/12/the-cost-of-production/</link>
      <pubDate>Thu, 12 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2012/07/12/the-cost-of-production/</guid>
      <description>&lt;p&gt;Last week an interesting discussion on &lt;a href=&#34;http://occamstypewriter.org/scurry/2012/07/02/open-access-who-pays-the-copy-editor/&#34;&gt;the cost of copy editing&lt;/a&gt; popped up over on Stephen Curry&amp;rsquo;s &lt;a href=&#34;http://occamstypewriter.org/scurry/&#34;&gt;blog&lt;/a&gt;. In addition the
&lt;a href=&#34;http://comments.sciencemag.org/content/10.1126/science.1220395#comments&#34;&gt;comment thread&lt;/a&gt; at the recent Science &lt;a href=&#34;http://www.sciencemag.org/content/335/6074/1279&#34;&gt;editorial&lt;/a&gt; seems to make this post somewhat timely. I used to manage the copy editing of a good portion of physical science related content from Springer from 2002 &amp;ndash; 2005. I&amp;rsquo;m also currently in the process of setting up a new online-only journal.&lt;/p&gt;
&lt;p&gt;Back in the early 2000s we were getting pages copy edited for about $20 per page. Right now we are looking at costs of between $10 and $35 dollars per page depending on the level of copy editing required, so for a 10 page article the cost could come in at about $350, or about 10% of the cost of an OA publication in an outlet like PLoS.&lt;/p&gt;
&lt;p&gt;High quality copy editing means significant work on grammar and clarity. The kind of functional checking for things like consistency in figure labeling, and so forth, is cheaper, and many typesetters have tools that can augment the checking for this kind of consistency: &lt;code&gt;s1,$/Figure/Fig\./&lt;/code&gt;.
If the burden of these costs were pushed back to authors, then authors who know that their English is sub-par, would have an incentive to get their language checked prior to submission. This certainly happens in some labs, and there are &lt;a href=&#34;http://webshop.elsevier.com/languageediting/&#34;&gt;many services&lt;/a&gt; available for authors to pay to get this work done.&lt;/p&gt;
&lt;p&gt;As we know most articles are not read much, and are cited less, even in journals such as Nature. If we could chose to on copy edit the items of the literature where it is going to provide significant value, I believe there would be an economy of effort. It is somewhat of a catch-22 in that one would want to apply the effort to articles that will attract attention, without knowing ahead of time which articles will attract attention. Wikipedia represents an interesting model for aligning effort and attention. There is a &lt;a href=&#34;https://strategy.wikimedia.org/wiki/Proposal:Journal_(A_peer-review_journal_to_allow/encourage_academics_to_write_Wikipedia_articles)&#34;&gt;proposal to create a wikipedia journal&lt;/a&gt;, and in such a scenario one would be able to do sub editing post-publication.&lt;/p&gt;
&lt;p&gt;I believe that the cost of copy editing should be placed on the authors, and that it does not represent an overly significant value add that publishers bring to the dissemination process. I feel that in terms of issues that we have with the scientific literature, lack of access to underlying data is a much bigger problem, than low-quality articles being difficult to read.&lt;/p&gt;
&lt;p&gt;In terms of the cost of production of scientific content, I feel the discussion around copy editing is a red herring. There are a couple of new initiatives that are really interesting. &lt;a href=&#34;http://peerj.com/&#34;&gt;PeerJ&lt;/a&gt; is trying a model of charging $100 per year per author, &lt;a href=&#34;https://www.scholasticahq.com/&#34;&gt;scholastica&lt;/a&gt; are charging $10 per submission. This question of where we will get to if charges normalise towards the $1500 dollar mark seems to me to be missing the point. Creating content on the web, especially where authors are not paid for their work, could be substantially reduced to the level of a few dollars per article. This would be disruptive for the existing publishing industry. As Jason Hoyt points out in a [recent presentation][ffd], this is also a model that the incumbents would be almost certainly unable to follow.
[ffd]: &lt;a href=&#34;http://www.slideshare.net/jasonhoyt/a-framework-for-disruption&#34;&gt;http://www.slideshare.net/jasonhoyt/a-framework-for-disruption&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;What about maintaining high quality? Well, there are a number of confounding factors with the current system. One of these factors is that the cost of production is usually separated from profits for STM content. One of the reasons for this is that the market is non-transparent. Contracts with libraries usually include a non-disclosure clause, so no one really knows how much other customers are paying for the same product. Vendors providing typesetting or hosting services have the same non-disclosure clauses for publishers, so publishers don&amp;rsquo;t know how much their competitors are paying for production costs.&lt;/p&gt;
&lt;p&gt;You can get rid of the first of these by having article processing fees, rather than a subscription model. You can get rid of the second of these if you invest in building your own publishing infrastructure, like scholastica have done, and PeerJ are doing.&lt;/p&gt;
&lt;p&gt;There is still the issue of reputation, and what we call quality of production. To get well structured research content one currently ideally wants the output to be in NLM XML, and that does require some processing of the input from the authors. I believe that structured editing tools can be built that will provide a compelling experience for authors, and that will create content that can be ready to be online immediately. We have been waiting for such tools for a long time, and there is a history of attempts to create such tools, but I remain optimistic. One of course needs appropriate incentives to authors to take up such tools, faster publication times, and cheaper publication costs may provide sufficient incentives.&lt;/p&gt;
&lt;p&gt;In this future, it may be possible to completely separate the cost of production from the activities that are associated with signification of high scientific quality, a grand un-bundling of the roles of the current STM publisher. This kind of scenario is already playing out in the print news media. I think it is an attractive future that offers benefits to research, and it is one that I look forward to.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Some Thoughts on Peer Review and Altmetrics</title>
      <link>http://scholarly-comms-product-blog.com/2012/06/18/some-thought-on-peerreview-and-altmetrics/</link>
      <pubDate>Mon, 18 Jun 2012 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2012/06/18/some-thought-on-peerreview-and-altmetrics/</guid>
      <description>&lt;p&gt;The upcoming &lt;a href=&#34;http://altmetrics.org/altmetrics12/&#34;&gt;altmetrics&lt;/a&gt; meeting, and a submitted abstract by &lt;a href=&#34;http://www.csid.unt.edu/about/People/barr.html&#34;&gt;Kelli Barr&lt;/a&gt; prompted me to note down some of my own thoughts on peer review and altmetrics. I would love to make it over to the meeting, but with just a few days now before my first child is born, it ain&amp;rsquo;t gonna happen.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ve not read Kelly&amp;rsquo;s paper, but after reading the abstract my take home message from it would be something along the lines of &amp;ldquo;don&amp;rsquo;t replace peer review with altmetrics because you will just replace one bias with another, and at least with peer review the bias is contained within the academic community&amp;rdquo;&lt;/p&gt;
&lt;p&gt;I personally don&amp;rsquo;t see any conflict between altmetrics and peer review. If anything, in a nutshell, I believe that altmetrics can help to create an augmented peer review, placing better information into the hands of those in the community who are making judgements on impact, on likelihood of future productivity, on who should get money.&lt;/p&gt;
&lt;p&gt;The abstract starts with&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The altmetrics community, according to its manifesto, has grown around the assumption that the use of peer review as a filtering mechanism for quality scholarship has outlived its usefulness in the changing landscape of scholarly communication (Priem et al. 2010).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I would classify myself as part of this community, but my interest has not grown out of the assumption listed here. The idea that strongly motivates me is the following&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The web, and the processing power associated with it, should provide better information to those interested, about the impact of research, and the existence of relevant and related research to the question at hand to the researcher. In this way the web should be a tool to reduce informational asymmetry.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The best current example of how this does not happen is the continued use of the impact factor as a proxy for impact. I think that it&amp;rsquo;s a case of since we can do this, and we can make systems that support the research process, we damn well should. There are many branches of thought that follow on from this: supporting peer review, recommendation of articles, graphing the flow of ideas within the literature, extending information about relevance to communities outside of the main research community for example patients.&lt;/p&gt;
&lt;p&gt;At it&amp;rsquo;s heart though, I&amp;rsquo;m a believer that as in other areas of human interaction, the web has the potential to help with some of the core activities of research, and not just by making it possible to read pdfs online.&lt;/p&gt;
&lt;p&gt;The abstract continues&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I argue that the altmetrics community should resist the attempt to supplant peer review with a host of altmetrics, no matter how diverse.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I totally agree with this. I think though, that peer review is not a single thing. It&amp;rsquo;s a description for a very variated process. When I was managing a number of journals I saw different types of peer review in process. Sometimes for small journals it consisted of getting the nod from the editor in chief. Sometimes it consisted of polling a large number of researchers where the work was felt to be potentially very problematic, leading in the end to the dismissal of the editor in chief (I&amp;rsquo;m talking here about different people). For some instantiations of what we could call peer review, I&amp;rsquo;m fully confident that a quite different approach could work just as well. &lt;a href=&#34;http://peerj.com/&#34;&gt;PeerJ&lt;/a&gt; may be a move in this direction.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;d make the analogy to code review. The reason for code review, within a programming organisation, is that humans are fallible. There are lots of different ways of doing code review, but at it&amp;rsquo;s heart, it&amp;rsquo;s always used as a sense check against human infallibility. I think this is one of the important aspects of peer review. Altmetrics can only help with this as by providing more information to the person doing the peer reviewing, there is a bigger chance that that person may be alerted either to potential errors, conflicts, or opportunities for interesting cross-pollination.&lt;/p&gt;
&lt;p&gt;My feeling on the following point&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Any evaluation scheme is simultaneously a system of incentives, and so assessing the impact of research according to a suite of altmetrics will inevitable steer research in particular directions, as peer review has done.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;is that the current data metrics: citations and the impact factor, are simply just very crude. The impact factor is a shit sandwich, and the only reason we continue to swallow it is that we don&amp;rsquo;t have open reproducible metrics. The impact factor itself if neither of these things. I think that projects like the &lt;a href=&#34;http://opencitations.wordpress.com/&#34;&gt;open citations&lt;/a&gt; are hugely important for helping to resolve this, and altmetrics itself. It&amp;rsquo;s early days, we have been talking about altmetrics since about the mid-2000&amp;rsquo;s, and early systems like &lt;a href=&#34;http://sourceforge.net/mailarchive/forum.php?forum_name=postgenomic-develop&#34;&gt;post-genomic&lt;/a&gt; gave an early taste, but I think it&amp;rsquo;s fair to say that systems that could really start to do something interesting have only been in existence for a little under five years now.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;but the judgments rendered will not be immune to the promotion of conventionality (i.e. groupthink, or cultural exclusivity on a larger scale), nor would it limit the volume of research published.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a fair comment, but my hope will be that by promoting a variety of metrics people will be able to pick the groupthink that they most like, rather than all having to stick with one. One could then imagine applying meta-analysis to application of altmetrics to uncover these biases.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Augmented Peer Review</title>
      <link>http://scholarly-comms-product-blog.com/2012/06/17/proposal-on-peer-review/</link>
      <pubDate>Sun, 17 Jun 2012 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2012/06/17/proposal-on-peer-review/</guid>
      <description>&lt;p&gt;Last year I was asked to contribute to a special issue on the evolution of peer review. I got quite excited about doing this, but then realised that I really didn&amp;rsquo;t have the time to write a paper. I&amp;rsquo;m not a practicing academic, I build products, and while at Mendeley I really had far too much on my plate to find the time to write up a paper. However the topic does interest me, and I am a strong believer that web scale technologies can help with the scientific communication process though a large number of avenues. I was looking through my folder of draft blog posts, and I found the skeleton of the proposal that I had started to put together. I&amp;rsquo;m posting this up now, including the editoral comments I got on the very rough draft. I remain lucky enough to be in a position to continue to build out the parts of the infrastrucutre that I think can help with all of this. I&amp;rsquo;ll probably not get around to finishing my thoughts on this proposal in exactly the way that I had been thinking about it last year, but I might flesh out some related thoughts over the coming months.&lt;/p&gt;
&lt;h1 id=&#34;delivering-an-augmented-peer-review-system-one-piece-at-a-time&#34;&gt;Delivering an Augmented Peer Review System, one piece at a time.&lt;/h1&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Rather than trying to replace peer review, I will argue that we should create an augmented version of the system. I  propose that web scale technologies can be used to deliver this Augmented Peer Review. I will describe some of the functions of the peer review system and show some of these can already be helped with existing systems, and describe how some of the other functions may be helped in the near future.
I will describe in detail how Mendeley has created a technology that can help with one core aspect of the peer review system, assisting in filtering the literature, and I will describe some of our upcoming plans to go further with this. I&amp;rsquo;ll describe some of the weaknesses of our approach and contrast these to weaknesses in the existing system. I will also describe some experiments that we are planning on running that may validate the predicative power of newly coined metrics. Finally I will argue that getting access to the citation and review graph is the biggest factor in holding back the improvement of the peer review system.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;There is little doubt that peer review is a topic of particular interest to academics. The Mendeley search catalog returns over 20 000 items with &amp;ldquo;peer review&amp;rdquo; in the title. There is a growing sentiment that &amp;ldquo;peer review is broken&amp;rdquo; (&lt;a href=&#34;http://www.the-scientist.com/templates/trackable/display/article1.jsp?a_day=1&amp;amp;index=1&amp;amp;year=2010&amp;amp;page=36&amp;amp;month=8&amp;amp;o_url=2010/8/1/36/1&#34;&gt;I hate your paper&lt;/a&gt; ), but broken or not, is it worth asking whether we can improve the current system in any way by applying social or technological changes to the practice of peer review? As a technologist I&amp;rsquo;m in the lucky position of working on systems that can have an impact on the peer review system from a technological point of view, and so those are the solutions that I will focus on in this paper.&lt;/p&gt;
&lt;h2 id=&#34;what-is-peer-review-anyway&#34;&gt;What is peer review anyway?&lt;/h2&gt;
&lt;p&gt;One of the great problems with offering solutions to the problem of peer review is that it is actually a many headed beast, a Hydra of a system. It is many different things to different people. Some of it&amp;rsquo;s core functions we can think of as being required, and others are more flexible depending on the policy of the peer review body.&lt;/p&gt;
&lt;p&gt;Generally Required:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A cross-check of correctness.
We assume that once past peer review an article is telling us something that is true about the world.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A cross-check of novelty.
We assume that the work has not been presented before, or by other people.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ensuring the correct link to related literature.
We assume that the reviewer will inform the author of any missing credits that the author should have included in their paper to prior work.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The creation of a pre-filtered body of literature, though the understanding of the filtering is not necessarily clear to the reader.&lt;br&gt;
It is assumed that the literature that passes through the gate-keeping of peer review represents a special body of knowledge that can be depended upon for quality and accuracy. For the vast majority of content that passes peer review this is probably true, but not for all.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Providing a record of prestige for the author.
Getting the &amp;ldquo;peer reviewd&amp;rdquo; hit is the cornerstone of building a career in academia. The more exclusive the journal, the better for the author.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Generally Optional:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Screening for suitability criteria for a given journal, either subject or impact based.
Some journals will reject a work not because the work itself is in any way wrong, but because the work does not fit the subject space of the journal. Other journals, notable the Nature stable of journals, will reject a work if the editor of the journal does not feel that the work is of sufficient &amp;ldquo;impact&amp;rdquo; or of general enough of a nature. The main Nature journal also has a policy of publishing works that relate to the physical sciences, and so does not accept literature in the computational or mathematical sciences. PLoS One takes exactly the opposite view on impact, and checks only for rigour in the reporting, however it also has a strong subject bias and does not publish in areas of pure physics.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An opportunity to help improve the quality of the work.
Though optional, many researchers put much time into helping to improve the quality of the manuscripts that they are reviewing. One of the main reported benefits of the peer review system is this improvement that authors feel of their work. This is almost entirely social, as there is rarely a published record of the reviewers comments.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An opportunity to build or block the creation of social connections though contributed work.
By contributing to the good peer reviewing of an article a researcher participates in the society of academia. This position can also be abused and used as a way to hinder the publication of competing researchers.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;augmenting-parts-of-the-functionality-of-peer-review&#34;&gt;Augmenting parts of the functionality of Peer Review&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;a cross-check of novelty&lt;/li&gt;
&lt;li&gt;ensuring the correct link to related literature&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both of these functions are under the pressure of the scale of the literature. In 2008 the Research Information Network assessed that researches in the UK read up to 260 articles each per year (Anon, 2008. Activities , costs and funding flows in the scholarly communications system in the UK Report commissioned by the Research Information Network ( RIN ) Full Report. , (May).). At this rate, assuming no further research were published, it would take a researcher approximately 77000 years to read their way through the items cataloged in PubMed. Nowadays even a well read well connected researcher can only hope to have a deep familiarity with a fraction of the literature, however even hundreds of millions of documents can be scalably compared with the kinds of data structures and algorithms that have been created for indexing the web, such as schema-less data stores like HBase, and algorithms like map-reduce.&lt;/p&gt;
&lt;p&gt;The productisation of checking for similarity in the literature has already begun with the introduction of the &lt;a href=&#34;http://www.crossref.org/crosscheck.html&#34;&gt;cross check&lt;/a&gt; service from cross ref
. Online services like the &lt;a href=&#34;http://www.biosemantics.org/jane/&#34;&gt;journal author name estimator&lt;/a&gt; can tell you what journals or articles are like the one you are currently writing . The next step will be to see such services integrated into the writing tools of the researcher.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;providing a record of prestige for the author&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is actually where the majority of this article will focus, and it discuss the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;altmetrics&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the work of Bollen et al&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;graph theory applied to impact&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tools for processing large graphs
The emergence of 3rd generation programming tools have driven the creation of libraries for managing graph structures that have realtivly humanly readable APIs. An example of this is &lt;a href=&#34;http://networkx.lanl.gov/&#34;&gt;networkx&lt;/a&gt;, a graph library in python. Robust scalable databases for processing large graphs have emerged, such as &lt;a href=&#34;http://neo4j.org/&#34;&gt;neo4j&lt;/a&gt;. The social web has driven the creation of very scalable systems for storing relational and non-relational data, such as &lt;a href=&#34;https://github.com/twitter/flockdb&#34;&gt;flockdb&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;readership statics, and compare Mendeley readership statistics to those provided by services such as Connotea&lt;/li&gt;
&lt;li&gt;the Mendely API and the type of queries that can be resolved on the back of it&lt;/li&gt;
&lt;li&gt;plans for future work around individual impact based on Mendeley statistics&lt;/li&gt;
&lt;li&gt;moving from a central data store to a distributed model, the web of linked data&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://altmetrics.org/manifesto/&#34;&gt;alt metrics manifesto&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.mendeley.com/research/diffusion-of-scientific-credits-and-the-ranking-of-scientists/&#34;&gt;Radicchi, F. et al., 2009. Diffusion of scientific credits and the ranking of scientists. Physical Review E, 80(5), p.11. Available at: http://arxiv.org/abs/0907.1050 [Accessed August 16, 2010].&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.mendeley.com/research/a-principal-component-analysis-of-39-scientic-impact-measures/&#34;&gt;Bollen, J. et al., 2009. A principal component analysis of 39 scientific impact measures. Methods, pp.1-19.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.mendeley.com/research/academic-search-engine-spam-google-scholar-s-resilience-against-it/&#34;&gt;Beel, J. &amp;amp; Gipp, B., 2010. Academic Search Engine Spam and Google Scholarâs Resilience Against it. Journal of Electronic Publishing, 13(3).&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.mendeley.com/research/nefarious-numbers/&#34;&gt;Arnold, D.N. &amp;amp; Fowler, K.K., 2010. Nefarious Numbers. , p.5. Available at: http://arxiv.org/abs/1010.0278 [Accessed October 6, 2010].&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;editorial-comments&#34;&gt;Editorial Comments&lt;/h2&gt;
&lt;p&gt;This abstract/outline contains a lot of highly relevant background and several good ideas and important observations. Services like Mendeley and Zotero are in a key position for providing the web-tools for realising open post-publication peer review. What I’m still missing in your outline is a coherent vision for how new papers can be openly evaluated by the community with these tools. Imagine you had infinite resources, what system for open evaluation of the scientific literature would you build?
I would like to invite you to contribute a full paper with the hope that you will describe a detailed vision for open evaluation (e.g. Mendeley users rating and reviewing papers and sharing their ratings and reviews). The vision should include a description of the required web-based infrastructure as well as mechanisms of motivating reviewers. Consider including a step-by-step description of the process by which a paper is evaluated. Make sure to address the design decisions listed in the email. Feel free to use figures to communicate the key concepts and processes of your vision. Please note that the innovative features that distinguish your approach from those described in other contributions to this special topic should be clearly communicated in the title, in the abstract, and through the headings and terminology used in the paper.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hindawi have an awesome reviewing system.</title>
      <link>http://scholarly-comms-product-blog.com/2011/10/06/hindawi-reviewing/</link>
      <pubDate>Thu, 06 Oct 2011 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2011/10/06/hindawi-reviewing/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.hindawi.com/&#34;&gt;Hindawi&lt;/a&gt; publishers is a really interesting outfit. They are an open access only publisher based in Egypt. They combing a fantastic use of technology with the ability to afford a large amount of human curation over the data that they use to streamline their publication and reviewing systems.&lt;/p&gt;
&lt;p&gt;One of their publishing vehicles is called the &lt;a href=&#34;http://www.isrn.com/journals/&#34;&gt;International Scholarly Research Network&lt;/a&gt; and at the recent &lt;a href=&#34;http://www.oaspa.org/coasp/&#34;&gt;coasp&lt;/a&gt; conference Paul Peters gave an overview of how their peer review system works, I think it&amp;rsquo;s genius. They take a received manuscript, look at all of the references in that manuscript, and then computationally compare overlap of the cited works with the works cited by their panel of potential reviewers. They identify up to five potential referees who have the closest discipline match, but with whom there is no conflict of interest. A review request is sent out, and the reviewers are asked should the work be published according to the ISRN criteria, or if not why not. If a qoura of reviewers accepts then the paper is published. If there is a disagreement, they all get to see each other&amp;rsquo;s comments and they have to come to an agreement on the final decision.&lt;/p&gt;
&lt;p&gt;When a paper is published the reviewers who supported the decision have their names published along side the paper so that the community can see the provenance of the peer review.&lt;/p&gt;
&lt;p&gt;When the decision is split I love the fact that they can see each other&amp;rsquo;s comments before making a final decision. This makes it easier for the community of reviewers to come to a concencus over what standard of reviewing to apply for the submissions to ISRN.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s a beautiful system, and it&amp;rsquo;s one where cantankerous reviewers can be identified quickly, and removed form the review request queue.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Megajournals</title>
      <link>http://scholarly-comms-product-blog.com/2011/10/03/megajournals/</link>
      <pubDate>Mon, 03 Oct 2011 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2011/10/03/megajournals/</guid>
      <description>&lt;p&gt;The idea of megajournals had not really formalised in my head before, but at the &lt;a href=&#34;http://www.oaspa.org/coasp/&#34;&gt;COASP&lt;/a&gt; meeting the talk was all about &amp;ldquo;&lt;em&gt;Megajournals&lt;/em&gt;&amp;rdquo;. [PLoSOne][plosone] is the archetype for this kind of journal, and it had not really struck me before as a huge revolution in the publishing industry, but after listening to a couple of days worth of talks on the topic I&amp;rsquo;m convincible.&lt;/p&gt;
&lt;p&gt;Megajournals are so called because they are structured to be able to publish many more articles than has been the normal practice with traditional journals. By my count there are currently about thirty five thousand academic titles. The vast majority of these titles are not indexed for impact, many of them are homebrew, and most of them are niche. They may publish a few issues per year, they may have a handful of articles per issue, and each one is mostly supported by a small community of researchers.&lt;/p&gt;
&lt;p&gt;The big boy journals; IOP titles, Nature, Science, Genetics, NEJM and so on, have productionised the process of publishing articles. They employ a large number of people to mange the journals and sometimes to do in-house peer review. They publish their issues weekly with a constant rolling online first systems. The big publishers have applied this model to bundle publishing across many titles. However all of these venues retain the sense of an issue. In addition many of them have explicit policies around the taste or flavour of the articles that they wish to publish.&lt;/p&gt;
&lt;p&gt;We live in a world of selectivity in academic publishing. The journals want articles that are constrained by size, or content, or topic or perceived impact. Every journal sets it&amp;rsquo;s stall on a combination of these criteria.&lt;/p&gt;
&lt;p&gt;The result is a high rejection rate. The work is not an appropriate topic, not impactfull enough, too long, to theoretical, not written be a friend of the editorial board, etc, etc. Nature&amp;rsquo;s rejection rate is famously greater than 98%. When a researcher gets rejected from a journal they generally have to go through the entire submission process again at another venue. It can take months for an article the make it&amp;rsquo;s way through the publishing pipeline before it gets to the final rejected state. Sometimes it can take years. Each time an article is submitted it needs to be reviewed. Reviewers need to be found, and they need to spend their time reading and commenting on that paper. If the article had been submitted to another journal before and rejected, then these reviewers are repeating the work that someone else had done. Waste waste waste waste, it&amp;rsquo;s a huge waste of time.&lt;/p&gt;
&lt;p&gt;This is a problem both for research (delayed time to publication, multiplication of reviewing effort), and a problem for journals with very high rejection rates as costs for maintaining a high rejection rate are large composed of the maintenance of a professional staff and the processing costs of dealing with a high number of manuscripts that the journal in the end is not actually going to publish.&lt;/p&gt;
&lt;p&gt;For a long time now people have been talking about innovating the peer review system. People have been talking about how open access is going to solve all of these issues. In the end I think really pragmatic approach is going to have a big impact, and that is the approach taken by the megajournal.&lt;/p&gt;
&lt;p&gt;The approach of the megajournal is simple to describe. They remove the pretence of having issues, and they radically simplify the criteria for publication, asking for the most part only that the paper is scientifically rigourous. (Think about that for a moment, these journals are asking only that the papers that they publish advance our knowledge about the universe around us, not asking that they do so according to some predefined criteria. They are looking to ensure that the articles pass scientific muster, and allowing posterity to take care of the impact).i&lt;/p&gt;
&lt;p&gt;By significantly lowering the rejection rate by not rejection based on taste, but only on correctness, you dramatically increase the efficiency of getting knowledge published. If you tie that to a business model where you generate revenue as a function of the volume of articles that you publish (author pays open access for example), then you create a nice scalable revenue stream. This worked to move PLoS into the black on the back of PLoS One.&lt;/p&gt;
&lt;p&gt;The high impact journals have taken notice, and Nature&amp;rsquo;s &lt;a href=&#34;http://www.nature.com/srep/index.html&#34;&gt;Scientific Reports&lt;/a&gt; is an explicit move to create a megajournal that can be used to publish content that gets rejected from the selective Nature branded titles. I think this is brilliant. If the authors are willing, then for their part they can rapidly get their work published without having to endure a long resubmission process. Nature already has a large volume of content that is being submitted that they are not publishing, so the new journal should not have a cold start of submissions. The content that goes on in this new journal will be OA, leading to an increase in OA content, and one could imagine a future in which this revenue stream could supplant the traditional subscription model for nature publishing group.&lt;/p&gt;
&lt;p&gt;There are a few interesting trends emerging around megajournals.&lt;/p&gt;
&lt;p&gt;The first is that they by no means imply low quality as measured by the impact factor. PloS One has a rejection rate of somewhere between 25 and 30% and an initial impact factor of 4.3. That&amp;rsquo;s very good, in fact submissions more or less tripled after it received an impact factor.&lt;/p&gt;
&lt;p&gt;Next is that when a top tier journal creates a magajournal vehicle then the reviewers of the new journal really need to be educated about what the acceptance criteria are for the new journal. Publishers from Nature, Hindawi and The Genetics Society of America all described how their reviewers continued to reject articles on the bases of more than just scientific correctness. If it&amp;rsquo;s going to work you have to be willing and able to publish work that is clearly low impact, but demonstrably correct.&lt;/p&gt;
&lt;p&gt;Lastly, for some of these submissions are continuing to grow at a healthy pace. PLoS One now publishes something like 1.5 % of all academic articles, and it&amp;rsquo;s share is growing quarter on quarter. The implication in my mind is clear, that if this trend continues many of the small titles will be cannibalised. I asked at the conference if there was any actual evidence of this happening yet. The audience were clear to say that none have seen any impact on their journals, that there seems to be no broader evidence for such a trend, that globally the entire number of articles is growing annually at a few percent, so that so far one would not expect to see an effect. However I predict that this trend will lead to an impact on many small niche journals, and I look forward to the state of the publishing landscape in five years from now.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The rude health of Open Access Publishing.</title>
      <link>http://scholarly-comms-product-blog.com/2011/10/03/coasp-impressions/</link>
      <pubDate>Mon, 03 Oct 2011 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2011/10/03/coasp-impressions/</guid>
      <description>&lt;p&gt;TL;DR OA publishing is maturing with a scalable business model that all the big publishers are jumping all over. Money will be made (but less than before), and more content will be more open. The poor lamentable nay-sayers who carp on unheard in the darkness will be forgotten, and their Cassandra-like predictions will fade to be recalled as little more than the mutterings of fools (OK, that last bit is probably opinion).&lt;/p&gt;
&lt;p&gt;Just over a week ago I had the great pleasure to present some thoughts &lt;a href=&#34;http://altmetrics.org/manifesto/&#34;&gt;alt-metrics&lt;/a&gt; to the &lt;a href=&#34;http://www.oaspa.org/coasp/&#34;&gt;3rd Conference on Open Access Publishing&lt;/a&gt;. The talks were recorded and my talk will be posted in time.&lt;/p&gt;
&lt;p&gt;I wanted to write up a few thoughts that spun up out of my experience of meeting with that community and seeing what&amp;rsquo;s going on there.
It seems like there is a upswell of coverage on open access right now. In the week that was in it Princeton asked it&amp;rsquo;s researchers &lt;a href=&#34;http://theconversation.edu.au/princeton-bans-academics-from-handing-all-copyright-to-journal-publishers-3596&#34;&gt;not to hand over copyright&lt;/a&gt; (&lt;a href=&#34;https://docs.google.com/viewer?url=http%3A%2F%2Fwww.cs.princeton.edu%2F~appel%2Fopen-access-report.pdf&#34;&gt;statement&lt;/a&gt;), a call to &lt;a href=&#34;http://cr.yp.to/writing/ieee.html&#34;&gt;not publish with IEEE&lt;/a&gt; got picked up by Hacker News and the Times Higher Education posted a call for academics to &lt;a href=&#34;http://www.timeshighereducation.co.uk/story.asp?sectioncode=26&amp;amp;storycode=417576&amp;amp;c=1&#34;&gt;not peer review for non-OA journals&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One could be forgiven for thinking that something is afoot.&lt;/p&gt;
&lt;p&gt;Open access also got picked up in some national press, a fact that was &lt;a href=&#34;http://scholarlykitchen.sspnet.org/2011/09/22/london-calling-open-access-pr-wends-its-way-from-london-into-a-major-us-newspaper/&#34;&gt;lamented by the Scholarly Kitchen&lt;/a&gt; (What&amp;rsquo;s up with the Scholarly Kitchen anyway? I&amp;rsquo;ve had to be moderately careful about what I wrote in this paragraph to not stray over into mocking or insulting, but the core of my feeling on the topic is that when it comes to discussing open access publishing they are, and specifically Kent Anderson is, disingenuous about their coverage of OA in a highly negative way. I&amp;rsquo;ve met some of the contributors and they seem like nice intelligent people, so this continuos editorial stance makes me think of them as somewhat akin to the Daily Mail in the UK).&lt;/p&gt;
&lt;p&gt;The main sentiment from the conference was that OA publishing as an industry is flourishing. There were representatives form many traditional scientific publishers, and the discussion was all about revenues, peer review models and the enormous growth that all of these titles were seeing in submissions. The biggest point of discussion over the few days of the conference was about the rise of the megajournal, more of which in a subsequent post.&lt;/p&gt;
&lt;p&gt;What as also very clear was the diversity of philosophical approach to what OA meant from a publisher point of view. &lt;a href=&#34;http://www.plos.org/publications/journals/&#34;&gt;Plos&lt;/a&gt; focussed on their ethical standards, &lt;a href=&#34;http://www.hindawi.com/&#34;&gt;Hindawi&lt;/a&gt; talked about their scale at creating titles and running peer review over those titles in a scalable way.&lt;/p&gt;
&lt;p&gt;Some of these trends were raised by &lt;a href=&#34;http://www.sdsc.edu/~bourne/&#34;&gt;Phil Bourne&lt;/a&gt; in his keynote. It&amp;rsquo;s clear that the academic has a different view from the publisher. There is still a desire to see more use of CC0 licences, and to allow wide ranging data mining on the artefacts that Open Access can produce. Phil again called for a more integrated eco system of derivative objects created on top of the literature. It&amp;rsquo;s not clear whether publishers will be in a position to do this, but OA objects should allow a competitive market place of these kinds of things to emerge. (Of course the great promise is that they &lt;em&gt;should&lt;/em&gt; and it is by no means a given that they will).&lt;/p&gt;
&lt;p&gt;Phil had been involved in a summer workshop on moving beyond the PDF and some of their output can be seen at the &lt;a href=&#34;https://sites.google.com/site/futureofresearchcommunications/force11-tools-framework&#34;&gt;FORCE11&lt;/a&gt; site. Slowly we move in the right direction, slowly, slowly, but surely.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://twitter.com/#!/p_binfield&#34;&gt;Pete Binfield&lt;/a&gt; was busy telling everyone that within three years up to 50% of all content that is freshly published may be open access. Heady predictions, but form the submission figures that he showed for PloSONE not a prediction that is impossible to believe in.&lt;/p&gt;
&lt;p&gt;Overall there was a feeling that we will see a continued and strong growth in OA published content. Back of the envelope figures were being mentioned as if they were the accepted norm and people felt that the academic publishing industry might reduce in global value from it&amp;rsquo;s current nine billion in turnover per year to a more modest two to three billion, with savings coming from the more sane cost structure that comes along with Open Access publishing. Of course the only way to tell if a change like that is going to happen is to wait it out and see.&lt;/p&gt;
&lt;p&gt;It seems like enough traditional publishers are willing to get a toe in the water.&lt;/p&gt;
&lt;p&gt;In the end, over the past couple of weeks the best comment I&amp;rsquo;ve seen on the current state of the OA movement has come from the inestimable &lt;a href=&#34;http://del-fi.org/jtw&#34;&gt;John Willbanks&lt;/a&gt; who pointed out that it&amp;rsquo;s &lt;a href=&#34;http://del-fi.org/post/10561649700/open-access-is-infrastructure-not-religion&#34;&gt;no longer a question of religion but of infrastructure&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Turning the Physics ArXiV into an Open Peer Review System.</title>
      <link>http://scholarly-comms-product-blog.com/2011/01/04/arxiv.org-open-peer-review-proposal/</link>
      <pubDate>Tue, 04 Jan 2011 00:00:00 +0000</pubDate>
      
      <guid>http://scholarly-comms-product-blog.com/2011/01/04/arxiv.org-open-peer-review-proposal/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://math-www.uni-paderborn.de/~axel/&#34;&gt;Axel Boldt&lt;/a&gt; posted an interesting &lt;a href=&#34;http://arxiv.org/pdf/1011.6590v1&#34;&gt;short paper&lt;/a&gt; discussing how to turn the physics ArXiV into an open peer review system. It&amp;rsquo;s a short read, about three pages, but if you are familiar with the problems around peer review then you can just jump to part three of the paper which is a little under a page.&lt;/p&gt;
&lt;p&gt;The solution proposed is to create a new role of editor on the ArXiV, and allow anyone to propose their paper for review.  An open, almost endless, review process could ensue if scientists wanted to contribute time to review items. The editor has to choose who gets to review the paper, and this layer of peer review would require some maintenance. An extension to the idea might be to allow anyone to peer review a paper that was in the &amp;ldquo;reviewing pool&amp;rdquo;, and then attach reviewing profiles to the people who had done reviews. Those profiles could include information about the connections between reviewers and reviwees, and one could imagine an editor refusing the peer review stamp if those explicit connections between reviewer and reviewee were too close. With another tweak, one could imagine assigning points to the reviewers, and if one has enough review points, then you get to somehow jump the queue when you submit a paper for review.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;d say Axel&amp;rsquo;s idea is a good starting point. Many of the social gaming features that you would want to incorporate into a site that allows social reviews are understood, and if you built it right it could work. I think the success would depend on getting an initial stable or high-quality editors associated with a &amp;ldquo;journal brand&amp;rdquo;, that would attract submissions, and in addition you would want to build a fairly seemless system, but it&amp;rsquo;s not implausible to see that such a system could be built.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>